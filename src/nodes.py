"""LangGraph èŠ‚ç‚¹é€»è¾‘å®ç°ã€‚"""

import os
import json
from typing import Literal
from pydantic import BaseModel, Field

from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import PydanticOutputParser
from langchain_community.retrievers import BM25Retriever
from src.state import AgentState

def get_llm():
    # å»ºè®®è°ƒé«˜ä¸€ç‚¹ temperatureï¼Œè®©é€šç”¨å›ç­”ç¨å¾®çµæ´»ä¸€ç‚¹ï¼Œä½†ä¸è¦å¤ªé«˜
    return ChatOpenAI(
        model="deepseek-chat",
        openai_api_key=os.getenv("DEEPSEEK_API_KEY"),
        openai_api_base=os.getenv("OPENAI_API_BASE", "https://api.deepseek.com"),
        temperature=0.3, 
        max_retries=2
    )

# === 1. Supervisor (é€šç”¨ç ”ç©¶ä¸»ç®¡) ===

class RouteResponse(BaseModel):
    """Supervisor å†³ç­–ç»“æ„"""
    observed_gap: str = Field(
        ..., 
        description="åˆ†æå½“å‰ä¿¡æ¯ä¸ç”¨æˆ·é—®é¢˜ä¹‹é—´çš„å·®è·ã€‚æˆ‘ä»¬è¿˜ç¼ºä»€ä¹ˆä¿¡æ¯æ‰èƒ½å®Œç¾å›ç­”ï¼Ÿ"
    )
    next: Literal["Searcher", "Answerer"] = Field(
        ..., description="å¦‚æœä¿¡æ¯æœ‰ç¼ºå£é€‰ Searcherï¼Œä¿¡æ¯å……è¶³é€‰ Answererã€‚"
    )
    search_query: str = Field(
        default="", description="é’ˆå¯¹ã€observed_gapã€‘ç”Ÿæˆçš„ä¸‹ä¸€æ­¥å…·ä½“æœç´¢æŒ‡ä»¤ã€‚"
    )
    reasoning: str = Field(
        ..., description="å†³ç­–ç†ç”±ã€‚"
    )

def supervisor_node(state: AgentState) -> dict:
    messages = state["messages"]
    # è·å–å½“å‰è½®æ¬¡ï¼Œé»˜è®¤ä¸º0
    current_loop = state.get("loop_count", 0)
    # === è·å–è®°å¿† ===
    past_searches = state.get("attempted_searches", [])
    failed_topics = state.get("failed_topics", [])
    # è®¾ç½®æœ€å¤§æœç´¢æ·±åº¦ï¼Œå»ºè®® 5-8 æ¬¡
    MAX_LOOPS = 6 

    llm = get_llm()
    
    # === å¼ºåˆ¶æ­¢æŸé€»è¾‘ ===
    if current_loop >= MAX_LOOPS:
        print(f"ğŸ›‘ è¾¾åˆ°æœ€å¤§å¾ªç¯æ¬¡æ•° ({MAX_LOOPS})ï¼Œå¼ºåˆ¶è½¬ Answererã€‚")
        return {
            "next": "Answerer",
            "current_search_query": "",
            "loop_count": current_loop  # ä¿æŒä¸å˜
        }

    parser = PydanticOutputParser(pydantic_object=RouteResponse)
    format_instructions = parser.get_format_instructions()

    # === æ„é€ è®°å¿†æ–‡æœ¬ ===
    # å°†åˆ—è¡¨æ ¼å¼åŒ–ä¸ºå­—ç¬¦ä¸²ï¼Œæ”¾å…¥ Prompt
    if past_searches:
        history_str = "\n".join([f"- {q}" for q in past_searches])
    else:
        history_str = "æ—  (è¿™æ˜¯ç¬¬ä¸€æ¬¡æœç´¢)"
        
    # === æ„é€ å¤±è´¥è¯é¢˜æ–‡æœ¬ ===
    if failed_topics:
        failed_str = "\n".join([f"- {q}" for q in failed_topics])
        failed_section = f"""ã€âŒ å·²ç¡®è®¤çŸ¥è¯†åº“ä¸­ç¼ºå¤±çš„è¯é¢˜ (ä¸è¦å†æœï¼)ã€‘
{failed_str}"""
    else:
        failed_section = "æ— "

    system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªå…¨èƒ½å‹çš„ç ”ç©¶é¡¹ç›®ä¸»ç®¡ã€‚
    å½“å‰ç ”ç©¶è½®æ¬¡ï¼š{current_loop + 1} / {MAX_LOOPS}ã€‚
    
    ã€ğŸš« å·²å°è¯•çš„æœç´¢è·¯å¾„ (ç»å¯¹ç¦æ­¢é‡å¤è¯­ä¹‰)ã€‘
    {history_str}
    
    {failed_section}
    
    ã€å·¥ä½œæµç¨‹ã€‘
    1. **åˆ†æç°çŠ¶**ï¼šé˜…è¯»å†å²æœç´¢æŠ¥å‘Šã€‚ç”¨æˆ·é—®äº†ä»€ä¹ˆï¼Ÿæˆ‘ä»¬ç°åœ¨çŸ¥é“äº†ä»€ä¹ˆï¼Ÿ
    2. **è¯†åˆ«ç¼ºå£ (Gap Analysis)**ï¼š
       - æ˜¯å¦è¿˜æœ‰æœªè§£é‡Šçš„**ä¸“æœ‰åè¯**ï¼Ÿ
       - æ˜¯å¦åªæ‰¾åˆ°äº†Aé¢ï¼Œè€Œå¿½ç•¥äº†**Bé¢**ï¼ˆå¦‚åªçœ‹äº†ä¼˜ç‚¹æ²¡çœ‹ç¼ºç‚¹ï¼‰ï¼Ÿ
       - æ˜¯å¦è¿˜éœ€è¦å…·ä½“çš„**æ•°æ®/æ¡ˆä¾‹**æ¥æ”¯æ’‘è®ºç‚¹ï¼Ÿ
    3. **å†³ç­–**ï¼š
       - å¦‚æœå­˜åœ¨å…³é”®ç¼ºå£ï¼ŒæŒ‡æ´¾ 'Searcher' è¿›è¡Œé’ˆå¯¹æ€§æŒ–æ˜ã€‚
       - å¦‚æœä¿¡æ¯å·²è¶³å¤Ÿå½¢æˆä¸€ä¸ªé€»è¾‘ä¸¥å¯†çš„å›ç­”ï¼Œæˆ–å¤šæ¬¡æœç´¢æ— æœï¼ŒæŒ‡æ´¾ 'Answerer'ã€‚
       - å¦‚æœç¼ºå£æ¶‰åŠã€âŒ ç¼ºå¤±è¯é¢˜ã€‘ï¼Œè¯·ç›´æ¥å¿½ç•¥è¯¥éƒ¨åˆ†ï¼Œä¸è¦å†æŒ‡æ´¾ Searcher å»æœè¿™äº›æ­»è·¯ã€‚
    
    ã€é‡è¦ã€‘ä½ è¿˜æœ‰ {MAX_LOOPS - current_loop} æ¬¡æœç´¢æœºä¼šã€‚è¯·çæƒœæ¬¡æ•°ï¼Œå°½é‡ç²¾å‡†ã€‚
    
    {format_instructions}
    """
    
    try:
        response = llm.invoke([SystemMessage(content=system_prompt)] + messages)
        content = response.content.strip()
        if content.startswith("```json"):
            content = content.replace("```json", "").replace("```", "")
        elif content.startswith("```"):
            content = content.replace("```", "")
        
        decision = parser.parse(content)
    except Exception as e:
        print(f"Supervisor Error: {e}")
        decision = RouteResponse(
            observed_gap="Error", next="Answerer", search_query="", reasoning="System Error"
        )

    print(f"\nğŸ¤” [Supervisor Loop {current_loop + 1}]\nå·²æœè¿‡: {past_searches}\nå¤±è´¥è¯é¢˜: {failed_topics}\nå†³å®š: {decision.next} -> {decision.search_query}\n")

    return {
        "next": decision.next,
        "current_search_query": decision.search_query,
        # æ¯æ¬¡ç»è¿‡ Supervisorï¼Œè®¡æ•°å™¨ +1
        "loop_count": current_loop + 1
    }

# === 2. Searcher (é€šç”¨æƒ…æŠ¥æœé›†å‘˜) ===

def search_node(state: AgentState) -> dict:
    query = state.get("current_search_query", "")
    source_docs = state.get("source_documents", [])
    vector_store = state.get("vector_store", None)
    
    if not query:
        return {"messages": [AIMessage(content="Searcher: æŒ‡ä»¤ä¸ºç©ºã€‚", name="Searcher")]}

    llm = get_llm()

    # A. å…³é”®è¯æ³›åŒ– (Keyword Expansion)
    # ä¸å†å¼ºåˆ¶åŠ  "root cause"ï¼Œè€Œæ˜¯æ ¹æ®è¯­ä¹‰æ‰©å±•
    expansion_prompt = f"""ä½ æ˜¯ä¸€ä¸ªæœç´¢ä¸“å®¶ã€‚è¯·é’ˆå¯¹æœç´¢æ„å›¾ "{query}"ï¼Œç”Ÿæˆ 2-3 ä¸ªç”¨äºå…³é”®è¯æ£€ç´¢çš„æ‰©å±•è¯ã€‚
    ç­–ç•¥ï¼š
    1. æå–æ ¸å¿ƒå®ä½“ï¼ˆEntityï¼‰ã€‚
    2. è¡¥å……åŒä¹‰è¯ã€ä¸“ä¸šæœ¯è¯­æˆ–è‹±æ–‡ç¿»è¯‘ã€‚
    3. å¦‚æœæ˜¯ç‰¹å®šé¢†åŸŸï¼ˆå¦‚æ³•å¾‹ã€åŒ»ç–—ï¼‰ï¼ŒåŠ å…¥ç›¸å…³é™å®šè¯ã€‚
    
    åªè¾“å‡ºå…³é”®è¯ï¼Œç”¨ç©ºæ ¼åˆ†éš”ã€‚"""
    
    bm25_keywords = llm.invoke([HumanMessage(content=expansion_prompt)]).content.strip().replace('"', '')
    
    results_bm25 = []
    results_vector = []

    # B. æ··åˆæ£€ç´¢æ‰§è¡Œ
    if source_docs:
        try:
            bm25_retriever = BM25Retriever.from_documents(source_docs)
            bm25_retriever.k = 3
            # ç»„åˆæŸ¥è¯¢ï¼šè‡ªç„¶è¯­è¨€ + æ‰©å±•å…³é”®è¯
            results_bm25 = bm25_retriever.invoke(f"{query} {bm25_keywords}")
        except: pass
    
    if vector_store:
        try:
            vector_retriever = vector_store.as_retriever(search_kwargs={"k": 4}) # å‘é‡å¤šå–ä¸€ç‚¹
            results_vector = vector_retriever.invoke(query)
        except: pass

    # C. ç»“æœåˆå¹¶ä¸å»é‡
    all_results = results_vector + results_bm25
    unique_docs = []
    seen = set()
    for doc in all_results:
        if doc.page_content not in seen:
            unique_docs.append(doc)
            seen.add(doc.page_content)
    
    final_docs = unique_docs[:6] # ç¨å¾®å¤šç»™ä¸€ç‚¹ä¸Šä¸‹æ–‡
    
    if not final_docs:
        return {
            "messages": [AIMessage(content=f"Searcher: æœªæ‰¾åˆ°å…³äº '{query}' çš„ç›¸å…³ä¿¡æ¯ã€‚", name="Searcher")],
            # å³ä½¿æ²¡æ‰¾åˆ°ï¼Œä¹Ÿè¦è®°å½•â€œæˆ‘æœè¿‡è¿™ä¸ªè¯äº†â€ï¼Œé˜²æ­¢ Supervisor åˆè®©æœä¸€é
            "attempted_searches": [query],
            # === æ ‡è®°ä¸ºå¤±è´¥è¯é¢˜ ===
            "failed_topics": [query]
        }

    # D. ä¿¡æ¯èƒå– (é€šç”¨åŒ–)
    context_text = "\n\n".join([f"[Ref {i+1}] {d.page_content}" for i, d in enumerate(final_docs)])
    
    filter_prompt = f"""ä½ æ˜¯ä¸€ä¸ªå®¢è§‚çš„æƒ…æŠ¥åˆ†æå‘˜ã€‚
    
    ã€æœç´¢ä»»åŠ¡ã€‘: "{query}"
    ã€æ£€ç´¢èµ„æ–™ã€‘:
    {context_text}
    
    è¯·ä»èµ„æ–™ä¸­æå–ä¸ä»»åŠ¡ç›¸å…³çš„ä¿¡æ¯ã€‚
    è¦æ±‚ï¼š
    1. ä¿æŒå®¢è§‚ï¼Œä¸è¦ç¼–é€ ã€‚
    2. æå–å…³é”®å®šä¹‰ã€æ•°æ®ã€è§‚ç‚¹ã€æ—¶é—´çº¿æˆ–å› æœå…³ç³»ã€‚
    3. å¦‚æœèµ„æ–™åŒ…å«çŸ›ç›¾ä¿¡æ¯ï¼Œè¯·ä¸€å¹¶åˆ—å‡ºã€‚
    4. å¦‚æœèµ„æ–™ä¸­å®Œå…¨æ²¡æœ‰ä¸æœç´¢ä»»åŠ¡ç›¸å…³çš„å†…å®¹ï¼Œè¯·æ˜ç¡®è¯´æ˜"æœªæ‰¾åˆ°ç›¸å…³å†…å®¹"ã€‚
    """
    
    extraction = llm.invoke([HumanMessage(content=filter_prompt)]).content
    
    # === æ–°å¢ï¼šæ£€æŸ¥æ˜¯å¦çœŸçš„æ‰¾åˆ°äº†ç›¸å…³å†…å®¹ ===
    # å¦‚æœLLMæ˜ç¡®è¡¨ç¤ºæœªæ‰¾åˆ°ç›¸å…³å†…å®¹ï¼Œåˆ™æ ‡è®°ä¸ºå¤±è´¥è¯é¢˜
    is_empty_result = "æœªæ‰¾åˆ°" in extraction or "æ²¡æœ‰æ‰¾åˆ°" in extraction or "æ— ç›¸å…³" in extraction
    
    if is_empty_result:
        return {
            "messages": [AIMessage(content=f"ã€æœç´¢æŠ¥å‘Šã€‘\næ£€ç´¢æ–¹å‘: {query}\næ‰©å±•è¯: {bm25_keywords}\nå‘ç°:\n{extraction}", name="Searcher")],
            "attempted_searches": [query],
            # === æ ‡è®°ä¸ºå¤±è´¥è¯é¢˜ ===
            "failed_topics": [query]
        }
    
    return {
        "messages": [AIMessage(content=f"ã€æœç´¢æŠ¥å‘Šã€‘\næ£€ç´¢æ–¹å‘: {query}\næ‰©å±•è¯: {bm25_keywords}\nå‘ç°:\n{extraction}", name="Searcher")],
        "final_evidence": final_docs,
        
        # === æ ¸å¿ƒä¿®æ”¹ï¼šå°†å½“å‰ Query å†™å…¥è®°å¿† ===
        # ç”±äº State å®šä¹‰äº† operator.addï¼Œè¿™ä¸ªåˆ—è¡¨ä¼šè¢«è¿½åŠ åˆ°æ€»åˆ—è¡¨ä¸­
        "attempted_searches": [query]
    }

# === 3. Answerer (é€šç”¨å†…å®¹åˆ›ä½œè€…) ===

def answer_node(state: AgentState) -> dict:
    messages = state["messages"]
    evidences = state.get("final_evidence", [])
    
    llm = get_llm()
    
    evidence_text = ""
    if evidences:
        evidence_text = "ã€åŸå§‹çŸ¥è¯†åº“ç‰‡æ®µã€‘\n"
        for i, doc in enumerate(evidences):
            content_preview = doc.page_content.replace('\n', ' ')[:300] # é™åˆ¶é•¿åº¦é˜²æ­¢ token æº¢å‡º
            evidence_text += f"> [Ref {i+1}] ...{content_preview}...\n(Source: {doc.metadata.get('source', 'Unknown')})\n\n"
    else:
        evidence_text = "ã€åŸå§‹çŸ¥è¯†åº“ç‰‡æ®µã€‘: æ— \n"
        
    system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ¥è¯†æ•´åˆä¸“å®¶ã€‚
    
    è¯·åŸºäºã€AIå›ç­”å†å²ã€‘å’Œã€åŸå§‹çŸ¥è¯†åº“ç‰‡æ®µã€‘å›ç­”ç”¨æˆ·é—®é¢˜ã€‚
    
    {evidence_text}
    
    ã€è¾“å‡ºç»“æ„è¦æ±‚ã€‘
    1. **æ·±åº¦å›ç­”**ï¼šè¯¦ç»†å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œå¼•ç”¨ [Ref X] ä½è¯ã€‚
    2. **ç»“è®º**ï¼šä¸€å¥è¯æ€»ç»“æ ¸å¿ƒè§‚ç‚¹ã€‚
    3. **ğŸ§ å»ºè®®è¿›ä¸€æ­¥ç ”ç©¶çš„é—®é¢˜**ï¼š
       - åŸºäºç°æœ‰çš„å›ç­”ï¼Œç”Ÿæˆ 3 ä¸ªç”¨æˆ·å¯èƒ½æ„Ÿå…´è¶£çš„**æ·±å±‚é—®é¢˜**ã€‚
       - è¿™äº›é—®é¢˜åº”è¯¥èƒ½å¼•å¯¼ç”¨æˆ·æŒ–æ˜æ–‡æ¡£ä¸­å°šæœªå……åˆ†å±•å¼€çš„ç»†èŠ‚ã€‚
    """
    
    response = llm.invoke([SystemMessage(content=system_prompt)] + messages)
    return {"messages": [response], "next": "END"}