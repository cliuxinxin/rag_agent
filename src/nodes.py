"""LangGraph èŠ‚ç‚¹é€»è¾‘å®ç°ã€‚"""

import os
import json
from typing import Literal
from pydantic import BaseModel, Field

from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import PydanticOutputParser
from langchain_community.retrievers import BM25Retriever
from langchain_core.documents import Document
from src.state import AgentState

# å¼•å…¥ FlashRank
try:
    from flashrank import Ranker, RerankRequest
    # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
    os.makedirs("opt", exist_ok=True)
    reranker = Ranker(model_name="ms-marco-MiniLM-L-12-v2", cache_dir="opt")
    USE_RERANKER = True
except ImportError:
    USE_RERANKER = False
    print("æœªå®‰è£… flashrankï¼Œå°†è·³è¿‡é‡æ’åºæ­¥éª¤ã€‚")
except Exception as e:
    USE_RERANKER = False
    print(f"FlashRank åˆå§‹åŒ–å¤±è´¥: {e}")

def get_llm():
    return ChatOpenAI(
        model="deepseek-chat",
        openai_api_key=os.getenv("DEEPSEEK_API_KEY"),
        openai_api_base=os.getenv("OPENAI_API_BASE", "https://api.deepseek.com"),
        temperature=0.3,
        max_retries=2
    )

# === 1. Supervisor ===

class RouteResponse(BaseModel):
    observed_gap: str = Field(..., description="åˆ†æä¿¡æ¯ç¼ºå£")
    next: Literal["Searcher", "Answerer"] = Field(..., description="ä¸‹ä¸€æ­¥è§’è‰²")
    search_query: str = Field(default="", description="æœç´¢æŒ‡ä»¤")
    reasoning: str = Field(..., description="ç†ç”±")

def supervisor_node(state: AgentState) -> dict:
    messages = state["messages"]
    current_loop = state.get("loop_count", 0)
    
    past_searches = state.get("attempted_searches", [])
    failed_topics = state.get("failed_topics", [])
    
    MAX_LOOPS = 6 
    llm = get_llm()
    
    # å˜é‡å®šä¹‰é˜²æŠ¥é”™
    if past_searches:
        history_str = "\n".join([f"- {q}" for q in past_searches])
    else:
        history_str = "æ— "

    if failed_topics:
        failed_str = "\n".join([f"- {q}" for q in failed_topics])
    else:
        failed_str = "æ— "

    if current_loop >= MAX_LOOPS:
        return {"next": "Answerer", "current_search_query": "", "loop_count": current_loop}

    parser = PydanticOutputParser(pydantic_object=RouteResponse)
    format_instructions = parser.get_format_instructions()

    system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªå…¨èƒ½å‹çš„ç ”ç©¶é¡¹ç›®ä¸»ç®¡ã€‚
    å½“å‰ç ”ç©¶è½®æ¬¡ï¼š{current_loop + 1} / {MAX_LOOPS}ã€‚
    
    ã€å·²å°è¯•çš„æœç´¢ã€‘
    {history_str}
    
    ã€âŒ å·²ç¡®è®¤æ— ç»“æœçš„è¯é¢˜ (ä¸è¦é‡æœ)ã€‘
    {failed_str}
    
    ã€å·¥ä½œæµç¨‹ã€‘
    1. åˆ†æç°çŠ¶ï¼šæˆ‘ä»¬çŸ¥é“äº†ä»€ä¹ˆï¼Ÿ
    2. **è¯†åˆ«ç¼ºå£**ï¼š
       - å¦‚æœç”¨æˆ·é—®â€œè¿™ç¯‡æ–‡ç« è®²äº†ä»€ä¹ˆ/æ€»ç»“å…¨æ–‡â€ï¼Œä¸”æˆ‘ä»¬è¿˜æ²¡æœè¿‡â€œæ‘˜è¦/ç›®å½•â€ï¼Œè¿™æ˜¯å·¨å¤§ç¼ºå£ã€‚
       - å¦‚æœæ˜¯ç»†èŠ‚é—®é¢˜ï¼Œæ£€æŸ¥æ˜¯å¦ç¼ºå°‘å…³é”®æ•°æ®ã€‚
    3. **æ™ºèƒ½å†³ç­–**ï¼š
       - å¦‚æœ Searcher å·²ç»æä¾›äº†æ¨æ–­çš„æ ‡é¢˜æˆ–æ ¸å¿ƒä¿¡æ¯ï¼Œè¯·ä¸è¦å†é‡å¤è¦æ±‚ç¡®è®¤æ ‡é¢˜ï¼Œç›´æ¥åŸºäºè¯¥ä¿¡æ¯è¿›è¡Œæ·±å…¥æŒ–æ˜ã€‚
       - ä¸€æ—¦è¯†åˆ«å‡ºå¯èƒ½çš„è®ºæ–‡æ ‡é¢˜æˆ–æ ¸å¿ƒä¸»é¢˜ï¼Œç«‹åˆ»è½¬å‘å†…å®¹æ·±æŒ–ï¼Œä¸è¦çº ç»“äºå…ƒæ•°æ®ï¼ˆMetadataï¼‰çš„ç¡®è®¤ã€‚
    4. å†³ç­–ï¼šæŒ‡æ´¾ Searcher æˆ– Answererã€‚
    
    {format_instructions}
    """
    
    try:
        response = llm.invoke([SystemMessage(content=system_prompt)] + messages)
        content = response.content.strip()
        if content.startswith("```json"):
            content = content.replace("```json", "").replace("```", "")
        elif content.startswith("```"):
            content = content.replace("```", "")
        decision = parser.parse(content)
    except Exception as e:
        print(f"Supervisor Error: {e}")
        decision = RouteResponse(
            observed_gap="Error", next="Answerer", search_query="", reasoning="System Error"
        )

    print(f"\nğŸ¤” [Supervisor Loop {current_loop + 1}]\nå†³å®š: {decision.next} -> {decision.search_query}\n")

    return {
        "next": decision.next,
        "current_search_query": decision.search_query,
        "loop_count": current_loop + 1
    }

# === 2. Searcher ===

def search_node(state: AgentState) -> dict:
    query = state.get("current_search_query", "")
    source_docs = state.get("source_documents", [])
    vector_store = state.get("vector_store", None)
    
    if not query:
        return {"messages": [AIMessage(content="Searcher: æŒ‡ä»¤ä¸ºç©ºã€‚", name="Searcher")]}

    llm = get_llm()

    # å…³é”®è¯æ‰©å±•
    expansion_prompt = f"""ä½ æ˜¯ä¸€ä¸ªæœç´¢ä¸“å®¶ã€‚è¯·é’ˆå¯¹æœç´¢æ„å›¾ "{query}"ï¼Œç”Ÿæˆ 3-4 ä¸ªç”¨äºå…³é”®è¯æ£€ç´¢çš„æ‰©å±•è¯ã€‚
    ã€ç‰¹æ®Šç­–ç•¥ã€‘ï¼š
    - **æ¦‚æ‹¬æ€§é—®é¢˜**ï¼šå¦‚æœç”¨æˆ·é—®â€œè¿™ç¯‡æ–‡ç« è®²äº†ä»€ä¹ˆâ€ã€â€œæ€»ç»“â€ã€â€œä¸»è¦å†…å®¹â€ï¼Œè¯·åŠ¡å¿…åŒ…å«ï¼š
      "Abstract", "Introduction", "Conclusion", "Summary", "Table of Contents", "Overview", "æ‘˜è¦", "ç»“è®º", "ç›®å½•"ã€‚
    - **ç»†èŠ‚é—®é¢˜**ï¼šæå–æ ¸å¿ƒå®ä½“ã€‚
    åªè¾“å‡ºå…³é”®è¯ï¼Œç”¨ç©ºæ ¼åˆ†éš”ã€‚"""
    
    bm25_keywords = llm.invoke([HumanMessage(content=expansion_prompt)]).content.strip().replace('"', '')
    
    results_bm25 = []
    results_vector = []

    # æ£€ç´¢
    if source_docs:
        try:
            bm25_retriever = BM25Retriever.from_documents(source_docs)
            bm25_retriever.k = 10 
            results_bm25 = bm25_retriever.invoke(f"{query} {bm25_keywords}")
        except: pass
    
    if vector_store:
        try:
            vector_retriever = vector_store.as_retriever(search_kwargs={"k": 10})
            results_vector = vector_retriever.invoke(query)
        except: pass

    # åˆå¹¶å»é‡
    all_results = results_vector + results_bm25
    unique_docs = {}
    for doc in all_results:
        if doc.page_content not in unique_docs:
            unique_docs[doc.page_content] = doc
    unique_docs_list = list(unique_docs.values())

    if not unique_docs_list:
        return {
            "messages": [AIMessage(content=f"Searcher: æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚", name="Searcher")],
            "attempted_searches": [query],
            "failed_topics": [query]
        }

    # é‡æ’åº
    if USE_RERANKER:
        try:
            passages = [
                {"id": i, "text": doc.page_content, "meta": doc.metadata} 
                for i, doc in enumerate(unique_docs_list)
            ]
            rerank_request = RerankRequest(query=query, passages=passages)
            reranked_results = reranker.rank(rerank_request)
            
            final_docs = []
            for item in reranked_results[:6]:
                doc = Document(page_content=item['text'], metadata=item['meta'])
                final_docs.append(doc)
        except Exception as e:
            print(f"Rerank Error: {e}")
            final_docs = unique_docs_list[:6]
    else:
        final_docs = unique_docs_list[:6]

    # ç¬”è®°ç”Ÿæˆ
    context_text = "\n\n".join([f"[Ref {i+1}] {d.page_content}" for i, d in enumerate(final_docs)])
    filter_prompt = f"""ä½ æ˜¯ä¸€ä¸ªæƒ…æŠ¥åˆ†æå‘˜ã€‚
    ä»»åŠ¡: "{query}"
    èµ„æ–™:
    {context_text}
    è¯·æå–å…³é”®ä¿¡æ¯ã€‚å¦‚æœæ˜¯æ¦‚æ‹¬æ€§é—®é¢˜ï¼Œé‡ç‚¹æå–ç»“æ„å’Œç»“è®ºã€‚"""
    extraction = llm.invoke([HumanMessage(content=filter_prompt)]).content
    
    current_note = f"### ğŸ” æœç´¢ä¸»é¢˜: {query} (å…³é”®è¯: {bm25_keywords})\n{extraction}\n"

    return {
        "messages": [AIMessage(content=f"ã€æœç´¢æŠ¥å‘Šã€‘\næ–¹å‘: {query}\nå‘ç°:\n{extraction}", name="Searcher")],
        "final_evidence": final_docs,
        "attempted_searches": [query],
        "research_notes": [current_note]
    }

# === 3. Answerer (æ ¸å¿ƒä¿®æ”¹ï¼šå¼ºåˆ¶æ‹¼æ¥é™„å½•) ===

def answer_node(state: AgentState) -> dict:
    messages = state["messages"]
    evidences = state.get("final_evidence", [])
    notes = state.get("research_notes", [])
    
    llm = get_llm()
    
    # æ„é€  Prompt ç”¨çš„ä¸Šä¸‹æ–‡
    notes_text = "ã€ğŸ•µï¸â€â™‚ï¸ è°ƒæŸ¥ç¬”è®°ã€‘\n" + "\n".join(notes) if notes else "æ— è°ƒæŸ¥è®°å½•ã€‚"
    evidence_text = "ã€ğŸ“š åŸå§‹ç‰‡æ®µã€‘\n"
    for i, doc in enumerate(evidences):
        content_preview = doc.page_content.replace('\n', ' ')[:200]
        evidence_text += f"> [Ref {i+1}] {content_preview}...\n"

    system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ¥è¯†åº“åŠ©æ‰‹ã€‚
    è¯·åŸºäºã€è°ƒæŸ¥ç¬”è®°ã€‘å’Œã€åŸå§‹ç‰‡æ®µã€‘å›ç­”ç”¨æˆ·é—®é¢˜ã€‚
    
    {notes_text}
    {evidence_text}
    
    ã€æ’°å†™è¦æ±‚ã€‘
    1. ç»“æ„æ¸…æ™°ï¼šæ ¸å¿ƒç»“è®º -> è¿‡ç¨‹ç»¼è¿° -> è¯¦ç»†åˆ†æã€‚
    2. ä¸¥è°¨å¼•ç”¨ï¼šæ–‡ä¸­å¿…é¡»å¼•ç”¨ [Ref X]ã€‚
    3. å»ºè®®è¿›ä¸€æ­¥æŒ–æ˜çš„é—®é¢˜ï¼šæ°å¥½ 3 ä¸ªï¼Œå¿…é¡»åŸºäºçŸ¥è¯†åº“ï¼Œä¸è¦é—®å…¬ç½‘é—®é¢˜ã€‚
    """
    
    response = llm.invoke([SystemMessage(content=system_prompt)] + messages)
    
    # === æ ¸å¿ƒä¿®å¤ï¼šå°†è¯¦ç»†æ•°æ®æ‹¼æ¥åˆ°å›å¤æœ«å°¾ ===
    # è¿™ä¸€æ­¥è‡³å…³é‡è¦ï¼Œå› ä¸º LLM é€šå¸¸ä¸ä¼šè‡ªå·±æŠŠåŸå§‹æ–‡æ¡£æŠ„ä¸€éè¿”å›ã€‚
    # æˆ‘ä»¬å¿…é¡»æ‰‹åŠ¨ appendï¼Œå‰ç«¯æ‰èƒ½æå–åˆ°å†…å®¹ç”¨äº Tooltip å’Œ Expanderã€‚
    
    appendix = "\n\n"
    
    if notes:
        appendix += "ã€ğŸ•µï¸â€â™‚ï¸ è°ƒæŸ¥ç¬”è®°ã€‘\n" + "\n".join(notes) + "\n\n"
    
    if evidences:
        appendix += "ã€ğŸ“š åŸå§‹ç‰‡æ®µã€‘\n"
        for i, doc in enumerate(evidences):
            # æ¸…ç†æ¢è¡Œç¬¦ï¼Œä¿æŒæ•´æ´
            content = doc.page_content.replace('\n', ' ')[:350] # æˆªå–å‰350å­—ç¬¦ï¼Œé˜²æ­¢è¿‡é•¿
            source = doc.metadata.get('source', 'Unknown')
            # æ ¼å¼å¿…é¡»ä¸¥æ ¼ç¬¦åˆå‰ç«¯æ­£åˆ™: > [Ref ID] Content...
            appendix += f"> [Ref {i+1}] {content}...\n(Source: {source})\n\n"
    
    # ä¿®æ”¹æ¶ˆæ¯å†…å®¹
    response.content += appendix
    
    return {"messages": [response], "next": "END"}