import json
from langchain_core.messages import HumanMessage, SystemMessage
from src.nodes.common import get_llm
from src.prompts import (
    get_newsroom_base_prompt,
    get_angle_generator_prompt,
    get_outline_architect_prompt,
    get_internal_researcher_prompt,
    get_section_drafter_prompt,
    get_editor_reviewer_prompt,
    get_final_polisher_prompt
)
from src.state import NewsroomState
# [æ–°å¢]
from src.tools.search import tavily_search


# === 1. ç­–åˆ’é˜¶æ®µ (Planning) ===
# [æ–°å¢] å®è§‚æœç´¢èŠ‚ç‚¹ (ä¸“é—¨è´Ÿè´£æœ)
def macro_search_node(state: NewsroomState) -> dict:
    """ç­–åˆ’é˜¶æ®µï¼šå®è§‚èƒŒæ™¯æœç´¢"""
    req = state["user_requirement"]
    enable_search = state.get("enable_web_search", False)
    llm = get_llm()
    
    if not enable_search:
        return {"macro_search_context": "", "run_logs": ["â­ï¸ æœªå¼€å¯è”ç½‘ï¼Œè·³è¿‡èƒŒæ™¯æœç´¢..."]}
    
    # 1. ç”Ÿæˆæœç´¢è¯
    query_gen_prompt = f"åŸºäºç”¨æˆ·éœ€æ±‚: '{req}'ï¼Œç”Ÿæˆä¸€ä¸ªç”¨äºGoogleæœç´¢èƒŒæ™¯ä¿¡æ¯çš„å…³é”®è¯ã€‚åªè¾“å‡ºè¯ï¼Œä¸è¦è§£é‡Šã€‚"
    keyword = llm.invoke([HumanMessage(content=query_gen_prompt)]).content.strip().replace('"', '')
    
    # 2. æ‰§è¡Œæœç´¢
    log_msg = f"ğŸŒ [ç­–åˆ’] æ­£åœ¨å…¨ç½‘æœç´¢å¤§èƒŒæ™¯: '{keyword}'..."
    raw_res = tavily_search(keyword, max_results=3)
    
    # 3. è¿”å›ç»“æœå’Œæ—¥å¿—
    context = raw_res if raw_res else ""
    return {
        "macro_search_context": context, 
        "run_logs": [log_msg, f"âœ… å·²æ‰¾åˆ°ç›¸å…³èƒŒæ™¯èµ„æ–™ ({len(context)} chars)"]
    }


def angle_generator_node(state: NewsroomState) -> dict:
    """é¦–å¸­ç­–åˆ’ï¼šç”Ÿæˆåˆ‡å…¥è§’åº¦ (åªè´Ÿè´£ç”Ÿæˆ)"""
    full_text = state["full_content"]
    req = state["user_requirement"]
    # [ä¿®æ”¹] ç›´æ¥ä» State è·å–æœç´¢ç»“æœï¼Œä¸å†è‡ªå·±æœ
    search_context = state.get("macro_search_context", "")

    llm = get_llm()
    base_sys = SystemMessage(content=get_newsroom_base_prompt(full_text, req))
    
    # ä¼ å…¥ prompt
    user_msg = HumanMessage(content=get_angle_generator_prompt(search_context))
    response = llm.invoke([base_sys, user_msg]).content

    # ... (JSON è§£æä»£ç ä¸å˜) ...
    try:
        clean_json = response.replace("```json", "").replace("```", "").strip()
        angles = json.loads(clean_json)
    except Exception:
        angles = [
            {"title": "æ·±åº¦åˆ†æ", "desc": "åŸºäºæ–‡æ¡£å†…å®¹çš„æ ‡å‡†åˆ†æ", "reasoning": "Fallback"},
            {"title": "è¦ç‚¹æç‚¼", "desc": "å¿«é€Ÿæå–æ ¸å¿ƒä¿¡æ¯", "reasoning": "Fallback"},
            {"title": "æ‰¹åˆ¤æ€§è§†è§’", "desc": "å¯»æ‰¾æ–‡æ¡£ä¸­çš„é€»è¾‘æ¼æ´", "reasoning": "Fallback"}
        ]

    return {"generated_angles": angles}


def outline_architect_node(state: NewsroomState) -> dict:
    """æ¶æ„å¸ˆï¼šç”Ÿæˆå¤§çº²"""
    full_text = state["full_content"]
    req = state["user_requirement"]
    angle_data = state.get("selected_angle", {})
    angle_str = f"{angle_data.get('title')} - {angle_data.get('desc')}"

    llm = get_llm()
    base_sys = SystemMessage(content=get_newsroom_base_prompt(full_text, req))
    user_msg = HumanMessage(content=get_outline_architect_prompt(angle_str))

    response = llm.invoke([base_sys, user_msg]).content

    try:
        clean_json = response.replace("```json", "").replace("```", "").strip()
        outline = json.loads(clean_json)
    except Exception:
        outline = [{"title": "ç”Ÿæˆå¤±è´¥", "gist": "è¯·é‡è¯•", "key_facts": ""}]

    return {"outline": outline, "current_section_index": 0, "section_drafts": [], "full_draft": ""}


# === 2. é‡‡ç¼–ä¸æ’°å†™å¾ªç¯ (Drafting Loop) ===
def internal_researcher_node(state: NewsroomState) -> dict:
    """å†…éƒ¨æ¢å‘˜ï¼šåœ¨æ–‡æ¡£ä¸­æŸ¥è¯äº‹å® (æ”¯æŒå¾®è§‚æœç´¢)"""
    full_text = state["full_content"]
    req = state["user_requirement"]
    outline = state["outline"]
    idx = state["current_section_index"]
    enable_search = state.get("enable_web_search", False) # è·å–å¼€å…³

    if idx >= len(outline):
        return {"next": "Reviewer"}

    section = outline[idx]
    llm = get_llm()
    
    # [æ–°å¢] å¾®è§‚æœç´¢
    search_context = ""
    logs = [] # [æ–°å¢] æ—¥å¿—åˆ—è¡¨
    
    if enable_search:
        query = f"{section['title']} {section.get('key_facts', '')}"
        # [æ–°å¢] è®°å½•æ—¥å¿—
        logs.append(f"ğŸŒ [é‡‡ç¼–] æ­£åœ¨æ ¸å®ç¬¬ {idx+1} ç« æ•°æ®: '{query}'")
        
        # æœç´¢ (åªå–å‰2æ¡ï¼Œä¿è¯é€Ÿåº¦)
        raw_res = tavily_search(query, max_results=2)
        if raw_res:
            search_context = raw_res
            logs.append("âœ… ç½‘ç»œå–è¯å®Œæˆ")
        else:
            logs.append("âš ï¸ ç½‘ç»œæœç´¢æ— ç»“æœï¼Œä½¿ç”¨æœ¬åœ°æ–‡æ¡£")

    base_sys = SystemMessage(content=get_newsroom_base_prompt(full_text, req))
    # [ä¿®æ”¹] ä¼ å…¥ search_context
    user_msg = HumanMessage(content=get_internal_researcher_prompt(
        section['title'], 
        section.get('key_facts', ''),
        search_context # <--- ä¼ å…¥
    ))

    notes = llm.invoke([base_sys, user_msg]).content
    # [ä¿®æ”¹] è¿”å› logs
    return {"research_cache": notes, "next": "Drafter", "run_logs": logs}


def section_drafter_node(state: NewsroomState) -> dict:
    """æ’°ç¨¿äººï¼šåŸºäºç¬”è®°å†™ä¸€ç« """
    full_text = state["full_content"]
    req = state["user_requirement"]
    outline = state["outline"]
    idx = state["current_section_index"]
    notes = state.get("research_cache", "")

    drafts = state.get("section_drafts", [])
    prev_context = drafts[-1] if drafts else "ï¼ˆè¿™æ˜¯æ–‡ç« çš„å¼€å¤´ï¼‰"

    section = outline[idx]

    llm = get_llm()
    base_sys = SystemMessage(content=get_newsroom_base_prompt(full_text, req))
    user_msg = HumanMessage(content=get_section_drafter_prompt(section['title'], notes, prev_context))

    content = llm.invoke([base_sys, user_msg]).content

    # æ¸…æ´—ï¼šå»æ‰å¯èƒ½é‡å¤ç”Ÿæˆçš„æ ‡é¢˜
    clean_content = content.replace(f"# {section['title']}", "").replace(f"## {section['title']}", "").strip()

    new_drafts = drafts + [f"## {section['title']}\n\n{clean_content}"]

    return {
        "section_drafts": new_drafts,
        "current_section_index": idx + 1,
        "next": "Dispatcher"
    }


def dispatcher_node(state: NewsroomState) -> dict:
    """è°ƒåº¦å™¨ï¼šå†³å®šæ˜¯ç»§ç»­å†™è¿˜æ˜¯å»å®¡é˜…"""
    idx = state["current_section_index"]
    outline = state["outline"]
    if idx < len(outline):
        return {"next": "Researcher"}
    full_text = "\n\n".join(state["section_drafts"])
    return {"next": "Reviewer", "full_draft": full_text}


# === 3. å®¡é˜…ä¸æ¶¦è‰² (Review & Polish) ===
def reviewer_node(state: NewsroomState) -> dict:
    """ä¸»ç¼–ï¼šå®¡é˜…"""
    full_text = state["full_content"]
    req = state["user_requirement"]
    full_draft = state["full_draft"]

    llm = get_llm()
    base_sys = SystemMessage(content=get_newsroom_base_prompt(full_text, req))
    user_msg = HumanMessage(content=get_editor_reviewer_prompt(full_draft))

    critique = llm.invoke([base_sys, user_msg]).content
    return {"critique_notes": critique, "next": "Polisher"}


def polisher_node(state: NewsroomState) -> dict:
    """æ¶¦è‰²å¸ˆï¼šå®šç¨¿"""
    full_text = state["full_content"]
    req = state["user_requirement"]
    full_draft = state["full_draft"]
    critique = state["critique_notes"]

    llm = get_llm()
    base_sys = SystemMessage(content=get_newsroom_base_prompt(full_text, req))
    user_msg = HumanMessage(content=get_final_polisher_prompt(full_draft, critique))

    final_article = llm.invoke([base_sys, user_msg]).content
    return {"final_article": final_article, "next": "END"}