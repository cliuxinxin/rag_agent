# src/nodes/read_nodes.py
from langchain_core.messages import HumanMessage, SystemMessage
from src.nodes.common import get_llm
from src.prompts import get_context_caching_system_prompt, get_read_planner_prompt, get_read_writer_prompt
from src.state import AgentState

def planner_node(state: AgentState) -> dict:
    full_text = state["full_content"]
    qa_history = state.get("qa_pairs", [])
    loop = state.get("loop_count", 0)
    MAX_LOOPS = 4 
    
    llm = get_llm()
    history_text = "\n".join(qa_history) if qa_history else "ï¼ˆæš‚æ— ï¼Œè¿™æ˜¯ç¬¬ä¸€è½®åˆ†æï¼‰"
    
    # ä½¿ç”¨æå–å‡ºæ¥çš„ Prompt
    task_prompt = get_read_planner_prompt(loop, MAX_LOOPS, history_text)
    
    messages = [
        SystemMessage(content=get_context_caching_system_prompt(full_text)),
        HumanMessage(content=task_prompt)
    ]
    
    response = llm.invoke(messages).content.strip()
    question = response.replace('"', '').replace("'", "")
    
    if "TERMINATE" in response or loop >= MAX_LOOPS:
        return {"next": "Writer", "current_question": ""}
    else:
        return {
            "next": "Researcher", 
            "current_question": question, 
            "loop_count": loop + 1
        }

def researcher_node(state: AgentState) -> dict:
    full_text = state["full_content"]
    question = state["current_question"]
    
    llm = get_llm()
    
    task_prompt = f"""
    ã€å½“å‰å¾…æ”»å…‹çš„é—®é¢˜ã€‘
    {question}
    
    ã€å›ç­”è¦æ±‚ã€‘
    ä½ ä¸ä»…ä»…æ˜¯ä¸€ä¸ªæ‘˜å½•æœºå™¨ï¼Œä½ æ˜¯ä¸€ä¸ªæœ‰æ™ºæ…§çš„åˆ†æå¸ˆã€‚è¯·æŒ‰ä»¥ä¸‹æ­¥éª¤å›ç­”ï¼š
    
    1.  **åŸæ–‡è¯æ®**ï¼šé¦–å…ˆï¼Œä»æ–‡ä¸­æ‰¾åˆ°ç›¸å…³çš„æ®µè½ã€å¯¹è¯æˆ–æ•°æ®ä½œä¸ºä¾æ®ã€‚
    2.  **å¸¸è¯†èåˆ**ï¼š**å…³é”®æ­¥éª¤ï¼** è¯·è°ƒç”¨ä½ çš„å¤–éƒ¨çŸ¥è¯†åº“ï¼ˆå¸¸è¯†ã€åŒ»å­¦å¸¸è¯†ã€ç¤¾ä¼šè¿ä½œé€»è¾‘ã€æŠ€æœ¯åŸç†ï¼‰ï¼Œå¯¹åŸæ–‡å†…å®¹è¿›è¡Œè§£è¯»ã€‚
        - ä¾‹å¦‚ï¼šæ–‡ä¸­è¯´"ç™½ç»†èƒä½"ï¼Œä½ è¦ç»“åˆå¸¸è¯†æŒ‡å‡ºè¿™æ„å‘³ç€"ç—…æ¯’æ„ŸæŸ“ï¼Œå…ç–«ç³»ç»Ÿå—å‹åˆ¶"ã€‚
        - ä¾‹å¦‚ï¼šæ–‡ä¸­è¯´"æ‰¾å…³ç³»è¿›åŒ»é™¢"ï¼Œä½ è¦ç»“åˆå¸¸è¯†æŒ‡å‡ºè¿™åæ˜ äº†"åŒ»ç–—èµ„æºæŒ¤å…‘ä¸‹çš„ç¤¾ä¼šèµ„æœ¬åšå¼ˆ"ã€‚
    3.  **æ·±åº¦ç»“è®º**ï¼šç»¼åˆåŸæ–‡å’Œå¸¸è¯†ï¼Œç»™å‡ºè¿™ä¸ªé—®é¢˜çš„æ·±åˆ»ç­”æ¡ˆã€‚
    
    è¯·ç›´æ¥è¾“å‡ºå›ç­”å†…å®¹ã€‚
    """
    
    messages = [
        SystemMessage(content=get_context_caching_system_prompt(full_text)),
        HumanMessage(content=task_prompt)
    ]
    
    answer = llm.invoke(messages).content
    
    # è®°å½•è¿™ä¸€è½®çš„ Q&A
    qa_entry = f"â“ **Q**: {question}\nğŸ’¡ **A**: {answer}"
    
    return {
        "qa_pairs": [qa_entry], # ç´¯åŠ åˆ° state ä¸­
        "next": "Planner"       # å›å» Planner çœ‹çœ‹è¿˜éœ€è¦é—®ä»€ä¹ˆ
    }

def writer_node(state: AgentState) -> dict:
    full_text = state["full_content"]
    qa_history = state.get("qa_pairs", [])
    doc_title = state.get("doc_title", "æ–‡æ¡£")
    
    llm = get_llm()
    
    # å°† Planner å’Œ Researcher è¾›è‹¦å‡ è½®æŒ–æ˜å‡ºæ¥çš„"æ·±åº¦ç´ æ"æ‹¼æ¥èµ·æ¥
    history_text = "\n\n".join(qa_history)
    
    task_prompt = get_read_writer_prompt(doc_title, history_text)
    
    messages = [
        SystemMessage(content=get_context_caching_system_prompt(full_text)),
        HumanMessage(content=task_prompt)
    ]
    
    report = llm.invoke(messages).content
    
    return {
        "final_report": report,
        "next": "Outlooker" # ä¾ç„¶ä¿ç•™ Outlooker åšæœ€åçš„å»¶ä¼¸
    }

def outlook_node(state: AgentState) -> dict:
    full_text = state["full_content"]
    current_report = state["final_report"]
    # è·å–ç§¯ç´¯çš„æ‰€æœ‰é—®ç­”å¯¹ï¼ˆæ€è€ƒè¿‡ç¨‹ï¼‰
    qa_history = state.get("qa_pairs", [])
    
    llm = get_llm()
    
    # 1. ç”Ÿæˆ Outlook å†…å®¹ (ä¿æŒåŸæœ‰é€»è¾‘)
    task_prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªæå…¶æ³¨é‡å®ç”¨çš„å’¨è¯¢é¡¾é—®ã€‚
    è¯·é˜…è¯»å½“å‰çš„åˆ†ææŠ¥å‘Šï¼Œå¹¶å¢åŠ ä¸€ä¸ª **# ğŸš€ æ‰©å±•æ€è€ƒä¸èµ„æº** ç« èŠ‚ã€‚
    
    - å¦‚æœæ˜¯æ•…äº‹/æ¡ˆä¾‹ï¼šæ¨èç›¸å…³çš„ä¹¦ç±ã€ç”µå½±æˆ–æ€¥æ•‘çŸ¥è¯†ã€‚
    - å¦‚æœæ˜¯æŠ€æœ¯ï¼šæ¨èç›¸å…³çš„ GitHub åº“ã€æ›¿ä»£æ–¹æ¡ˆå¯¹æ¯”ã€‚
    
    è¯·ç›´æ¥è¾“å‡º Markdown å†…å®¹è¿½åŠ åˆ°æœ«å°¾ã€‚
    """
    
    messages = [
        SystemMessage(content=get_context_caching_system_prompt(full_text)),
        HumanMessage(content=task_prompt)
    ]
    
    outlook_content = llm.invoke(messages).content
    
    # 2. æ‹¼æ¥ï¼šåŸæŠ¥å‘Š + Outlook
    final_full_report = current_report + "\n\n" + outlook_content
    
    # === 3. æ–°å¢æ ¸å¿ƒé€»è¾‘ï¼šå°†æ€è€ƒè¿‡ç¨‹è¿½åŠ åˆ°æ–‡æœ« ===
    # ä½¿ç”¨ HTML <details> æ ‡ç­¾å®ç°æŠ˜å æ•ˆæœï¼Œæ—¢ä¿ç•™äº†æ•°æ®ï¼Œåˆä¸å½±å“é˜…è¯»ä½“éªŒ
    if qa_history:
        log_section = "\n\n---\n\n<details>\n<summary>ğŸ§  ç‚¹å‡»æŸ¥çœ‹ AI å®Œæ•´æ€è€ƒä¸æ¨æ¼”è¿‡ç¨‹ (Trace Logs)</summary>\n\n"
        
        log_section += "> ä»¥ä¸‹è®°å½•äº† Agent ä»é˜…è¯»åˆ°æé—®ã€å†åˆ°ç»“åˆå¸¸è¯†æ¨ç†çš„å®Œæ•´æ€ç»´é“¾ã€‚\n\n"
        
        for i, pair in enumerate(qa_history):
            # pair çš„æ ¼å¼å·²ç»æ˜¯ "â“ Q: ... \nğŸ’¡ A: ..."
            # æˆ‘ä»¬ç¨å¾®ç¾åŒ–ä¸€ä¸‹æ ¼å¼
            log_section += f"#### ğŸ”„ ç¬¬ {i+1} è½®æ€è€ƒ\n"
            log_section += f"{pair}\n\n"
            
        log_section += "</details>\n"
        
        final_full_report += log_section
    
    return {
        "final_report": final_full_report,
        "next": "END"
    }