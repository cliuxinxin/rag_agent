# src/nodes/chat_nodes.py
from typing import Literal
from pydantic import BaseModel, Field
from langchain_core.messages import SystemMessage, AIMessage, HumanMessage
from langchain_core.output_parsers import PydanticOutputParser
from src.state import AgentState
from src.nodes.common import get_llm
from src.logger import get_logger
from src.bm25 import SimpleBM25Retriever
from src.storage import peek_kb_random_chunks

# èŽ·å– logger å®žä¾‹
logger = get_logger("Node_Chat")

# === Supervisor ===

class RouteResponse(BaseModel):
    observed_gap: str = Field(..., description="åˆ†æžä¿¡æ¯ç¼ºå£")
    next: Literal["Searcher", "Answerer"] = Field(..., description="ä¸‹ä¸€æ­¥è§’è‰²")
    search_query: str = Field(default="", description="æœç´¢æŒ‡ä»¤")
    reasoning: str = Field(..., description="ç†ç”±")

def supervisor_node(state: AgentState) -> dict:
    messages = state["messages"]
    current_loop = state.get("loop_count", 0)
    past_searches = state.get("attempted_searches", [])
    failed_topics = state.get("failed_topics", [])
    
    # [Log] è®°å½•è¿›å…¥èŠ‚ç‚¹
    logger.info(f"======== [Supervisor] è¿›å…¥ç¬¬ {current_loop} è½®æ€è€ƒ ========")
    
    MAX_LOOPS = 6 
    llm = get_llm()
    
    history_str = "\n".join([f"- {q}" for q in past_searches]) if past_searches else "æ— "
    failed_str = "\n".join([f"- {q}" for q in failed_topics]) if failed_topics else "æ— "

    if current_loop >= MAX_LOOPS:
        logger.warning(f"[Supervisor] è¾¾åˆ°æœ€å¤§å¾ªçŽ¯æ¬¡æ•° {MAX_LOOPS}ï¼Œå¼ºåˆ¶ç»“æŸã€‚")
        return {"next": "Answerer", "current_search_query": "", "loop_count": current_loop}

    parser = PydanticOutputParser(pydantic_object=RouteResponse)
    format_instructions = parser.get_format_instructions()

    # è¿™é‡Œå»ºè®®ä»¥åŽä¹Ÿæå–åˆ° src/prompts.py
    system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªå…¨èƒ½åž‹çš„ç ”ç©¶é¡¹ç›®ä¸»ç®¡ã€‚
    å½“å‰ç ”ç©¶è½®æ¬¡ï¼š{current_loop + 1} / {MAX_LOOPS}ã€‚
    ã€å·²å°è¯•çš„æœç´¢ã€‘{history_str}
    ã€âŒ æ— ç»“æžœè¯é¢˜ã€‘{failed_str}
    
    è¯·åˆ†æžçŽ°çŠ¶ï¼Œè¯†åˆ«ä¿¡æ¯ç¼ºå£ï¼ŒæŒ‡æ´¾ Searcher æˆ– Answererã€‚
    {format_instructions}
    """
    
    try:
        response = llm.invoke([SystemMessage(content=system_prompt)] + messages)
        content = response.content.strip().replace("```json", "").replace("```", "")
        decision = parser.parse(content)
        
        # [Log] å…³é”®å†³ç­–æ—¥å¿—
        logger.info(f"[Supervisor] å†³ç­–: {decision.next} | ç†ç”±: {decision.reasoning[:50]}...")
        if decision.next == "Searcher":
            logger.info(f"[Supervisor] æŒ‡æ´¾æœç´¢è¯: '{decision.search_query}'")
            
    except Exception as e:
        logger.error(f"[Supervisor] è§£æžé”™è¯¯æˆ– LLM å¼‚å¸¸: {e}", exc_info=True)
        decision = RouteResponse(
            observed_gap="Error", next="Answerer", search_query="", reasoning="System Error"
        )

    return {
        "next": decision.next,
        "current_search_query": decision.search_query,
        "loop_count": current_loop + 1
    }

# === Searcher ===

def search_node(state: AgentState) -> dict:
    query = state.get("current_search_query", "")
    source_docs = state.get("source_documents", [])
    vector_store = state.get("vector_store", None)
    kb_names = state.get("kb_names", [])
    # èŽ·å–å½“å‰çš„åŠ¨æ€ç”»åƒ
    current_summary = state.get("kb_summary", "æœªçŸ¥é¢†åŸŸ")
    
    # [Log] è®°å½•æœç´¢åŠ¨ä½œ
    logger.info(f"[Searcher] å¼€å§‹æ‰§è¡Œæœç´¢ä»»åŠ¡: '{query}' | å½“å‰å¯¹åº“çš„ç†è§£: {current_summary}")
    
    if not query:
        logger.warning("[Searcher] æ”¶åˆ°ç©ºæŸ¥è¯¢æŒ‡ä»¤")
        return {"messages": [AIMessage(content="Searcher: æŒ‡ä»¤ä¸ºç©ºã€‚", name="Searcher")]}

    llm = get_llm()

    # === 1. [æ ¸å¿ƒé€šç”¨é€»è¾‘] èŽ·å–æ ·æœ¬ ===
    # æ— è®º current_summary æ˜¯å¦ä¸ºç©ºï¼Œéƒ½èŽ·å–æ ·æœ¬ï¼Œå¢žå¼º Prompt çš„"ä½“æ„Ÿ"
    # è¿™æ­¥æ“ä½œéžå¸¸å¿«ï¼ˆæ¯«ç§’çº§ï¼‰ï¼Œä¸ä¼šå½±å“æ€§èƒ½
    kb_preview_text = peek_kb_random_chunks(kb_names, sample_size=3)
    
    logger.info(f"[Searcher] æ­£åœ¨åŸºäºŽé‡‡æ ·å†…å®¹ç”Ÿæˆå…³é”®è¯...")

    # === 2. é€šç”¨åž‹ Promptï¼šä¸é¢„è®¾ä»»ä½•ç«‹åœºï¼Œåªåš"ç¿»è¯‘" ===
    # ç»“åˆé‡‡æ ·å’Œ summaryï¼ˆå¦‚æžœæœ‰çš„è¯ï¼‰ï¼ŒåŒé‡ä¼˜åŒ–
    summary_context = ""
    if current_summary and len(current_summary) > 10 and "æœªçŸ¥" not in current_summary and "æš‚æ—¶æœªçŸ¥" not in current_summary:
        summary_context = f"\nã€è¡¥å……ï¼šæˆ‘ä»¬ä¹‹å‰äº†è§£åˆ°è¿™ä¸ªçŸ¥è¯†åº“ã€‘{current_summary}\n"
    
    expansion_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„"æœ¯è¯­å¯¹é½"ä¸“å®¶ã€‚

ã€ä»»åŠ¡ã€‘
ç”¨æˆ·æƒ³æœç´¢ï¼š"{query}"

ã€çŸ¥è¯†åº“å®žåœ°é‡‡æ ·ã€‘
(ä»¥ä¸‹æ˜¯ä»Žæ•°æ®åº“ä¸­éšæœºæŠ½å–çš„ 3 ä¸ªç‰‡æ®µï¼Œè¯·ä»”ç»†è§‚å¯Ÿå…¶**å¹´ä»£ã€è¯­ä½“ã€ä¸“ä¸šæœ¯è¯­**)
--- é‡‡æ ·å¼€å§‹ ---
{kb_preview_text}
--- é‡‡æ ·ç»“æŸ ---
{summary_context}
ã€æŒ‡ä»¤ã€‘
1. è¯·æ¨¡ä»¿ã€çŸ¥è¯†åº“é‡‡æ ·ã€‘çš„è¡Œæ–‡é£Žæ ¼å’Œç”¨è¯ä¹ æƒ¯ã€‚
2. å°†ç”¨æˆ·çš„æœç´¢æ„å›¾**ç¿»è¯‘**æˆæœ€å¯èƒ½å‡ºçŽ°åœ¨è¯¥æ•°æ®åº“ä¸­çš„ 3-4 ä¸ªå…³é”®è¯ã€‚
3. **ä¸¥ç¦ä½¿ç”¨çŽ°ä»£è¯æ±‡**ï¼Œé™¤éžé‡‡æ ·ä¸­å‡ºçŽ°äº†çŽ°ä»£è¯æ±‡ã€‚
   - å¦‚æžœé‡‡æ ·æ˜¯å¤æ–‡ï¼Œå°±ç”¨å¤æ–‡è¯ã€‚
   - å¦‚æžœé‡‡æ ·æ˜¯ä»£ç ï¼Œå°±ç”¨ç±»åã€å‡½æ•°åã€‚

è¯·ç›´æŽ¥è¾“å‡ºå…³é”®è¯ï¼Œç”¨ç©ºæ ¼åˆ†éš”ï¼š"""
    
    try:
        bm25_keywords = llm.invoke([HumanMessage(content=expansion_prompt)]).content.strip().replace('"', '').replace('\n', ' ')
        logger.info(f"[Searcher] é‡‡æ ·å¯¹é½åŽçš„å…³é”®è¯: {bm25_keywords}")
    except Exception as e:
        logger.error(f"[Searcher] å…³é”®è¯ç”Ÿæˆå¤±è´¥: {e}")
        bm25_keywords = query
    
    results_bm25 = []
    results_vector = []

    if source_docs:
        try:
            bm25_retriever = SimpleBM25Retriever(source_docs)
            results_bm25 = bm25_retriever.search(f"{query} {bm25_keywords}", k=10)
        except Exception as e:
            logger.warning(f"BM25 æ£€ç´¢å¤±è´¥: {e}")
            results_bm25 = []
    
    if vector_store:
        try:
            vector_retriever = vector_store.as_retriever(search_kwargs={"k": 10})
            results_vector = vector_retriever.invoke(query)
        except: pass

    # åˆå¹¶åŽ»é‡
    all_results = results_vector + results_bm25
    unique_docs = {}
    for doc in all_results:
        if doc.page_content not in unique_docs:
            unique_docs[doc.page_content] = doc
    
    final_docs = list(unique_docs.values())[:6]
    
    logger.info(f"[Searcher] æ£€ç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(final_docs)} æ¡ç›¸å…³ç‰‡æ®µ")

    if not final_docs:
        logger.warning(f"[Searcher] æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ï¼ŒæŸ¥è¯¢: '{query}'")
        return {
            "messages": [AIMessage(content=f"Searcher: æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚", name="Searcher")],
            "attempted_searches": [query],
            "failed_topics": [query],
            "kb_summary": current_summary  # ä¿æŒåŽŸæœ‰ç”»åƒ
        }

    # === 3. [æ ¸å¿ƒæ–°å¢ž] åŠ¨æ€æ›´æ–°çŸ¥è¯†åº“ç”»åƒ (Learn from Docs) ===
    new_summary = current_summary
    if final_docs:
        # æå–è¿™æ¬¡æ£€ç´¢åˆ°çš„å†…å®¹çš„æ‘˜è¦
        content_preview = "\n".join([d.page_content[:200] for d in final_docs[:3]])
        
        update_profile_prompt = f"""æˆ‘ä»¬è¦ç»´æŠ¤ä¸€ä¸ª"çŸ¥è¯†åº“ç”»åƒ"ï¼Œé€šè¿‡é˜…è¯»æ£€ç´¢åˆ°çš„ç‰‡æ®µæ¥ä¸æ–­ä¿®æ­£æˆ‘ä»¬å¯¹è¿™ä¸ªçŸ¥è¯†åº“çš„è®¤çŸ¥ã€‚

ã€æ—§ç”»åƒã€‘{current_summary}
ã€æ–°æ£€ç´¢åˆ°çš„ç‰‡æ®µã€‘
{content_preview}

è¯·ç»“åˆã€æ–°ç‰‡æ®µã€‘ï¼Œç”¨ä¸€å¥è¯æ›´æ–°ã€æ—§ç”»åƒã€‘ã€‚
æè¿°è¿™ä¸ªçŸ¥è¯†åº“ä¸»è¦æ˜¯å…³äºŽä»€ä¹ˆé¢†åŸŸçš„ï¼ŸåŒ…å«å“ªäº›æ ¸å¿ƒæŠ€æœ¯æ ˆæˆ–ä¸šåŠ¡ï¼Ÿ
ä¸è¦å¤ªé•¿ï¼Œåªä¿ç•™æ ¸å¿ƒç‰¹å¾ã€‚"""
        
        # è¿™æ˜¯ä¸€ä¸ªåŽå°"å­¦ä¹ "è¿‡ç¨‹ï¼Œä¸åº”è¯¥é˜»å¡žå¤ªä¹…ï¼Œä½†ä¸ºäº†æ•ˆæžœæˆ‘ä»¬åŒæ­¥æ‰§è¡Œ
        try:
            new_summary = llm.invoke([HumanMessage(content=update_profile_prompt)]).content.strip()
            logger.info(f"[Learning] çŸ¥è¯†åº“ç”»åƒå·²æ›´æ–°: {new_summary}")
        except Exception as e:
            logger.error(f"[Learning] ç”»åƒæ›´æ–°å¤±è´¥: {e}")
            new_summary = current_summary

    # === 4. ç”Ÿæˆç¬”è®° (ä¿æŒåŽŸæœ‰é€»è¾‘) ===
    context_text = "\n\n".join([f"[Ref {i+1}] {d.page_content}" for i, d in enumerate(final_docs)])
    filter_prompt = f"ä»»åŠ¡: '{query}'\nèµ„æ–™:\n{context_text}\nè¯·æå–å…³é”®ä¿¡æ¯ã€‚"
    extraction = llm.invoke([HumanMessage(content=filter_prompt)]).content
    
    current_note = f"### ðŸ” æœç´¢ä¸»é¢˜: {query} (å…³é”®è¯: {bm25_keywords})\n{extraction}\n"
    
    logger.info(f"[Searcher] ç¬”è®°æå–å®Œæˆï¼Œå‡†å¤‡è¿”å›žã€‚")

    return {
        "messages": [AIMessage(content=f"ã€æœç´¢æŠ¥å‘Šã€‘\næ–¹å‘: {query}\nå‘çŽ°:\n{extraction}", name="Searcher")],
        "final_evidence": final_docs,
        "attempted_searches": [query],
        "research_notes": [current_note],
        # [æ–°å¢ž] å°†æ›´æ–°åŽçš„ç”»åƒå›žå†™åˆ°çŠ¶æ€ä¸­ï¼Œä¾›ä¸‹ä¸€è½®ä½¿ç”¨
        "kb_summary": new_summary
    }

# === Answerer ===

def answer_node(state: AgentState) -> dict:
    logger.info("[Answerer] å¼€å§‹ç”Ÿæˆæœ€ç»ˆå›žç­”...")
    
    messages = state["messages"]
    evidences = state.get("final_evidence", [])
    notes = state.get("research_notes", [])
    llm = get_llm()
    
    notes_text = "ã€ðŸ•µï¸â€â™‚ï¸ è°ƒæŸ¥ç¬”è®°ã€‘\n" + "\n".join(notes) if notes else "æ— è°ƒæŸ¥è®°å½•ã€‚"
    evidence_text = "ã€ðŸ“š åŽŸå§‹ç‰‡æ®µã€‘\n"
    for i, doc in enumerate(evidences):
        evidence_text += f"> [Ref {i+1}] {doc.page_content[:200]}...\n"

    system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ¥è¯†åº“åŠ©æ‰‹ã€‚
    è¯·åŸºäºŽã€è°ƒæŸ¥ç¬”è®°ã€‘å’Œã€åŽŸå§‹ç‰‡æ®µã€‘å›žç­”ç”¨æˆ·é—®é¢˜ã€‚
    {notes_text}
    {evidence_text}
    ä¸¥è°¨å¼•ç”¨ [Ref X]ã€‚
    """
    
    try:
        response = llm.invoke([SystemMessage(content=system_prompt)] + messages)
        
        logger.info(f"[Answerer] å›žç­”ç”Ÿæˆå®Œæ¯• (é•¿åº¦: {len(response.content)})")
    except Exception as e:
        logger.error(f"[Answerer] ç”Ÿæˆå›žç­”å¤±è´¥: {e}", exc_info=True)
        raise e
    
    # æ‹¼æŽ¥é™„å½•ä¾›å‰ç«¯æ˜¾ç¤º
    appendix = "\n\n"
    if notes: appendix += "ã€ðŸ•µï¸â€â™‚ï¸ è°ƒæŸ¥ç¬”è®°ã€‘\n" + "\n".join(notes) + "\n\n"
    if evidences:
        appendix += "ã€ðŸ“š åŽŸå§‹ç‰‡æ®µã€‘\n"
        for i, doc in enumerate(evidences):
            appendix += f"> [Ref {i+1}] {doc.page_content[:350]}...\n(Source: {doc.metadata.get('source','Unknown')})\n\n"
    
    response.content += appendix
    return {"messages": [response], "next": "END"}