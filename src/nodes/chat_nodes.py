# src/nodes/chat_nodes.py
from typing import Literal
from pydantic import BaseModel, Field
from langchain_core.messages import SystemMessage, AIMessage, HumanMessage
from langchain_core.output_parsers import PydanticOutputParser
from src.state import AgentState
from src.nodes.common import get_llm
from src.logger import get_logger

# èŽ·å– logger å®žä¾‹
logger = get_logger("Node_Chat")

# === Supervisor ===

class RouteResponse(BaseModel):
    observed_gap: str = Field(..., description="åˆ†æžä¿¡æ¯ç¼ºå£")
    next: Literal["Searcher", "Answerer"] = Field(..., description="ä¸‹ä¸€æ­¥è§’è‰²")
    search_query: str = Field(default="", description="æœç´¢æŒ‡ä»¤")
    reasoning: str = Field(..., description="ç†ç”±")

def supervisor_node(state: AgentState) -> dict:
    messages = state["messages"]
    current_loop = state.get("loop_count", 0)
    past_searches = state.get("attempted_searches", [])
    failed_topics = state.get("failed_topics", [])
    
    # [Log] è®°å½•è¿›å…¥èŠ‚ç‚¹
    logger.info(f"======== [Supervisor] è¿›å…¥ç¬¬ {current_loop} è½®æ€è€ƒ ========")
    
    MAX_LOOPS = 6 
    llm = get_llm()
    
    history_str = "\n".join([f"- {q}" for q in past_searches]) if past_searches else "æ— "
    failed_str = "\n".join([f"- {q}" for q in failed_topics]) if failed_topics else "æ— "

    if current_loop >= MAX_LOOPS:
        logger.warning(f"[Supervisor] è¾¾åˆ°æœ€å¤§å¾ªçŽ¯æ¬¡æ•° {MAX_LOOPS}ï¼Œå¼ºåˆ¶ç»“æŸã€‚")
        return {"next": "Answerer", "current_search_query": "", "loop_count": current_loop}

    parser = PydanticOutputParser(pydantic_object=RouteResponse)
    format_instructions = parser.get_format_instructions()

    # è¿™é‡Œå»ºè®®ä»¥åŽä¹Ÿæå–åˆ° src/prompts.py
    system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªå…¨èƒ½åž‹çš„ç ”ç©¶é¡¹ç›®ä¸»ç®¡ã€‚
    å½“å‰ç ”ç©¶è½®æ¬¡ï¼š{current_loop + 1} / {MAX_LOOPS}ã€‚
    ã€å·²å°è¯•çš„æœç´¢ã€‘{history_str}
    ã€âŒ æ— ç»“æžœè¯é¢˜ã€‘{failed_str}
    
    è¯·åˆ†æžçŽ°çŠ¶ï¼Œè¯†åˆ«ä¿¡æ¯ç¼ºå£ï¼ŒæŒ‡æ´¾ Searcher æˆ– Answererã€‚
    {format_instructions}
    """
    
    try:
        response = llm.invoke([SystemMessage(content=system_prompt)] + messages)
        content = response.content.strip().replace("```json", "").replace("```", "")
        decision = parser.parse(content)
        
        # [Log] å…³é”®å†³ç­–æ—¥å¿—
        logger.info(f"[Supervisor] å†³ç­–: {decision.next} | ç†ç”±: {decision.reasoning[:50]}...")
        if decision.next == "Searcher":
            logger.info(f"[Supervisor] æŒ‡æ´¾æœç´¢è¯: '{decision.search_query}'")
            
    except Exception as e:
        logger.error(f"[Supervisor] è§£æžé”™è¯¯æˆ– LLM å¼‚å¸¸: {e}", exc_info=True)
        decision = RouteResponse(
            observed_gap="Error", next="Answerer", search_query="", reasoning="System Error"
        )

    return {
        "next": decision.next,
        "current_search_query": decision.search_query,
        "loop_count": current_loop + 1
    }

# === Searcher ===

def search_node(state: AgentState) -> dict:
    query = state.get("current_search_query", "")
    source_docs = state.get("source_documents", [])
    vector_store = state.get("vector_store", None)
    
    # [Log] è®°å½•æœç´¢åŠ¨ä½œ
    logger.info(f"[Searcher] å¼€å§‹æ‰§è¡Œæœç´¢ä»»åŠ¡: '{query}'")
    
    if not query:
        logger.warning("[Searcher] æ”¶åˆ°ç©ºæŸ¥è¯¢æŒ‡ä»¤")
        return {"messages": [AIMessage(content="Searcher: æŒ‡ä»¤ä¸ºç©ºã€‚", name="Searcher")]}

    llm = get_llm()

    # ç®€å•æ‰©å……å…³é”®è¯
    expansion_prompt = f"é’ˆå¯¹æœç´¢æ„å›¾ '{query}'ï¼Œç”Ÿæˆ 3-4 ä¸ªå…³é”®è¯ï¼Œç©ºæ ¼åˆ†éš”ã€‚"
    try:
        bm25_keywords = llm.invoke([HumanMessage(content=expansion_prompt)]).content.strip().replace('"', '')
        logger.info(f"[Searcher] æ‰©å±•å…³é”®è¯: {bm25_keywords}")
    except Exception as e:
        logger.error(f"[Searcher] å…³é”®è¯æ‰©å±•å¤±è´¥: {e}")
        bm25_keywords = query
    
    results_bm25 = []
    results_vector = []

    if source_docs:
        try:
            bm25_retriever = BM25Retriever.from_documents(source_docs)
            bm25_retriever.k = 10 
            results_bm25 = bm25_retriever.invoke(f"{query} {bm25_keywords}")
        except: pass
    
    if vector_store:
        try:
            vector_retriever = vector_store.as_retriever(search_kwargs={"k": 10})
            results_vector = vector_retriever.invoke(query)
        except: pass

    # åˆå¹¶åŽ»é‡
    all_results = results_vector + results_bm25
    unique_docs = {}
    for doc in all_results:
        if doc.page_content not in unique_docs:
            unique_docs[doc.page_content] = doc
    
    final_docs = list(unique_docs.values())[:6]
    
    logger.info(f"[Searcher] æ£€ç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(final_docs)} æ¡ç›¸å…³ç‰‡æ®µ")

    if not final_docs:
        logger.warning(f"[Searcher] æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ï¼ŒæŸ¥è¯¢: '{query}'")
        return {
            "messages": [AIMessage(content=f"Searcher: æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚", name="Searcher")],
            "attempted_searches": [query],
            "failed_topics": [query]
        }

    # ç¬”è®°ç”Ÿæˆ
    context_text = "\n\n".join([f"[Ref {i+1}] {d.page_content}" for i, d in enumerate(final_docs)])
    filter_prompt = f"ä»»åŠ¡: '{query}'\nèµ„æ–™:\n{context_text}\nè¯·æå–å…³é”®ä¿¡æ¯ã€‚"
    extraction = llm.invoke([HumanMessage(content=filter_prompt)]).content
    
    current_note = f"### ðŸ” æœç´¢ä¸»é¢˜: {query} (å…³é”®è¯: {bm25_keywords})\n{extraction}\n"
    
    logger.info(f"[Searcher] ç¬”è®°æå–å®Œæˆï¼Œå‡†å¤‡è¿”å›žã€‚")

    return {
        "messages": [AIMessage(content=f"ã€æœç´¢æŠ¥å‘Šã€‘\næ–¹å‘: {query}\nå‘çŽ°:\n{extraction}", name="Searcher")],
        "final_evidence": final_docs,
        "attempted_searches": [query],
        "research_notes": [current_note]
    }

# === Answerer ===

def answer_node(state: AgentState) -> dict:
    logger.info("[Answerer] å¼€å§‹ç”Ÿæˆæœ€ç»ˆå›žç­”...")
    
    messages = state["messages"]
    evidences = state.get("final_evidence", [])
    notes = state.get("research_notes", [])
    llm = get_llm()
    
    notes_text = "ã€ðŸ•µï¸â€â™‚ï¸ è°ƒæŸ¥ç¬”è®°ã€‘\n" + "\n".join(notes) if notes else "æ— è°ƒæŸ¥è®°å½•ã€‚"
    evidence_text = "ã€ðŸ“š åŽŸå§‹ç‰‡æ®µã€‘\n"
    for i, doc in enumerate(evidences):
        evidence_text += f"> [Ref {i+1}] {doc.page_content[:200]}...\n"

    system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ¥è¯†åº“åŠ©æ‰‹ã€‚
    è¯·åŸºäºŽã€è°ƒæŸ¥ç¬”è®°ã€‘å’Œã€åŽŸå§‹ç‰‡æ®µã€‘å›žç­”ç”¨æˆ·é—®é¢˜ã€‚
    {notes_text}
    {evidence_text}
    ä¸¥è°¨å¼•ç”¨ [Ref X]ã€‚
    """
    
    try:
        response = llm.invoke([SystemMessage(content=system_prompt)] + messages)
        
        logger.info(f"[Answerer] å›žç­”ç”Ÿæˆå®Œæ¯• (é•¿åº¦: {len(response.content)})")
    except Exception as e:
        logger.error(f"[Answerer] ç”Ÿæˆå›žç­”å¤±è´¥: {e}", exc_info=True)
        raise e
    
    # æ‹¼æŽ¥é™„å½•ä¾›å‰ç«¯æ˜¾ç¤º
    appendix = "\n\n"
    if notes: appendix += "ã€ðŸ•µï¸â€â™‚ï¸ è°ƒæŸ¥ç¬”è®°ã€‘\n" + "\n".join(notes) + "\n\n"
    if evidences:
        appendix += "ã€ðŸ“š åŽŸå§‹ç‰‡æ®µã€‘\n"
        for i, doc in enumerate(evidences):
            appendix += f"> [Ref {i+1}] {doc.page_content[:350]}...\n(Source: {doc.metadata.get('source','Unknown')})\n\n"
    
    response.content += appendix
    return {"messages": [response], "next": "END"}