import json
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
from langgraph.graph import StateGraph, END
from src.state import WriterState
from src.nodes import get_llm
from src.storage import load_kbs
from langchain_community.retrievers import BM25Retriever

# ==========================================
# PART 1: è°ƒç ”ä¸å¤§çº²ç”Ÿæˆ Agent
# ==========================================

def plan_node(state: WriterState) -> dict:
    """è§„åˆ’å¸ˆï¼šåˆ†æéœ€æ±‚æˆ–ç°æœ‰å¤§çº²ï¼Œå†³å®šéœ€è¦è°ƒç ”ä»€ä¹ˆæ–¹å‘"""
    req = state["user_requirement"]
    outline = state.get("current_outline", [])
    loop = state.get("loop_count", 0)
    
    # é™åˆ¶è°ƒç ”è½®æ¬¡ï¼ˆä¾‹å¦‚æœ€å¤šæŸ¥ 3 æ¬¡ï¼‰
    if loop >= 3:
        return {"next": "ReportGenerator"}

    llm = get_llm()
    
    # å¦‚æœå·²æœ‰å¤§çº²ï¼ˆè¯´æ˜æ˜¯ä¿®æ”¹åé‡æ–°è°ƒç ”ï¼‰ï¼Œåˆ™åŸºäºå¤§çº²è§„åˆ’
    if outline:
        outline_str = "\n".join([f"- {item['title']}: {item['desc']}" for item in outline])
        prompt = f"""
        å½“å‰ä»»åŠ¡ï¼šå®Œå–„å†™ä½œé¡¹ç›®çš„è°ƒç ”ã€‚
        ç”¨æˆ·éœ€æ±‚ï¼š{req}
        å½“å‰å¤§çº²ï¼š
        {outline_str}
        
        è¯·åˆ†æå¤§çº²ä¸­å“ªäº›éƒ¨åˆ†ç¼ºä¹äº‹å®æ”¯æ’‘ï¼Ÿæå‡º 2-3 ä¸ªå…·ä½“çš„æœç´¢å…³é”®è¯æˆ–é—®é¢˜ï¼Œç”¨äºè¡¥å……è°ƒç ”ã€‚
        """
    else:
        # ç¬¬ä¸€æ¬¡ç”Ÿæˆï¼ŒåŸºäºéœ€æ±‚è§„åˆ’
        prompt = f"""
        å½“å‰ä»»åŠ¡ï¼šä¸ºå†™ä½œé¡¹ç›®åšå‰æœŸè°ƒç ”ã€‚
        ç”¨æˆ·éœ€æ±‚ï¼š{req}
        
        è¯·åˆ†æä¸ºäº†å†™å¥½è¿™ç¯‡æ–‡ç« ï¼Œæˆ‘ä»¬éœ€è¦æœé›†å“ªäº›æ ¸å¿ƒä¿¡æ¯ï¼Ÿ
        è¯·è¾“å‡º 2-3 ä¸ªå…·ä½“çš„æœç´¢å…³é”®è¯æˆ–æ–¹å‘ã€‚
        """
    
    # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œç›´æ¥è®© LLM è¾“å‡ºå…³é”®è¯ï¼Œå®é™…å¯ä»¥åšæˆ Structured Output
    plan = llm.invoke([HumanMessage(content=prompt)]).content
    
    return {
        "planning_steps": [plan],
        "next": "Researcher",
        "loop_count": loop + 1
    }

def research_node(state: WriterState) -> dict:
    """ç ”ç©¶å‘˜ï¼šæ‰§è¡Œæœç´¢"""
    source_type = state["source_type"]
    source_data = state["source_data"]
    plans = state.get("planning_steps", [])
    latest_plan = plans[-1] if plans else ""
    
    # 1. è·å–ä¸Šä¸‹æ–‡ï¼ˆçŸ¥è¯†åº“æ£€ç´¢ æˆ– é™æ€æ–‡æœ¬ï¼‰
    retrieved_text = ""
    
    if source_type == "kb":
        # è§£æ KB åˆ—è¡¨
        try:
            kb_list = json.loads(source_data) if isinstance(source_data, str) else source_data
            docs, _ = load_kbs(kb_list)
            if docs:
                # åŸºäº Plan è¿›è¡Œæ£€ç´¢
                retriever = BM25Retriever.from_documents(docs)
                retriever.k = 5
                # ç®€å•æå– Plan ä¸­çš„å…³é”®è¯è¿›è¡Œæ£€ç´¢
                results = retriever.invoke(latest_plan)
                retrieved_text = "\n".join([f"[KB Ref] {d.page_content}" for d in results])
        except Exception as e:
            print(f"Research Error: {e}")
            
    elif source_type in ["file", "text"]:
        # å¯¹äºä¸Šä¼ æ–‡ä»¶ï¼Œå…¨æ–‡ä½œä¸ºä¸Šä¸‹æ–‡ï¼ˆå¦‚æœä¸é•¿ï¼‰ï¼Œæˆ–è€…åšç®€å•åˆ‡ç‰‡æ£€ç´¢
        # è¿™é‡Œä¸ºäº†ç®€å•ï¼Œå‡è®¾ text æ¨¡å¼ä¸‹ source_data å°±æ˜¯å…¨æ–‡
        full_text = source_data
        if len(full_text) > 10000:
            # ç®€å•æˆªæ–­æˆ–åšç®€å•æŸ¥æ‰¾
            retrieved_text = full_text[:10000] 
        else:
            retrieved_text = full_text

    if not retrieved_text:
        retrieved_text = "æœªæ‰¾åˆ°ç›¸å…³èµ„æ–™ã€‚"

    # 2. æ€»ç»“å‘ç°
    llm = get_llm()
    summary_prompt = f"""
    æ ¹æ®æœç´¢è®¡åˆ’ï¼š{latest_plan}
    æˆ‘ä»¬æ£€ç´¢åˆ°äº†ä»¥ä¸‹å†…å®¹ï¼š
    {retrieved_text[:3000]}
    
    è¯·æå–å¯¹å†™ä½œæœ‰å¸®åŠ©çš„äº‹å®ã€æ•°æ®æˆ–è§‚ç‚¹ï¼Œå½¢æˆä¸€æ¡è°ƒç ”ç¬”è®°ã€‚
    """
    note = llm.invoke([HumanMessage(content=summary_prompt)]).content
    
    return {
        "research_notes": state.get("research_notes", []) + [note],
        "next": "PlanCheck" # æœå®Œä¸€æ¬¡ï¼Œå›å»æ£€æŸ¥æ˜¯å¦è¿˜éœ€è¦æœ
    }

def plan_check_node(state: WriterState) -> dict:
    """ç®€å•çš„å¾ªç¯æ§åˆ¶å™¨"""
    loop = state.get("loop_count", 0)
    # æ¯”å¦‚æˆ‘ä»¬ç¡¬æ€§è§„å®šæœ 2 è½®å°±å¤Ÿäº†
    if loop >= 2:
        return {"next": "ReportGenerator"}
    return {"next": "Planner"} # å›å» Planner ç»§ç»­è§„åˆ’

def report_node(state: WriterState) -> dict:
    """æŠ¥å‘Šç”Ÿæˆå™¨ï¼šæ±‡æ€»æ‰€æœ‰ç¬”è®°"""
    req = state["user_requirement"]
    notes = state.get("research_notes", [])
    
    llm = get_llm()
    
    notes_text = "\n\n".join(notes)
    
    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªé«˜çº§åˆ†æå¸ˆã€‚
    
    ã€å†™ä½œéœ€æ±‚ã€‘{req}
    ã€ç´¯è®¡è°ƒç ”ç¬”è®°ã€‘
    {notes_text}
    
    è¯·æ ¹æ®ä»¥ä¸Šä¿¡æ¯ï¼Œæ’°å†™ä¸€ä»½ç»“æ„æ¸…æ™°çš„ã€Šæ·±åº¦è°ƒç ”æŠ¥å‘Šã€‹ã€‚
    åŒ…å«ï¼šæ ¸å¿ƒä¸»æ—¨ã€å…³é”®è®ºæ®ã€ç´ æå‚¨å¤‡ã€å»ºè®®çš„å™äº‹è§’åº¦ã€‚
    """
    
    report = llm.invoke([HumanMessage(content=prompt)]).content
    return {"research_report": report, "next": "Outliner"}

def outline_node(state: WriterState) -> dict:
    """å¤§çº²ç”Ÿæˆå™¨ï¼šå¢å¼ºå®¹é”™èƒ½åŠ›"""
    req = state["user_requirement"]
    report = state["research_report"]
    
    llm = get_llm()
    
    prompt = f"""
    åŸºäºè°ƒç ”æŠ¥å‘Šï¼Œè®¾è®¡æ–‡ç« å¤§çº²ã€‚
    
    ã€éœ€æ±‚ã€‘{req}
    ã€æŠ¥å‘Šã€‘{report[:3000]} 
    
    è¯·è¾“å‡ºä¸¥æ ¼çš„ JSON æ ¼å¼ï¼ˆList[Dict]ï¼‰ï¼ŒåŒ…å« title, descã€‚
    ä¸è¦è¾“å‡ºä»»ä½• Markdown æ ‡è®°ï¼ˆå¦‚ ```jsonï¼‰ï¼Œåªè¾“å‡ºçº¯ JSON æ–‡æœ¬ã€‚
    
    ç¤ºä¾‹æ ¼å¼ï¼š
    [
        {{"title": "ç¬¬ä¸€ç« ï¼š...", "desc": "..."}},
        {{"title": "ç¬¬äºŒç« ï¼š...", "desc": "..."}}
    ]
    """
    
    res = llm.invoke([HumanMessage(content=prompt)]).content
    
    # === å¢å¼ºæ¸…æ´—é€»è¾‘ ===
    clean_json = res.replace("```json", "").replace("```", "").strip()
    # æœ‰æ—¶å€™ LLM ä¼šåœ¨å¼€å¤´åŠ  "Here is the json..."ï¼Œæˆ‘ä»¬è¦å°è¯•æ‰¾åˆ°ç¬¬ä¸€ä¸ª [
    start_idx = clean_json.find("[")
    end_idx = clean_json.rfind("]")
    
    new_outline = []
    
    if start_idx != -1 and end_idx != -1:
        clean_json = clean_json[start_idx : end_idx + 1]
        try:
            new_outline = json.loads(clean_json)
        except json.JSONDecodeError as e:
            print(f"JSON Parse Error: {e}")
            # é™çº§ç­–ç•¥ï¼šå¦‚æœè§£æå¤±è´¥ï¼Œåˆ›å»ºä¸€ä¸ªå•ç« æç¤º
            new_outline = [{"title": "å¤§çº²è§£æå¤±è´¥", "desc": f"åŸå§‹å†…å®¹ï¼š{clean_json[:100]}..."}]
    else:
         new_outline = [{"title": "ç”Ÿæˆæ ¼å¼é”™è¯¯", "desc": "AI æœªè¿”å›æœ‰æ•ˆçš„ JSON æ ¼å¼ã€‚"}]

    return {"current_outline": new_outline, "next": "END"}

# ==========================================
# PART 2: è¿­ä»£å†™ä½œ Agent
# ==========================================

# === 3. è¾…åŠ©å‡½æ•°ï¼šç”Ÿæˆç—…æ¯’å¼æ‘˜è¦ (ç”¨äºå‰ç«¯é•¿å›¾) ===
def generate_viral_card_content(title, full_text):
    """ä¸“é—¨ç”Ÿæˆç”¨äºé•¿å›¾å¤´éƒ¨çš„ç—…æ¯’å¼æ‘˜è¦"""
    llm = get_llm()
    prompt = f"""
    è¯·ä¸ºè¿™ç¯‡æ–‡ç« å†™ä¸€æ®µ"ç¤¾äº¤åª’ä½“æ‘˜è¦"ï¼Œç”¨äºç”Ÿæˆåˆ†äº«å¡ç‰‡ã€‚
    
    ã€æ–‡ç« æ ‡é¢˜ã€‘{title}
    ã€å…¨æ–‡å†…å®¹æ‘˜è¦ã€‘
    {full_text[:4000]}...
    
    ã€è¦æ±‚ã€‘
    1. **å­—æ•°**ï¼š150å­—ä»¥å†…ã€‚
    2. **æ ¼å¼**ï¼š
       ğŸ’¡ **æ ¸å¿ƒæ´å¯Ÿ**ï¼š[ä¸€å¥è¯æ€»ç»“]
       ğŸ”¥ **å…³é”®æ•°æ®**ï¼š[åˆ—å‡º2-3ä¸ªæœ€ç‹ çš„æ•°æ®ï¼Œç”¨åˆ—è¡¨å½¢å¼]
       ğŸš€ **å¯ç¤º**ï¼š[å¯¹è¯»è€…çš„ä¸€å¥è¯å»ºè®®]
    3. **é£æ ¼**ï¼šä¸è¦ç”¨è¡¨æ ¼ã€‚è¯­æ°”æå…·å¸å¼•åŠ›ï¼Œè®©äººæƒ³ç‚¹å¼€å…¨æ–‡ã€‚
    """
    return llm.invoke([HumanMessage(content=prompt)]).content

# === 1. å†™ä½œèŠ‚ç‚¹ï¼šå¢åŠ "ä¸¥ç¦ä½¿ç”¨è¡¨æ ¼"å’Œ"ä¸»ç¼–é£æ ¼" ===
def iterative_writer_node(state: WriterState) -> dict:
    report = state["research_report"]
    outline = state["current_outline"]
    idx = state["current_section_index"]
    previous_context = state.get("full_draft", "") # æ³¨æ„ï¼šè¿™é‡Œå–çš„æ˜¯ç´¯ç§¯çš„å…¨æ–‡
    
    if idx < 0 or idx >= len(outline):
        return {"current_section_content": "", "next": "END"}
    
    target_section = outline[idx]
    
    llm = get_llm()
    
    prompt = f"""
    ä½ æ˜¯ä¸€ä½é¡¶çº§ç§‘æŠ€åª’ä½“ï¼ˆå¦‚ã€Šæ™šç‚¹LatePostã€‹ã€ã€Šæœºå™¨ä¹‹å¿ƒã€‹ï¼‰çš„èµ„æ·±ä¸»ç¼–ã€‚
    ä½ çš„ä»»åŠ¡æ˜¯åŸºäºè°ƒç ”æŠ¥å‘Šå’Œå¤§çº²ï¼Œæ’°å†™æ–‡ç« çš„ç¬¬ {idx + 1} éƒ¨åˆ†ã€‚
    
    ã€æ ¸å¿ƒè°ƒç ”æŠ¥å‘Šã€‘
    {report}
    
    ã€å½“å‰ç« èŠ‚ä¿¡æ¯ã€‘
    æ ‡é¢˜ï¼š{target_section['title']}
    å†™ä½œæŒ‡å¼•ï¼š{target_section['desc']}
    
    ã€ä¸Šæ–‡è„‰ç»œ (Context)ã€‘
    {previous_context[-3000:]} 
    
    ã€ğŸ”´ æ ¼å¼ä¸å†…å®¹è´Ÿé¢çº¦æŸ (å¿…é¡»éµå®ˆ)ã€‘
    1. **ä¸¥ç¦ä½¿ç”¨ Markdown è¡¨æ ¼**ï¼šå› ä¸ºåœ¨æ‰‹æœºé•¿å›¾ä¸­è¡¨æ ¼éš¾ä»¥é˜…è¯»ã€‚è¯·ä½¿ç”¨**åˆ—è¡¨**ï¼ˆBullet pointsï¼‰æˆ–**å¼•ç”¨å—**æ¥å±•ç¤ºå¯¹æ¯”æ•°æ®ã€‚
    2. **ç¦æ­¢é‡å¤æ ‡é¢˜**ï¼šç»å¯¹ä¸è¦åœ¨æ­£æ–‡å¼€å¤´è¾“å‡ºç« èŠ‚æ ‡é¢˜ï¼ˆå¦‚"ç¬¬ä¸€ç« ..."ï¼‰ï¼Œç›´æ¥å¼€å§‹å†™æ­£æ–‡æ®µè½ã€‚
    3. **ä¸¥ç¦ç¼–é€ æ•…äº‹**ï¼šé™¤éè°ƒç ”æŠ¥å‘Šä¸­æœ‰æ˜ç¡®æ¡ˆä¾‹ï¼Œå¦åˆ™ä¸è¦è™šæ„å…·ä½“çš„å®éªŒå®¤èŠ±çµ®ã€‚
    
    ã€ğŸ”µ å†™ä½œé£æ ¼è¦æ±‚ã€‘
    1. **æ·±åº¦æ´å¯Ÿ**ï¼šåˆ†ææ•°æ®èƒŒåçš„äºŒé˜¶æ•ˆåº”ï¼Œä¸è¦åªç½—åˆ—æ•°å­—ã€‚
    2. **å™äº‹å¼ åŠ›**ï¼šä½¿ç”¨é€šä¿—çš„ç±»æ¯”ï¼ˆå¦‚å°†æŠ€æœ¯æ¦‚å¿µæ¯”ä½œç”Ÿæ´»å¸¸è¯†ï¼‰ã€‚
    3. **ç”¨æˆ·åˆ©ç›Š**ï¼šæ®µè½ç»“å°¾é€‚å½“ç‚¹å‡º"è¿™å¯¹è¯»è€…æ„å‘³ç€ä»€ä¹ˆ"ã€‚
    
    è¯·ç›´æ¥è¾“å‡ºæ­£æ–‡ Markdown å†…å®¹ã€‚
    """
    
    content = llm.invoke([HumanMessage(content=prompt)]).content
    
    return {"current_section_content": content, "next": "END"}

def social_summary_node(state: WriterState) -> dict:
    """
    ç”Ÿæˆç¤¾äº¤åª’ä½“æ‘˜è¦å¡ç‰‡
    """
    report = state["research_report"]
    outline = state["current_outline"]
    
    # æ‹¼æ¥å®Œæ•´çš„æ–‡ç« å†…å®¹
    full_text = ""
    for section in outline:
        if section.get('content'):
            full_text += f"## {section['title']}\n\n{section['content']}\n\n"
    
    # ä»æŠ¥å‘Šä¸­æå–æ ‡é¢˜
    title = report[:50] if len(report) > 50 else report  # ç®€å•æå–æ ‡é¢˜
    
    # ç”Ÿæˆç¤¾äº¤åª’ä½“æ‘˜è¦
    summary = generate_viral_card_content(title, full_text)
    
    return {"social_summary": summary, "next": "END"}

# ==========================================
# æ„å»ºå›¾
# ==========================================

# 1. è°ƒç ”ä¸å¤§çº²å›¾ (Research & Outline Flow)
def build_research_graph():
    wf = StateGraph(WriterState)
    wf.add_node("Planner", plan_node)
    wf.add_node("Researcher", research_node)
    wf.add_node("PlanCheck", plan_check_node)
    wf.add_node("ReportGenerator", report_node)
    wf.add_node("Outliner", outline_node)
    
    # é€»è¾‘ï¼šPlanner -> Researcher -> PlanCheck -> (Loop Planner OR ReportGenerator)
    wf.set_entry_point("Planner")
    
    wf.add_edge("Planner", "Researcher")
    wf.add_edge("Researcher", "PlanCheck")
    
    wf.add_conditional_edges(
        "PlanCheck",
        lambda x: x["next"],
        {"Planner": "Planner", "ReportGenerator": "ReportGenerator"}
    )
    
    wf.add_edge("ReportGenerator", "Outliner")
    wf.add_edge("Outliner", END)
    
    return wf.compile()

# 2. å†™ä½œå›¾ (Drafting Flow)
def build_drafting_graph():
    wf = StateGraph(WriterState)
    wf.add_node("Writer", iterative_writer_node)
    wf.add_node("SocialSummary", social_summary_node)  # æ·»åŠ ç¤¾äº¤åª’ä½“æ‘˜è¦èŠ‚ç‚¹
    wf.set_entry_point("Writer")
    wf.add_edge("Writer", "SocialSummary")  # å†™ä½œå®Œæˆåç”Ÿæˆç¤¾äº¤åª’ä½“æ‘˜è¦
    wf.add_edge("SocialSummary", END)
    return wf.compile()

# === 2. å¤§çº²ä¿®æ”¹èŠ‚ç‚¹ï¼šæ”¯æŒç»“æ„åŒ–è°ƒæ•´ ===
def outline_refiner_node(state: WriterState) -> dict:
    current_outline = state["current_outline"]
    instruction = state["edit_instruction"]
    
    llm = get_llm()
    
    # å°†å¤§çº²è½¬ä¸ºå­—ç¬¦ä¸²ï¼Œæ–¹ä¾¿ LLM ç†è§£ç»“æ„
    outline_str = json.dumps(current_outline, ensure_ascii=False, indent=2)
    
    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡ç« æ¶æ„å¸ˆã€‚è¯·æ ¹æ®æŒ‡ä»¤è°ƒæ•´å½“å‰çš„å¤§çº²ç»“æ„ã€‚
    
    ã€å½“å‰å¤§çº² JSONã€‘
    {outline_str}
    
    ã€ä¿®æ”¹æŒ‡ä»¤ã€‘
    {instruction}
    
    ã€ä»»åŠ¡ã€‘
    1. ç†è§£ç”¨æˆ·çš„æŒ‡ä»¤ï¼ˆå¯èƒ½æ˜¯åˆ é™¤æŸç« ã€å¢åŠ æŸç« ã€åˆå¹¶ç« èŠ‚ã€æˆ–ä¿®æ”¹å…·ä½“å†…å®¹ï¼‰ã€‚
    2. è¾“å‡ºä¿®æ”¹åçš„å®Œæ•´ JSON æ•°æ®ï¼ˆList[Dict]ï¼‰ã€‚
    3. **ä¿æŒ JSON æ ¼å¼**ï¼šåŒ…å« `title`, `desc`, `content` (å¦‚æœæ˜¯æ–°ç« èŠ‚ï¼Œcontentä¸ºç©ºï¼›å¦‚æœæ˜¯æ—§ç« èŠ‚ï¼Œä¿ç•™åŸcontent)ã€‚
    4. ä¸è¦è¾“å‡º Markdown æ ‡è®°ï¼Œåªè¾“å‡ºçº¯ JSON å­—ç¬¦ä¸²ã€‚
    """
    
    res = llm.invoke([HumanMessage(content=prompt)]).content
    
    # æ¸…æ´—æ•°æ®
    clean_json = res.replace("```json", "").replace("```", "").strip()
    start = clean_json.find("[")
    end = clean_json.rfind("]")
    
    new_outline = current_outline # é»˜è®¤å›é€€
    
    if start != -1 and end != -1:
        try:
            new_outline = json.loads(clean_json[start:end+1])
        except Exception as e:
            print(f"Outline Refine Error: {e}")
            
    return {"current_outline": new_outline, "next": "END"}

# 3. å¤§çº²ä¿®æ”¹å›¾ (Outline Refinement Flow)
def build_refine_graph():
    wf = StateGraph(WriterState)
    wf.add_node("Refiner", outline_refiner_node)
    wf.set_entry_point("Refiner")
    wf.add_edge("Refiner", END)
    return wf.compile()

# å¯¼å‡º
research_graph = build_research_graph()
drafting_graph = build_drafting_graph()
refine_graph = build_refine_graph()
