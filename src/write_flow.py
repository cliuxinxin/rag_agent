import json
from langchain_core.messages import SystemMessage, HumanMessage
from langgraph.graph import StateGraph, END
from src.state import WriterState
from src.nodes import get_llm
# ç§»é™¤æ—§çš„æ£€ç´¢ä¾èµ–ï¼Œç°åœ¨åªé  DeepSeek å¤§çª—å£é˜…è¯»
# from src.storage import load_kbs 

# ==========================================
# 0. ç¼“å­˜æ„ŸçŸ¥ System Prompt (æ ¸å¿ƒè®¾è®¡)
# ==========================================
def get_cached_system_prompt(content: str) -> str:
    """
    æ„é€ ç¬¦åˆ Context Caching æ ‡å‡†çš„ System Promptã€‚
    å°†å…¨æ–‡æ”¾åœ¨å¼€å¤´ï¼Œåç»­ Planner, Researcher, Writer éƒ½å¤ç”¨æ­¤å‰ç¼€ã€‚
    """
    return f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ·±åº¦å†™ä½œåŠ©æ‰‹ã€‚
ä»¥ä¸‹æ˜¯é¡¹ç›®çš„æ ¸å¿ƒå‚è€ƒç´ æï¼ˆå·²ç¼“å­˜å…¨æ–‡ï¼‰ï¼Œè¯·åŸºäºæ­¤å†…å®¹è¿›è¡Œåˆ†æã€è§„åˆ’å’Œå†™ä½œã€‚
åˆ‡å‹¿ç¼–é€ ç´ æä¸­ä¸å­˜åœ¨çš„äº‹å®ã€‚

<DOCUMENT_START>
{content}
<DOCUMENT_END>
"""

# ==========================================
# PART 1: è°ƒç ”ä¸å¤§çº²ç”Ÿæˆ (å…¨çŸ¥è§†è§’)
# ==========================================

def plan_node(state: WriterState) -> dict:
    """è§„åˆ’å¸ˆï¼šé˜…è¯»å…¨æ–‡ï¼Œåˆ¶å®šè°ƒç ”æ–¹å‘"""
    req = state["user_requirement"]
    full_text = state["full_content"] # è·å–ç¼“å­˜çš„å…¨æ–‡
    outline = state.get("current_outline", [])
    loop = state.get("loop_count", 0)
    
    # é™åˆ¶è°ƒç ”è½®æ¬¡
    if loop >= 3:
        return {"next": "ReportGenerator"}

    llm = get_llm()
    system_msg = SystemMessage(content=get_cached_system_prompt(full_text))
    
    if outline:
        # åŸºäºç°æœ‰å¤§çº²åæ€
        task_prompt = f"""
        å½“å‰ä»»åŠ¡ï¼šå®Œå–„è°ƒç ”ã€‚
        ç”¨æˆ·éœ€æ±‚ï¼š{req}
        å½“å‰å¤§çº²ï¼š{json.dumps(outline, ensure_ascii=False)}
        
        è¯·ç»“åˆã€å…¨æ–‡å†…å®¹ã€‘ï¼Œåˆ†æå¤§çº²ä¸­å“ªäº›éƒ¨åˆ†è¿˜ç¼ºä¹æ·±åº¦æˆ–äº‹å®æ”¯æ’‘ï¼Ÿ
        è¯·æå‡º 1 ä¸ªå…·ä½“çš„æŒ–æ˜æ–¹å‘ã€‚
        """
    else:
        # åˆå§‹è§„åˆ’
        task_prompt = f"""
        å½“å‰ä»»åŠ¡ï¼šå†™ä½œå‰æœŸè§„åˆ’ã€‚
        ç”¨æˆ·éœ€æ±‚ï¼š{req}
        
        è¯·å¿«é€Ÿé€šè¯»ã€å…¨æ–‡å†…å®¹ã€‘ï¼Œä¸ºäº†å†™å¥½è¿™ç¯‡æ–‡ç« ï¼Œæˆ‘ä»¬éœ€è¦é‡ç‚¹æ¢³ç†å“ªäº›ä¿¡æ¯ï¼Ÿ
        è¯·æå‡º 1 ä¸ªå…·ä½“çš„è°ƒç ”åˆ‡å…¥ç‚¹ã€‚
        """
    
    plan = llm.invoke([system_msg, HumanMessage(content=task_prompt)]).content
    
    return {
        "planning_steps": [plan],
        "next": "Researcher",
        "loop_count": loop + 1
    }

def research_node(state: WriterState) -> dict:
    """ç ”ç©¶å‘˜ï¼šåœ¨å…¨æ–‡ä¸­æŸ¥æ‰¾è¯æ®"""
    full_text = state["full_content"]
    plans = state.get("planning_steps", [])
    latest_plan = plans[-1] if plans else "é€šç”¨åˆ†æ"
    
    llm = get_llm()
    system_msg = SystemMessage(content=get_cached_system_prompt(full_text))
    
    task_prompt = f"""
    ã€è°ƒç ”ç›®æ ‡ã€‘{latest_plan}
    
    è¯·åœ¨ã€å…¨æ–‡å†…å®¹ã€‘ä¸­ä»”ç»†æŸ¥æ‰¾ç›¸å…³äº‹å®ã€æ•°æ®æˆ–æ¡ˆä¾‹ã€‚
    è¾“å‡ºä¸€æ®µ 300å­—å·¦å³çš„"è°ƒç ”ç¬”è®°"ï¼Œå¹¶åœ¨æ¯ä¸€æ¡å‘ç°åæ³¨æ˜æ¥æºä½ç½®ï¼ˆå¦‚"æ ¹æ®æ–‡æ¡£å‰åŠéƒ¨åˆ†..."ï¼‰ã€‚
    """
    
    note = llm.invoke([system_msg, HumanMessage(content=task_prompt)]).content
    
    return {
        "research_notes": [note],
        "next": "PlanCheck"
    }

def plan_check_node(state: WriterState) -> dict:
    loop = state.get("loop_count", 0)
    if loop >= 3:
        return {"next": "ReportGenerator"}
    return {"next": "Planner"}

def report_node(state: WriterState) -> dict:
    """æŠ¥å‘Šç”Ÿæˆï¼šæ±‡æ€»ç¬”è®° + å…¨æ–‡ç†è§£"""
    req = state["user_requirement"]
    full_text = state["full_content"]
    notes = state.get("research_notes", [])
    
    llm = get_llm()
    system_msg = SystemMessage(content=get_cached_system_prompt(full_text))
    
    notes_text = "\n\n".join(notes)
    
    task_prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªé«˜çº§åˆ†æå¸ˆã€‚
    ã€å†™ä½œéœ€æ±‚ã€‘{req}
    ã€ä¸“é¡¹è°ƒç ”ç¬”è®°ã€‘
    {notes_text}
    
    è¯·ç»“åˆã€å…¨æ–‡å†…å®¹ã€‘å’Œç¬”è®°ï¼Œæ’°å†™ã€Šæ·±åº¦è°ƒç ”æŠ¥å‘Šã€‹ã€‚
    åŒ…å«ï¼šæ ¸å¿ƒè§‚ç‚¹ã€å…³é”®æ•°æ®ã€é€»è¾‘è„‰ç»œã€‚
    """
    
    report = llm.invoke([system_msg, HumanMessage(content=task_prompt)]).content
    return {"research_report": report, "next": "Outliner"}

def outline_node(state: WriterState) -> dict:
    """å¤§çº²ç”Ÿæˆ"""
    req = state["user_requirement"]
    report = state["research_report"]
    full_text = state["full_content"]
    
    llm = get_llm()
    system_msg = SystemMessage(content=get_cached_system_prompt(full_text))
    
    task_prompt = f"""
    åŸºäºè°ƒç ”æŠ¥å‘Šï¼Œè®¾è®¡æ–‡ç« å¤§çº²ã€‚
    ã€ç”¨æˆ·éœ€æ±‚ã€‘{req}
    ã€è°ƒç ”æŠ¥å‘Šã€‘{report}
    
    è¯·è¾“å‡º JSON æ ¼å¼ï¼ˆList[Dict]ï¼‰ï¼ŒåŒ…å« title, descã€‚
    ç¤ºä¾‹ï¼š["title": "...", "desc": "..."]
    """
    
    res = llm.invoke([system_msg, HumanMessage(content=task_prompt)]).content
    
    # JSON æ¸…æ´—
    clean_json = res.replace("```json", "").replace("```", "").strip()
    start_idx = clean_json.find("[")
    end_idx = clean_json.rfind("]")
    new_outline = []
    if start_idx != -1 and end_idx != -1:
        try:
            new_outline = json.loads(clean_json[start_idx : end_idx + 1])
        except:
            new_outline = [{"title": "æ ¼å¼è§£æå¤±è´¥", "desc": "è¯·é‡è¯•"}]
    else:
         new_outline = [{"title": "ç”Ÿæˆå¤±è´¥", "desc": res[:100]}]

    return {"current_outline": new_outline, "next": "END"}

# ==========================================
# PART 2: è¿­ä»£å†™ä½œ (å¤ç”¨ç¼“å­˜)
# ==========================================

def iterative_writer_node(state: WriterState) -> dict:
    full_text = state["full_content"] # æ ¸å¿ƒï¼šåŸæ–‡ç¼“å­˜
    report = state["research_report"]
    outline = state["current_outline"]
    idx = state["current_section_index"]
    previous_context = state.get("full_draft", "") # å·²ç”Ÿæˆçš„æ­£æ–‡
    
    if idx < 0 or idx >= len(outline):
        return {"current_section_content": "", "next": "END"}
    
    target_section = outline[idx]
    llm = get_llm()
    
    # æ„é€  System Prompt (å¤ç”¨ Context Caching)
    system_msg = SystemMessage(content=get_cached_system_prompt(full_text))
    
    # === ä¿®æ”¹ç‚¹ï¼šå¢å¼º Prompt çº¦æŸ ===
    prompt = f"""
    ä½ æ˜¯ä¸€ä½è¿½æ±‚å®Œç¾çš„ç§‘æŠ€ä¸“æ ä¸»ç¼–ã€‚
    æ­£åœ¨æ’°å†™æ–‡ç« çš„ç¬¬ {idx + 1} éƒ¨åˆ†ï¼Œç« èŠ‚æ ‡é¢˜ä¸ºï¼šã€{target_section['title']}ã€‘ã€‚
    
    ã€æ ¸å¿ƒç´ æ (Fact Base)ã€‘
    {report}
    
    ã€æœ¬ç« å†™ä½œæŒ‡å¼•ã€‘
    {target_section['desc']}
    
    ã€ä¸Šæ–‡è„‰ç»œ (Context)ã€‘
    {previous_context[-4000:]} 
    
    ã€ğŸ”´ ä¸¥æ ¼çš„æ ¼å¼ä¸å†…å®¹æ¸…æ´—è§„åˆ™ (å¿…é¡»éµå®ˆ)ã€‘
    1. **ç¦æ­¢é‡å¤æ ‡é¢˜**ï¼šè¾“å‡ºçš„æ­£æ–‡å¼€å¤´**ä¸è¦**å†å†™ä¸€éâ€œ{target_section['title']}â€æˆ–â€œå¼•è¨€â€ç­‰æ ‡é¢˜ï¼Œç›´æ¥å†™æ­£æ–‡æ®µè½ã€‚
    2. **åˆ é™¤æ— æ•ˆæŒ‡ä»£**ï¼š
       - ä¸¥ç¦å‡ºç°â€œå¦‚å›¾1æ‰€ç¤ºâ€ã€â€œè§è¡¨2â€ã€â€œAs shown in Figure 2â€ç­‰æ–‡å­—ï¼Œå› ä¸ºè¯»è€…çœ‹ä¸åˆ°åŸå›¾ã€‚
       - è¯·å°†å›¾è¡¨å†…å®¹è½¬åŒ–ä¸ºæ–‡å­—æè¿°ï¼ˆä¾‹å¦‚ï¼šâ€œæ•°æ®æ˜¾ç¤º...â€ï¼‰ã€‚
    3. **æ•°å­—ä¸é€»è¾‘è‡ªæ£€**ï¼š
       - ä¸¥ç¦â€œå¤è¯»æœºâ€ç°è±¡ï¼ˆå¦‚â€œä»0.65é™è‡³0.65â€ï¼‰ã€‚
       - æ¶‰åŠæ•°å­—å¯¹æ¯”æ—¶ï¼Œå¿…é¡»æ£€æŸ¥é€»è¾‘é€šé¡ºï¼Œå¹¶å¸¦ä¸Šå•ä½ï¼ˆå¦‚â€œç¾å…ƒ/ç™¾ä¸‡ä»¤ç‰Œâ€ï¼‰ã€‚
    4. **ç¬¦å·è§„èŒƒ**ï¼š
       - æ•°å­¦å…¬å¼è¯·ä½¿ç”¨ LaTeX æ ¼å¼å¹¶ç”¨ $ åŒ…è£¹ï¼ˆå¦‚ $O(L^2)$ï¼‰ï¼Œæˆ–è€…ä½¿ç”¨é€šä¿—ä¸­æ–‡æè¿°ï¼ˆå¦‚â€œäºŒæ¬¡æ–¹å¤æ‚åº¦â€ï¼‰ã€‚
       - ä¸è¦è¾“å‡ºè£¸éœ²çš„ LaTeX å‘½ä»¤ï¼ˆå¦‚ (H_I)ï¼‰ã€‚
    5. **ç»Ÿä¸€ç¼–å·**ï¼š
       - å¦‚æœéœ€è¦åˆ—ç‚¹ï¼Œè¯·ä½¿ç”¨æ ‡å‡†çš„ Markdown åˆ—è¡¨ï¼ˆ- æˆ– 1.ï¼‰ã€‚
       - ä¸è¦ä½¿ç”¨â€œä¸ƒã€â€è¿™ç§ä¸­æ–‡å¤§å†™æ•°å­—ï¼Œé™¤éå¤§çº²é‡Œæ˜ç¡®è¦æ±‚ã€‚
    
    ã€å†™ä½œè¦æ±‚ã€‘
    - è¯­æ°”ï¼šä¸“ä¸šã€å®¢è§‚ã€æœ‰æ·±åº¦ï¼Œç±»ä¼¼â€œæœºå™¨ä¹‹å¿ƒâ€æˆ–â€œæ–°æ™ºå…ƒâ€çš„æ·±åº¦åˆ†æé£æ ¼ã€‚
    - ç»“æ„ï¼šå¤šç”¨çŸ­å¥ï¼Œé€‚å½“åˆ†æ®µï¼Œå…³é”®æ¦‚å¿µå¯ä»¥ç”¨ **åŠ ç²—**ã€‚
    - æ‰¿æ¥ï¼šå¼€å¤´å¿…é¡»è‡ªç„¶æ‰¿æ¥ã€ä¸Šæ–‡è„‰ç»œã€‘ï¼Œä¸è¦ç”Ÿç¡¬è·³è½¬ã€‚
    
    è¯·è¾“å‡ºæœ¬ç« çš„ Markdown æ­£æ–‡ã€‚
    """
    
    content = llm.invoke([system_msg, HumanMessage(content=prompt)]).content
    
    # === æ–°å¢ï¼šç®€å•çš„åå¤„ç†æ¸…æ´— ===
    # é˜²æ­¢ AI è¿˜æ˜¯ä¸å¬è¯ï¼Œè¾“å‡ºäº†æ ‡é¢˜ï¼Œè¿™é‡Œåšä¸€ä¸ªç®€å•çš„å­—ç¬¦ä¸²å‰”é™¤
    clean_content = content.strip()
    # å¦‚æœå¼€å¤´å°±æ˜¯æ ‡é¢˜ï¼Œå»æ‰å®ƒ
    if clean_content.startswith(target_section['title']):
        clean_content = clean_content[len(target_section['title']):].strip()
    # å»æ‰å¯èƒ½å­˜åœ¨çš„ Markdown æ ‡é¢˜æ ‡è®° (e.g. ## Title)
    import re
    clean_content = re.sub(r'^#+\s*' + re.escape(target_section['title']) + r'\s*\n', '', clean_content, flags=re.IGNORECASE).strip()

    return {"current_section_content": clean_content, "next": "END"}

# ==========================================
# PART 3: å¤§çº²é‡æ„ (å¤ç”¨ç¼“å­˜)
# ==========================================

def outline_refiner_node(state: WriterState) -> dict:
    full_text = state["full_content"]
    current_outline = state["current_outline"]
    current_report = state["research_report"]
    instruction = state["edit_instruction"]
    req = state["user_requirement"]
    
    llm = get_llm()
    system_msg = SystemMessage(content=get_cached_system_prompt(full_text))
    
    # 1. æ›´æ–°æŠ¥å‘Š
    report_prompt = f"""
    ç”¨æˆ·æŒ‡ä»¤ï¼š{instruction}
    
    è¯·åŸºäºã€å…¨æ–‡å†…å®¹ã€‘ï¼Œè¡¥å……æ›´æ–°ã€Šè°ƒç ”æŠ¥å‘Šã€‹ä»¥æ”¯æŒè¯¥æŒ‡ä»¤ã€‚
    ã€åŸæŠ¥å‘Šã€‘{current_report}
    """
    new_report = llm.invoke([system_msg, HumanMessage(content=report_prompt)]).content
    
    # 2. é‡æ„å¤§çº²
    outline_prompt = f"""
    åŸºäºæ–°æŠ¥å‘Šå’ŒæŒ‡ä»¤é‡æ„å¤§çº²ã€‚
    ã€æ–°æŠ¥å‘Šã€‘{new_report}
    ã€æ—§å¤§çº²ã€‘{json.dumps(current_outline, ensure_ascii=False)}
    
    è¾“å‡ºçº¯ JSONã€‚
    """
    res = llm.invoke([system_msg, HumanMessage(content=outline_prompt)]).content
    
    # JSON æ¸…æ´—
    clean_json = res.replace("```json", "").replace("```", "").strip()
    start = clean_json.find("[")
    end = clean_json.rfind("]")
    new_outline = current_outline
    if start != -1 and end != -1:
        try:
            new_outline = json.loads(clean_json[start:end+1])
        except: pass
            
    return {
        "research_report": new_report,
        "current_outline": new_outline,
        "next": "END"
    }

# è¾…åŠ©èŠ‚ç‚¹ï¼šç¤¾äº¤æ‘˜è¦ (ä¸éœ€è¦ç¼“å­˜å…¨æ–‡ï¼Œåªçœ‹ç”Ÿæˆç»“æœ)
def social_summary_node(state: WriterState) -> dict:
    from src.write_flow import generate_viral_card_content
    # è¿™é‡Œæˆ‘ä»¬ç®€å•mockä¸€ä¸‹ï¼Œæˆ–è€…ä½ éœ€è¦æŠŠ generate_viral_card_content å®šä¹‰åœ¨è¿™é‡Œ
    # ä¸ºäº†é˜²æ­¢å¾ªç¯å¼•ç”¨ï¼Œæˆ‘ä»¬ç›´æ¥åœ¨è¿™é‡Œå®ç°ç®€ç‰ˆ
    report = state["research_report"]
    outline = state["current_outline"]
    
    full_gen = ""
    for sec in outline:
        if sec.get('content'):
            full_gen += sec['content']
            
    # ç®€å•è°ƒç”¨ LLM
    llm = get_llm()
    prompt = f"å†™ä¸€ä¸ªç¤¾äº¤åª’ä½“æ‘˜è¦ã€‚\næ ‡é¢˜ï¼š{report[:30]}\nå†…å®¹ï¼š{full_gen[:2000]}"
    summary = llm.invoke([HumanMessage(content=prompt)]).content
    
    return {"social_summary": summary, "next": "END"}

# æ„å»ºå›¾
def build_research_graph():
    wf = StateGraph(WriterState)
    wf.add_node("Planner", plan_node)
    wf.add_node("Researcher", research_node)
    wf.add_node("PlanCheck", plan_check_node)
    wf.add_node("ReportGenerator", report_node)
    wf.add_node("Outliner", outline_node)
    
    wf.set_entry_point("Planner")
    wf.add_edge("Planner", "Researcher")
    wf.add_edge("Researcher", "PlanCheck")
    wf.add_conditional_edges("PlanCheck", lambda x: x["next"], {"Planner": "Planner", "ReportGenerator": "ReportGenerator"})
    wf.add_edge("ReportGenerator", "Outliner")
    wf.add_edge("Outliner", END)
    return wf.compile()

def build_drafting_graph():
    wf = StateGraph(WriterState)
    wf.add_node("Writer", iterative_writer_node)
    wf.add_node("SocialSummary", social_summary_node)
    wf.set_entry_point("Writer")
    wf.add_edge("Writer", "SocialSummary")
    wf.add_edge("SocialSummary", END)
    return wf.compile()

def build_refine_graph():
    wf = StateGraph(WriterState)
    wf.add_node("Refiner", outline_refiner_node)
    wf.set_entry_point("Refiner")
    wf.add_edge("Refiner", END)
    return wf.compile()

research_graph = build_research_graph()
drafting_graph = build_drafting_graph()
refine_graph = build_refine_graph()

# å¯¼å‡º generate_viral_card_content ä¾›å‰ç«¯ä½¿ç”¨ (ä¿æŒå…¼å®¹æ€§)
def generate_viral_card_content(title, full_text):
    llm = get_llm()
    prompt = f"å†™ç¤¾äº¤æ‘˜è¦ã€‚\næ ‡é¢˜ï¼š{title}\nå†…å®¹ï¼š{full_text[:3000]}"
    return llm.invoke([HumanMessage(content=prompt)]).content