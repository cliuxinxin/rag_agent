import json
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
from langgraph.graph import StateGraph, END
from src.state import WriterState
from src.nodes import get_llm
from src.storage import load_kbs
from langchain_community.retrievers import BM25Retriever

# ==========================================
# PART 1: è°ƒç ”ä¸å¤§çº²ç”Ÿæˆ Agent
# ==========================================

def plan_node(state: WriterState) -> dict:
    """è§„åˆ’å¸ˆï¼šåˆ†æéœ€æ±‚æˆ–ç°æœ‰å¤§çº²ï¼Œå†³å®šéœ€è¦è°ƒç ”ä»€ä¹ˆæ–¹å‘"""
    req = state["user_requirement"]
    outline = state.get("current_outline", [])
    loop = state.get("loop_count", 0)
    
    # é™åˆ¶è°ƒç ”è½®æ¬¡ï¼ˆä¾‹å¦‚æœ€å¤šæŸ¥ 3 æ¬¡ï¼‰
    if loop >= 3:
        return {"next": "ReportGenerator"}

    llm = get_llm()
    
    # å¦‚æœå·²æœ‰å¤§çº²ï¼ˆè¯´æ˜æ˜¯ä¿®æ”¹åé‡æ–°è°ƒç ”ï¼‰ï¼Œåˆ™åŸºäºå¤§çº²è§„åˆ’
    if outline:
        outline_str = "\n".join([f"- {item['title']}: {item['desc']}" for item in outline])
        prompt = f"""
        å½“å‰ä»»åŠ¡ï¼šå®Œå–„å†™ä½œé¡¹ç›®çš„è°ƒç ”ã€‚
        ç”¨æˆ·éœ€æ±‚ï¼š{req}
        å½“å‰å¤§çº²ï¼š
        {outline_str}
        
        è¯·åˆ†æå¤§çº²ä¸­å“ªäº›éƒ¨åˆ†ç¼ºä¹äº‹å®æ”¯æ’‘ï¼Ÿæå‡º 2-3 ä¸ªå…·ä½“çš„æœç´¢å…³é”®è¯æˆ–é—®é¢˜ï¼Œç”¨äºè¡¥å……è°ƒç ”ã€‚
        """
    else:
        # ç¬¬ä¸€æ¬¡ç”Ÿæˆï¼ŒåŸºäºéœ€æ±‚è§„åˆ’
        prompt = f"""
        å½“å‰ä»»åŠ¡ï¼šä¸ºå†™ä½œé¡¹ç›®åšå‰æœŸè°ƒç ”ã€‚
        ç”¨æˆ·éœ€æ±‚ï¼š{req}
        
        è¯·åˆ†æä¸ºäº†å†™å¥½è¿™ç¯‡æ–‡ç« ï¼Œæˆ‘ä»¬éœ€è¦æœé›†å“ªäº›æ ¸å¿ƒä¿¡æ¯ï¼Ÿ
        è¯·è¾“å‡º 2-3 ä¸ªå…·ä½“çš„æœç´¢å…³é”®è¯æˆ–æ–¹å‘ã€‚
        """
    
    # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œç›´æ¥è®© LLM è¾“å‡ºå…³é”®è¯ï¼Œå®é™…å¯ä»¥åšæˆ Structured Output
    plan = llm.invoke([HumanMessage(content=prompt)]).content
    
    return {
        "planning_steps": [plan],
        "next": "Researcher",
        "loop_count": loop + 1
    }

def research_node(state: WriterState) -> dict:
    """ç ”ç©¶å‘˜ï¼šæ‰§è¡Œæœç´¢"""
    source_type = state["source_type"]
    source_data = state["source_data"]
    plans = state.get("planning_steps", [])
    latest_plan = plans[-1] if plans else ""
    
    # 1. è·å–ä¸Šä¸‹æ–‡ï¼ˆçŸ¥è¯†åº“æ£€ç´¢ æˆ– é™æ€æ–‡æœ¬ï¼‰
    retrieved_text = ""
    
    if source_type == "kb":
        # è§£æ KB åˆ—è¡¨
        try:
            kb_list = json.loads(source_data) if isinstance(source_data, str) else source_data
            docs, _ = load_kbs(kb_list)
            if docs:
                # åŸºäº Plan è¿›è¡Œæ£€ç´¢
                retriever = BM25Retriever.from_documents(docs)
                retriever.k = 5
                # ç®€å•æå– Plan ä¸­çš„å…³é”®è¯è¿›è¡Œæ£€ç´¢
                results = retriever.invoke(latest_plan)
                retrieved_text = "\n".join([f"[KB Ref] {d.page_content}" for d in results])
        except Exception as e:
            print(f"Research Error: {e}")
            
    elif source_type in ["file", "text"]:
        # å¯¹äºä¸Šä¼ æ–‡ä»¶ï¼Œå…¨æ–‡ä½œä¸ºä¸Šä¸‹æ–‡ï¼ˆå¦‚æœä¸é•¿ï¼‰ï¼Œæˆ–è€…åšç®€å•åˆ‡ç‰‡æ£€ç´¢
        # è¿™é‡Œä¸ºäº†ç®€å•ï¼Œå‡è®¾ text æ¨¡å¼ä¸‹ source_data å°±æ˜¯å…¨æ–‡
        full_text = source_data
        if len(full_text) > 10000:
            # ç®€å•æˆªæ–­æˆ–åšç®€å•æŸ¥æ‰¾
            retrieved_text = full_text[:10000] 
        else:
            retrieved_text = full_text

    if not retrieved_text:
        retrieved_text = "æœªæ‰¾åˆ°ç›¸å…³èµ„æ–™ã€‚"

    # 2. æ€»ç»“å‘ç°
    llm = get_llm()
    summary_prompt = f"""
    æ ¹æ®æœç´¢è®¡åˆ’ï¼š{latest_plan}
    æˆ‘ä»¬æ£€ç´¢åˆ°äº†ä»¥ä¸‹å†…å®¹ï¼š
    {retrieved_text[:3000]}
    
    è¯·æå–å¯¹å†™ä½œæœ‰å¸®åŠ©çš„äº‹å®ã€æ•°æ®æˆ–è§‚ç‚¹ï¼Œå½¢æˆä¸€æ¡è°ƒç ”ç¬”è®°ã€‚
    """
    note = llm.invoke([HumanMessage(content=summary_prompt)]).content
    
    return {
        "research_notes": state.get("research_notes", []) + [note],
        "next": "PlanCheck" # æœå®Œä¸€æ¬¡ï¼Œå›å»æ£€æŸ¥æ˜¯å¦è¿˜éœ€è¦æœ
    }

def plan_check_node(state: WriterState) -> dict:
    """ç®€å•çš„å¾ªç¯æ§åˆ¶å™¨"""
    loop = state.get("loop_count", 0)
    # æ¯”å¦‚æˆ‘ä»¬ç¡¬æ€§è§„å®šæœ 2 è½®å°±å¤Ÿäº†
    if loop >= 2:
        return {"next": "ReportGenerator"}
    return {"next": "Planner"} # å›å» Planner ç»§ç»­è§„åˆ’

def report_node(state: WriterState) -> dict:
    """æŠ¥å‘Šç”Ÿæˆå™¨ï¼šæ±‡æ€»æ‰€æœ‰ç¬”è®°"""
    req = state["user_requirement"]
    notes = state.get("research_notes", [])
    
    llm = get_llm()
    
    notes_text = "\n\n".join(notes)
    
    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªé«˜çº§åˆ†æå¸ˆã€‚
    
    ã€å†™ä½œéœ€æ±‚ã€‘{req}
    ã€ç´¯è®¡è°ƒç ”ç¬”è®°ã€‘
    {notes_text}
    
    è¯·æ ¹æ®ä»¥ä¸Šä¿¡æ¯ï¼Œæ’°å†™ä¸€ä»½ç»“æ„æ¸…æ™°çš„ã€Šæ·±åº¦è°ƒç ”æŠ¥å‘Šã€‹ã€‚
    åŒ…å«ï¼šæ ¸å¿ƒä¸»æ—¨ã€å…³é”®è®ºæ®ã€ç´ æå‚¨å¤‡ã€å»ºè®®çš„å™äº‹è§’åº¦ã€‚
    """
    
    report = llm.invoke([HumanMessage(content=prompt)]).content
    return {"research_report": report, "next": "Outliner"}

def outline_node(state: WriterState) -> dict:
    """å¤§çº²ç”Ÿæˆå™¨ï¼šå¢å¼ºå®¹é”™èƒ½åŠ›"""
    req = state["user_requirement"]
    report = state["research_report"]
    
    llm = get_llm()
    
    prompt = f"""
    åŸºäºè°ƒç ”æŠ¥å‘Šï¼Œè®¾è®¡æ–‡ç« å¤§çº²ã€‚
    
    ã€éœ€æ±‚ã€‘{req}
    ã€æŠ¥å‘Šã€‘{report[:3000]} 
    
    è¯·è¾“å‡ºä¸¥æ ¼çš„ JSON æ ¼å¼ï¼ˆList[Dict]ï¼‰ï¼ŒåŒ…å« title, descã€‚
    ä¸è¦è¾“å‡ºä»»ä½• Markdown æ ‡è®°ï¼ˆå¦‚ ```jsonï¼‰ï¼Œåªè¾“å‡ºçº¯ JSON æ–‡æœ¬ã€‚
    
    ç¤ºä¾‹æ ¼å¼ï¼š
    [
        {{"title": "ç¬¬ä¸€ç« ï¼š...", "desc": "..."}},
        {{"title": "ç¬¬äºŒç« ï¼š...", "desc": "..."}}
    ]
    """
    
    res = llm.invoke([HumanMessage(content=prompt)]).content
    
    # === å¢å¼ºæ¸…æ´—é€»è¾‘ ===
    clean_json = res.replace("```json", "").replace("```", "").strip()
    # æœ‰æ—¶å€™ LLM ä¼šåœ¨å¼€å¤´åŠ  "Here is the json..."ï¼Œæˆ‘ä»¬è¦å°è¯•æ‰¾åˆ°ç¬¬ä¸€ä¸ª [
    start_idx = clean_json.find("[")
    end_idx = clean_json.rfind("]")
    
    new_outline = []
    
    if start_idx != -1 and end_idx != -1:
        clean_json = clean_json[start_idx : end_idx + 1]
        try:
            new_outline = json.loads(clean_json)
        except json.JSONDecodeError as e:
            print(f"JSON Parse Error: {e}")
            # é™çº§ç­–ç•¥ï¼šå¦‚æœè§£æå¤±è´¥ï¼Œåˆ›å»ºä¸€ä¸ªå•ç« æç¤º
            new_outline = [{"title": "å¤§çº²è§£æå¤±è´¥", "desc": f"åŸå§‹å†…å®¹ï¼š{clean_json[:100]}..."}]
    else:
         new_outline = [{"title": "ç”Ÿæˆæ ¼å¼é”™è¯¯", "desc": "AI æœªè¿”å›æœ‰æ•ˆçš„ JSON æ ¼å¼ã€‚"}]

    return {"current_outline": new_outline, "next": "END"}

# ==========================================
# PART 2: è¿­ä»£å†™ä½œ Agent
# ==========================================

# === 3. è¾…åŠ©å‡½æ•°ï¼šç”Ÿæˆç—…æ¯’å¼æ‘˜è¦ (ç”¨äºå‰ç«¯é•¿å›¾) ===
def generate_viral_card_content(title, full_text):
    """ä¸“é—¨ç”Ÿæˆç”¨äºé•¿å›¾å¤´éƒ¨çš„ç—…æ¯’å¼æ‘˜è¦"""
    llm = get_llm()
    prompt = f"""
    è¯·ä¸ºè¿™ç¯‡æ–‡ç« å†™ä¸€æ®µ"ç¤¾äº¤åª’ä½“æ‘˜è¦"ï¼Œç”¨äºç”Ÿæˆåˆ†äº«å¡ç‰‡ã€‚
    
    ã€æ–‡ç« æ ‡é¢˜ã€‘{title}
    ã€å…¨æ–‡å†…å®¹æ‘˜è¦ã€‘
    {full_text[:4000]}...
    
    ã€è¦æ±‚ã€‘
    1. **å­—æ•°**ï¼š150å­—ä»¥å†…ã€‚
    2. **æ ¼å¼**ï¼š
       ğŸ’¡ **æ ¸å¿ƒæ´å¯Ÿ**ï¼š[ä¸€å¥è¯æ€»ç»“]
       ğŸ”¥ **å…³é”®æ•°æ®**ï¼š[åˆ—å‡º2-3ä¸ªæœ€ç‹ çš„æ•°æ®ï¼Œç”¨åˆ—è¡¨å½¢å¼]
       ğŸš€ **å¯ç¤º**ï¼š[å¯¹è¯»è€…çš„ä¸€å¥è¯å»ºè®®]
    3. **é£æ ¼**ï¼šä¸è¦ç”¨è¡¨æ ¼ã€‚è¯­æ°”æå…·å¸å¼•åŠ›ï¼Œè®©äººæƒ³ç‚¹å¼€å…¨æ–‡ã€‚
    """
    return llm.invoke([HumanMessage(content=prompt)]).content

# === 1. æ ¸å¿ƒå†™ä½œèŠ‚ç‚¹ï¼šå¢åŠ "å»é‡"ä¸"è¿è´¯æ€§"å¼ºçº¦æŸ ===
def iterative_writer_node(state: WriterState) -> dict:
    report = state["research_report"]
    outline = state["current_outline"]
    idx = state["current_section_index"]
    previous_context = state.get("full_draft", "")
    
    if idx < 0 or idx >= len(outline):
        return {"current_section_content": "", "next": "END"}
    
    target_section = outline[idx]
    llm = get_llm()
    
    # åŠ¨æ€æ„å»º"å·²æåŠä¿¡æ¯"åˆ—è¡¨ï¼Œé˜²æ­¢å¤è¯»æœº
    # è¿™æ˜¯ä¸€ä¸ªç®€å•çš„ Prompt Trickï¼Œå‘Šè¯‰ AI ä¸Šæ–‡è®²è¿‡å•¥
    prompt = f"""
    ä½ æ˜¯ä¸€ä½é¡¶çº§ç§‘æŠ€åª’ä½“çš„èµ„æ·±ä¸»ç¼–ã€‚
    æ­£åœ¨æ’°å†™æ–‡ç« çš„ç¬¬ {idx + 1} éƒ¨åˆ†ï¼šã€{target_section['title']}ã€‘ã€‚
    
    ã€æ ¸å¿ƒè°ƒç ”èµ„æ–™ã€‘
    {report}
    
    ã€æœ¬ç« å†™ä½œæŒ‡å¼•ã€‘
    {target_section['desc']}
    
    ã€ä¸Šæ–‡è„‰ç»œ (Context)ã€‘
    {previous_context[-3000:]}
    
    ã€ğŸ”´ æ ¸å¿ƒå†™ä½œçº¦æŸ (è¿åå°†å¯¼è‡´ä¸åˆæ ¼)ã€‘
    1. **ä¸¥ç¦ä¿¡æ¯é‡å¤**ï¼šæ£€æµ‹ã€ä¸Šæ–‡è„‰ç»œã€‘ã€‚å¦‚æœä¸Šæ–‡å·²ç»è¯¦ç»†å¹å˜˜è¿‡"IMOé‡‘ç‰Œ"ã€"Codeforces 2701åˆ†"æˆ–"DSAæœºåˆ¶"ï¼Œæœ¬ç« **ç»å¯¹ä¸è¦**å†æ¬¡è¯¦ç»†æè¿°ï¼Œä¸€ç¬”å¸¦è¿‡å³å¯ã€‚
    2. **æµç•…è¡”æ¥**ï¼šå¼€å¤´å¿…é¡»æ‰¿æ¥ä¸Šæ–‡çš„è¯­æ°”å’Œé€»è¾‘ï¼Œä¸è¦ç”Ÿç¡¬åœ°å¦èµ·ç‚‰ç¶ã€‚
    3. **ä¸¥ç¦è¡¨æ ¼**ï¼šæ‰‹æœºé˜…è¯»ä½“éªŒå·®ï¼Œè¯·ç”¨åˆ—è¡¨æˆ–æ–‡å­—å¯¹æ¯”ã€‚
    4. **ç¦æ­¢ç« èŠ‚å·**ï¼šæ­£æ–‡å¼€å¤´ä¸è¦å†™"ç¬¬ä¸€ç« "æˆ–ç« èŠ‚æ ‡é¢˜ã€‚
    
    ã€ğŸ”µ æ·±åº¦ä¸é£æ ¼ã€‘
    - è¯­æ°”ï¼šå®¢è§‚çŠ€åˆ©ï¼Œæœ‰è¡Œä¸šæ´å¯ŸåŠ›ã€‚
    - ç»“æ„ï¼šå¤šç”¨çŸ­å¥ï¼Œé€‚å½“åˆ†æ®µã€‚
    
    è¯·è¾“å‡ºæœ¬ç« çš„æ­£æ–‡ Markdownã€‚
    """
    
    content = llm.invoke([HumanMessage(content=prompt)]).content
    return {"current_section_content": content, "next": "END"}

def social_summary_node(state: WriterState) -> dict:
    """
    ç”Ÿæˆç¤¾äº¤åª’ä½“æ‘˜è¦å¡ç‰‡
    """
    report = state["research_report"]
    outline = state["current_outline"]
    
    # æ‹¼æ¥å®Œæ•´çš„æ–‡ç« å†…å®¹
    full_text = ""
    for section in outline:
        if section.get('content'):
            full_text += f"## {section['title']}\n\n{section['content']}\n\n"
    
    # ä»æŠ¥å‘Šä¸­æå–æ ‡é¢˜
    title = report[:50] if len(report) > 50 else report  # ç®€å•æå–æ ‡é¢˜
    
    # ç”Ÿæˆç¤¾äº¤åª’ä½“æ‘˜è¦
    summary = generate_viral_card_content(title, full_text)
    
    return {"social_summary": summary, "next": "END"}

# ==========================================
# æ„å»ºå›¾
# ==========================================

# 1. è°ƒç ”ä¸å¤§çº²å›¾ (Research & Outline Flow)
def build_research_graph():
    wf = StateGraph(WriterState)
    wf.add_node("Planner", plan_node)
    wf.add_node("Researcher", research_node)
    wf.add_node("PlanCheck", plan_check_node)
    wf.add_node("ReportGenerator", report_node)
    wf.add_node("Outliner", outline_node)
    
    # é€»è¾‘ï¼šPlanner -> Researcher -> PlanCheck -> (Loop Planner OR ReportGenerator)
    wf.set_entry_point("Planner")
    
    wf.add_edge("Planner", "Researcher")
    wf.add_edge("Researcher", "PlanCheck")
    
    wf.add_conditional_edges(
        "PlanCheck",
        lambda x: x["next"],
        {"Planner": "Planner", "ReportGenerator": "ReportGenerator"}
    )
    
    wf.add_edge("ReportGenerator", "Outliner")
    wf.add_edge("Outliner", END)
    
    return wf.compile()

# 2. å†™ä½œå›¾ (Drafting Flow)
def build_drafting_graph():
    wf = StateGraph(WriterState)
    wf.add_node("Writer", iterative_writer_node)
    wf.add_node("SocialSummary", social_summary_node)  # æ·»åŠ ç¤¾äº¤åª’ä½“æ‘˜è¦èŠ‚ç‚¹
    wf.set_entry_point("Writer")
    wf.add_edge("Writer", "SocialSummary")  # å†™ä½œå®Œæˆåç”Ÿæˆç¤¾äº¤åª’ä½“æ‘˜è¦
    wf.add_edge("SocialSummary", END)
    return wf.compile()

# === 2. æ·±åº¦é‡æ„èŠ‚ç‚¹ï¼šåŒæ­¥æ›´æ–°æŠ¥å‘Šä¸å¤§çº² ===
def outline_refiner_node(state: WriterState) -> dict:
    """
    æ ¹æ®ç”¨æˆ·æŒ‡ä»¤ï¼Œå…ˆæ›´æ–°è°ƒç ”æŠ¥å‘Šï¼Œå†é‡æ„å¤§çº²ã€‚
    """
    current_outline = state["current_outline"]
    current_report = state["research_report"]
    instruction = state["edit_instruction"] # ç”¨æˆ·çš„ä¿®æ”¹æŒ‡ä»¤
    req = state["user_requirement"] # åŸå§‹éœ€æ±‚
    
    llm = get_llm()
    
    # --- æ­¥éª¤ 1: æ›´æ–°è°ƒç ”æŠ¥å‘Š ---
    # AI éœ€è¦æ ¹æ®æ–°æŒ‡ä»¤ï¼Œè¡¥å……ç›¸å…³çŸ¥è¯†åˆ°æŠ¥å‘Šä¸­
    report_prompt = f"""
    ç”¨æˆ·å¸Œæœ›ä¿®æ”¹æ–‡ç« ç»“æ„ã€‚ä¸ºäº†æ”¯æŒè¿™ä¸€ä¿®æ”¹ï¼Œæˆ‘ä»¬éœ€è¦å…ˆæ›´æ–°ã€Šæ·±åº¦è°ƒç ”æŠ¥å‘Šã€‹ã€‚
    
    ã€åŸå§‹éœ€æ±‚ã€‘{req}
    ã€å½“å‰æŠ¥å‘Šã€‘{current_report}
    ã€ä¿®æ”¹æŒ‡ä»¤ã€‘{instruction}
    
    è¯·é‡å†™æˆ–è¡¥å……è°ƒç ”æŠ¥å‘Šï¼š
    1. å¦‚æœæŒ‡ä»¤æ¶‰åŠæ–°é¢†åŸŸï¼ˆå¦‚"å¢åŠ æœªæ¥è¶‹åŠ¿"ï¼‰ï¼Œè¯·åŸºäºä½ çš„çŸ¥è¯†åº“è¡¥å……ç›¸å…³æ·±åº¦åˆ†æåˆ°æŠ¥å‘Šä¸­ã€‚
    2. å¦‚æœæŒ‡ä»¤æ˜¯åˆ é™¤ï¼Œè¯·ç²¾ç®€æŠ¥å‘Šä¸­å¯¹åº”çš„å†—ä½™éƒ¨åˆ†ã€‚
    3. ä¿æŒæŠ¥å‘Šçš„ä¸“ä¸šæ€§å’Œæ·±åº¦ã€‚
    
    è¯·ç›´æ¥è¾“å‡ºæ›´æ–°åçš„è°ƒç ”æŠ¥å‘Šå…¨æ–‡ã€‚
    """
    new_report = llm.invoke([HumanMessage(content=report_prompt)]).content
    
    # --- æ­¥éª¤ 2: é‡æ„å¤§çº² ---
    # åŸºäºæ–°æŠ¥å‘Šå’ŒæŒ‡ä»¤ï¼Œé‡æ–°ç”Ÿæˆ JSON å¤§çº²
    outline_str = json.dumps(current_outline, ensure_ascii=False, indent=2)
    
    outline_prompt = f"""
    åŸºäºæ›´æ–°åçš„è°ƒç ”æŠ¥å‘Šå’Œç”¨æˆ·æŒ‡ä»¤ï¼Œé‡æ„æ–‡ç« å¤§çº²ã€‚
    
    ã€æ›´æ–°åçš„æŠ¥å‘Šã€‘{new_report[:3000]}...
    ã€æ—§å¤§çº²ã€‘{outline_str}
    ã€ä¿®æ”¹æŒ‡ä»¤ã€‘{instruction}
    
    è¯·è¾“å‡ºæ–°çš„ JSON å¤§çº² (List[Dict])ã€‚
    è¦æ±‚ï¼š
    1. ç»“æ„å®Œæ•´ï¼ŒåŒ…å« title, desc, content(è‹¥ä¿ç•™æ—§ç« èŠ‚åˆ™ä¿ç•™å†…å®¹ï¼Œæ–°ç« èŠ‚ä¸ºç©º)ã€‚
    2. åªè¾“å‡ºçº¯ JSONï¼Œæ—  Markdown æ ‡è®°ã€‚
    """
    
    res = llm.invoke([HumanMessage(content=outline_prompt)]).content
    
    # æ¸…æ´— JSON
    clean_json = res.replace("```json", "").replace("```", "").strip()
    start = clean_json.find("[")
    end = clean_json.rfind("]")
    
    new_outline = current_outline # é»˜è®¤å›é€€
    if start != -1 and end != -1:
        try:
            new_outline = json.loads(clean_json[start:end+1])
        except: pass
            
    # è¿”å›æ›´æ–°åçš„æŠ¥å‘Šå’Œå¤§çº²
    return {
        "research_report": new_report,
        "current_outline": new_outline,
        "next": "END"
    }

# 3. å¤§çº²ä¿®æ”¹å›¾ (Outline Refinement Flow)
def build_refine_graph():
    wf = StateGraph(WriterState)
    wf.add_node("Refiner", outline_refiner_node)
    wf.set_entry_point("Refiner")
    wf.add_edge("Refiner", END)
    return wf.compile()

# å¯¼å‡º
research_graph = build_research_graph()
drafting_graph = build_drafting_graph()
refine_graph = build_refine_graph()
