import os
from langchain_core.messages import HumanMessage, SystemMessage
from langgraph.graph import StateGraph, END
from src.state import AgentState
from src.nodes import get_llm

# === 0. ç¼“å­˜æ„ŸçŸ¥ System Prompt (ä¿æŒä¸å˜) ===
def get_cached_system_prompt(content: str) -> str:
    return f"""ä½ æ˜¯ä¸€ä¸ªå¤„äº"DeepSeek Context Caching"æ¨¡å¼ä¸‹çš„é¡¶çº§æ·±åº¦é˜…è¯»ä¸“å®¶ã€‚
ä»¥ä¸‹æ˜¯æˆ‘ä»¬éœ€è¦æ·±åº¦å‰–æçš„æ–‡æ¡£å…¨æ–‡ï¼ˆå·²ç¼“å­˜ï¼‰ï¼Œè¯·ä»”ç»†é˜…è¯»æ¯ä¸€ä¸ªæ®µè½ï¼š

<DOCUMENT_START>
{content}
<DOCUMENT_END>
"""

# === 1. Planner Node: çœŸæ­£çš„"é˜…è¯»ç­–ç•¥å®¶" ===
# æ”¹è¿›ç‚¹ï¼šå…·å¤‡äº†"é€šç”¨é˜…è¯»èƒ½åŠ›"ï¼Œèƒ½å¤„ç†å™äº‹ã€æ–°é—»ã€è¯„è®ºç­‰éæŠ€æœ¯æ–‡ç« 
def planner_node(state: AgentState) -> dict:
    full_text = state["full_content"]
    qa_history = state.get("qa_pairs", [])
    loop = state.get("loop_count", 0)
    MAX_LOOPS = 4 
    
    llm = get_llm()
    
    # æ ¼å¼åŒ–å·²æœ‰çš„é—®ç­”å¯¹ï¼Œè®© Planner çŸ¥é“æˆ‘ä»¬å·²ç»ææ‡‚äº†ä»€ä¹ˆ
    history_text = "\n".join(qa_history) if qa_history else "ï¼ˆæš‚æ— ï¼Œè¿™æ˜¯ç¬¬ä¸€è½®åˆ†æï¼‰"
    
    # === æ ¸å¿ƒä¿®æ”¹ï¼šé€šç”¨çš„æ·±åº¦é˜…è¯» Prompt ===
    # è¿™ä¸€æ­¥å®ç°äº†æ‚¨è¯´çš„"ç²—ç•¥é˜…è¯»å¹¶æå‡ºé—®é¢˜"
    task_prompt = f"""
    å½“å‰åˆ†æè½®æ¬¡: {loop + 1}/{MAX_LOOPS}
    
    ã€æˆ‘ä»¬å·²æœ‰çš„ç†è§£ï¼ˆå·²è§£å†³çš„é—®é¢˜ï¼‰ã€‘
    {history_text}
    
    ã€ä»»åŠ¡ç›®æ ‡ã€‘
    è¯·å…ˆå¿«é€Ÿé€šè¯»å…¨æ–‡ï¼Œåˆ¤æ–­æ–‡ç« ä½“è£ï¼ˆæ˜¯æŠ€æœ¯æ–‡æ¡£ã€çºªå®å™äº‹ã€æ–°é—»æŠ¥é“ï¼Œè¿˜æ˜¯è§‚ç‚¹è¯„è®ºï¼Ÿï¼‰ã€‚
    ç„¶åï¼Œæå‡ºä¸‹ä¸€ä¸ªæœ€å€¼å¾—æŒ–æ˜çš„**"æ·±åº¦é—®é¢˜"**ã€‚
    
    ã€æé—®ç­–ç•¥ã€‘
    ä¸è¦é—®æµ…æ˜¾çš„äº‹å®ï¼ˆå¦‚"ä½œè€…æ˜¯è°ï¼Ÿ"ï¼‰ï¼Œè¦é—®éœ€è¦**ç»“åˆåŸæ–‡ä¸å¸¸è¯†**æ‰èƒ½å›ç­”çš„é—®é¢˜ï¼š
    
    1.  **å¦‚æœæ˜¯ã€çºªå®/å™äº‹ã€‘ï¼ˆå¦‚ï¼šã€Šæµæ„Ÿä¸‹çš„åŒ—äº¬ä¸­å¹´ã€‹ï¼‰**ï¼š
        - å…³æ³¨**å› æœé“¾ä¸å†³ç­–ç‚¹**ï¼šä¾‹å¦‚"ä¸ºä»€ä¹ˆæ–‡ä¸­æåˆ°çš„æŸä¸ªå†³å®šå¯¼è‡´äº†åç»­çš„å´©ç›˜ï¼Ÿå¸¸è¯†ä¸Šåº”è¯¥æ€ä¹ˆåšï¼Ÿ"
        - å…³æ³¨**èµ„æºä¸åšå¼ˆ**ï¼šä¾‹å¦‚"åœ¨èµ„æºï¼ˆå¦‚ICUã€é‡‘é’±ï¼‰å—é™æ—¶ï¼Œä¸»è§’é¢ä¸´äº†ä»€ä¹ˆæ ·çš„äººæ€§è€ƒéªŒï¼Ÿ"
        
    2.  **å¦‚æœæ˜¯ã€è§‚ç‚¹/è¯„è®ºã€‘**ï¼š
        - å…³æ³¨**é€»è¾‘æ¼æ´ä¸åº•å±‚å‡è®¾**ï¼šä½œè€…åŸºäºä»€ä¹ˆé¢„è®¾å‰æï¼Ÿè¿™äº›å‰æåœ¨ç°å®ä¸­æˆç«‹å—ï¼Ÿ
        
    3.  **å¦‚æœæ˜¯ã€æŠ€æœ¯/ç§‘æ™®ã€‘**ï¼š
        - å…³æ³¨**æ ¸å¿ƒåŸç†ä¸åº”ç”¨è¾¹ç•Œ**ï¼šè¿™ä¸ªæŠ€æœ¯è§£å†³äº†ä»€ä¹ˆæœ¬è´¨é—®é¢˜ï¼Ÿæœ‰ä»€ä¹ˆä»£ä»·ï¼Ÿ
        
    ã€è¾“å‡ºè¦æ±‚ã€‘
    - å¦‚æœä½ è§‰å¾—æ–‡ç« çš„æ ¸å¿ƒé€»è¾‘ã€å…³é”®å†³ç­–ã€æ·±å±‚å«ä¹‰éƒ½å·²ç»åˆ†æé€å½»äº†ï¼Œè¾“å‡º "TERMINATE"ã€‚
    - å¦åˆ™ï¼Œè¾“å‡º**ä¸€ä¸ª**å…·ä½“çš„ã€æœ‰æ·±åº¦çš„é—®é¢˜ã€‚
    """
    
    messages = [
        SystemMessage(content=get_cached_system_prompt(full_text)),
        HumanMessage(content=task_prompt)
    ]
    
    response = llm.invoke(messages).content.strip()
    
    # æ¸…ç†ä¸€ä¸‹å¼•å·
    question = response.replace('"', '').replace("'", "")
    
    if "TERMINATE" in response or loop >= MAX_LOOPS:
        # å¦‚æœåˆ†æå®Œäº†ï¼Œè½¬äº¤ç»™ä½œå®¶è¿›è¡Œç»¼åˆè¾“å‡º
        return {"next": "Writer", "current_question": ""}
    else:
        # è¿˜æœ‰é—®é¢˜æ²¡ææ‡‚ï¼Œäº¤ç»™ç ”ç©¶å‘˜å»æŸ¥
        return {
            "next": "Researcher", 
            "current_question": question, 
            "loop_count": loop + 1
        }

# === 2. Researcher Node: ç»“åˆå¸¸è¯†çš„è§£ç­”è€… ===
# æ”¹è¿›ç‚¹ï¼šæ˜ç¡®è¦æ±‚"ç»“åˆå¸¸è¯†"è¿›è¡Œè§£ç­”
def researcher_node(state: AgentState) -> dict:
    full_text = state["full_content"]
    question = state["current_question"]
    
    llm = get_llm()
    
    task_prompt = f"""
    ã€å½“å‰å¾…æ”»å…‹çš„é—®é¢˜ã€‘
    {question}
    
    ã€å›ç­”è¦æ±‚ã€‘
    ä½ ä¸ä»…ä»…æ˜¯ä¸€ä¸ªæ‘˜å½•æœºå™¨ï¼Œä½ æ˜¯ä¸€ä¸ªæœ‰æ™ºæ…§çš„åˆ†æå¸ˆã€‚è¯·æŒ‰ä»¥ä¸‹æ­¥éª¤å›ç­”ï¼š
    
    1.  **åŸæ–‡è¯æ®**ï¼šé¦–å…ˆï¼Œä»æ–‡ä¸­æ‰¾åˆ°ç›¸å…³çš„æ®µè½ã€å¯¹è¯æˆ–æ•°æ®ä½œä¸ºä¾æ®ã€‚
    2.  **å¸¸è¯†èåˆ**ï¼š**å…³é”®æ­¥éª¤ï¼** è¯·è°ƒç”¨ä½ çš„å¤–éƒ¨çŸ¥è¯†åº“ï¼ˆå¸¸è¯†ã€åŒ»å­¦å¸¸è¯†ã€ç¤¾ä¼šè¿ä½œé€»è¾‘ã€æŠ€æœ¯åŸç†ï¼‰ï¼Œå¯¹åŸæ–‡å†…å®¹è¿›è¡Œè§£è¯»ã€‚
        - ä¾‹å¦‚ï¼šæ–‡ä¸­è¯´"ç™½ç»†èƒä½"ï¼Œä½ è¦ç»“åˆå¸¸è¯†æŒ‡å‡ºè¿™æ„å‘³ç€"ç—…æ¯’æ„ŸæŸ“ï¼Œå…ç–«ç³»ç»Ÿå—å‹åˆ¶"ã€‚
        - ä¾‹å¦‚ï¼šæ–‡ä¸­è¯´"æ‰¾å…³ç³»è¿›åŒ»é™¢"ï¼Œä½ è¦ç»“åˆå¸¸è¯†æŒ‡å‡ºè¿™åæ˜ äº†"åŒ»ç–—èµ„æºæŒ¤å…‘ä¸‹çš„ç¤¾ä¼šèµ„æœ¬åšå¼ˆ"ã€‚
    3.  **æ·±åº¦ç»“è®º**ï¼šç»¼åˆåŸæ–‡å’Œå¸¸è¯†ï¼Œç»™å‡ºè¿™ä¸ªé—®é¢˜çš„æ·±åˆ»ç­”æ¡ˆã€‚
    
    è¯·ç›´æ¥è¾“å‡ºå›ç­”å†…å®¹ã€‚
    """
    
    messages = [
        SystemMessage(content=get_cached_system_prompt(full_text)),
        HumanMessage(content=task_prompt)
    ]
    
    answer = llm.invoke(messages).content
    
    # è®°å½•è¿™ä¸€è½®çš„ Q&A
    qa_entry = f"â“ **Q**: {question}\nğŸ’¡ **A**: {answer}"
    
    return {
        "qa_pairs": [qa_entry], # ç´¯åŠ åˆ° state ä¸­
        "next": "Planner"       # å›å» Planner çœ‹çœ‹è¿˜éœ€è¦é—®ä»€ä¹ˆ
    }

# === 3. Writer Node: ç»¼åˆè¾“å‡ºè€… ===
# æ”¹è¿›ç‚¹ï¼šèƒ½å¤Ÿå¤„ç†é€šç”¨é•¿æ–‡ï¼Œå°†ç¢ç‰‡åŒ–çš„ Q&A èåˆæˆè¿è´¯çš„æ·±åº¦æŠ¥å‘Š
def writer_node(state: AgentState) -> dict:
    full_text = state["full_content"]
    qa_history = state.get("qa_pairs", [])
    doc_title = state.get("doc_title", "æ–‡æ¡£")
    
    llm = get_llm()
    
    # å°† Planner å’Œ Researcher è¾›è‹¦å‡ è½®æŒ–æ˜å‡ºæ¥çš„"æ·±åº¦ç´ æ"æ‹¼æ¥èµ·æ¥
    history_text = "\n\n".join(qa_history)
    
    task_prompt = f"""
    æˆ‘ä»¬å·²ç»å®Œæˆäº†å¯¹ã€Š{doc_title}ã€‹çš„æ·±åº¦é˜…è¯»ã€‚
    
    ã€æ·±åº¦æ€è€ƒç´ æï¼ˆQ&A è®°å½•ï¼‰ã€‘
    {history_text}
    
    ã€å†™ä½œä»»åŠ¡ã€‘
    è¯·æ ¹æ®æ–‡æ¡£ç±»å‹ï¼Œåˆ©ç”¨ä¸Šè¿°ç´ æï¼Œæ’°å†™ä¸€ä»½**æ·±åº¦å¯¼è¯»ä¸åˆ†ææŠ¥å‘Š**ã€‚
    
    è¯·è‡ªé€‚åº”é€‰æ‹©ä»¥ä¸‹ç»“æ„ä¹‹ä¸€ï¼š
    
    **æ¨¡å¼ Aï¼šå¦‚æœæ˜¯ã€çºªå®/æ•…äº‹/ç¤¾ä¼šæ–°é—»ã€‘**ï¼ˆå¦‚åŒ»ç–—ç»å†ã€äººç‰©ä¼ è®°ï¼‰
    1.  **æ ¸å¿ƒå†²çªä¸èƒŒæ™¯**ï¼šç”¨ä¸€å¥è¯æ¦‚æ‹¬æ•…äº‹çš„æœ¬è´¨çŸ›ç›¾ã€‚
    2.  **å…³é”®å†³ç­–å¤ç›˜ (Timeline & Decisions)**ï¼š
        - æŒ‰æ—¶é—´çº¿æ¢³ç†å…³é”®èŠ‚ç‚¹ã€‚
        - **é‡ç‚¹åˆ†æ**ï¼šåœ¨å“ªäº›èŠ‚ç‚¹åšé”™äº†ï¼Ÿç»“åˆå¸¸è¯†ï¼Œæ­£ç¡®çš„åšæ³•åº”è¯¥æ˜¯ä»€ä¹ˆï¼Ÿ
    3.  **æ·±å±‚ç¤¾ä¼š/äººæ€§æ´å¯Ÿ**ï¼š
        - é€è¿‡æ•…äº‹è¡¨è±¡ï¼Œçœ‹åˆ°äº†ä»€ä¹ˆç¤¾ä¼šè¿ä½œé€»è¾‘ï¼ˆå¦‚åŒ»ç–—èµ„æºã€å®¶åº­å…³ç³»ï¼‰ï¼Ÿ
    4.  **è­¦ç¤ºä¸è¡ŒåŠ¨æŒ‡å—**ï¼š
        - æ™®é€šè¯»è€…è¯»å®Œè¿™ç¯‡é•¿æ–‡ï¼Œæ˜å¤©åº”è¯¥åšä»€ä¹ˆæ”¹å˜ï¼Ÿ
    
    **æ¨¡å¼ Bï¼šå¦‚æœæ˜¯ã€æŠ€æœ¯/å­¦æœ¯/è¯´æ˜æ–‡ã€‘**
    1.  **æ ¸å¿ƒç†å¿µ (The Big Idea)**ï¼šä¸€å¥è¯è§£é‡Šå®ƒè§£å†³äº†ä»€ä¹ˆé—®é¢˜ã€‚
    2.  **å®ç°é€»è¾‘/æ¶æ„æ‹†è§£**ï¼šåŸºäº Q&A ç´ æï¼Œè§£é‡Šå…¶è¿ä½œåŸç†ã€‚
    3.  **ä¼˜åŠ£åŠ¿æ·±åº¦è¾©è¯**ï¼šç»“åˆå¸¸è¯†ï¼Œåˆ†æå®ƒçš„å±€é™æ€§åœ¨å“ªé‡Œï¼Ÿ
    4.  **åº”ç”¨åœºæ™¯**ï¼šåˆ°åº•è¯¥åœ¨ä»€ä¹ˆæƒ…å†µä¸‹ä½¿ç”¨ï¼Ÿ
    
    ã€è¦æ±‚ã€‘
    - æ ‡é¢˜è‡ªæ‹Ÿï¼Œå…·æœ‰å¸å¼•åŠ›ã€‚
    - å¿…é¡»å……åˆ†åˆ©ç”¨ Q&A ä¸­çš„åˆ†ææˆæœï¼Œä¸è¦å¿½ç•¥ Planner çš„åŠ³åŠ¨ã€‚
    - è¯­æ°”ä¸“ä¸šã€å®¢è§‚ã€æœ‰æ·±åº¦ã€‚
    """
    
    messages = [
        SystemMessage(content=get_cached_system_prompt(full_text)),
        HumanMessage(content=task_prompt)
    ]
    
    report = llm.invoke(messages).content
    
    return {
        "final_report": report,
        "next": "Outlooker" # ä¾ç„¶ä¿ç•™ Outlooker åšæœ€åçš„å»¶ä¼¸
    }

# === 4. Outlooker Node: æœ€åçš„å‡å + æ€è€ƒè¿‡ç¨‹å­˜æ¡£ ===
def outlook_node(state: AgentState) -> dict:
    full_text = state["full_content"]
    current_report = state["final_report"]
    # è·å–ç§¯ç´¯çš„æ‰€æœ‰é—®ç­”å¯¹ï¼ˆæ€è€ƒè¿‡ç¨‹ï¼‰
    qa_history = state.get("qa_pairs", [])
    
    llm = get_llm()
    
    # 1. ç”Ÿæˆ Outlook å†…å®¹ (ä¿æŒåŸæœ‰é€»è¾‘)
    task_prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªæå…¶æ³¨é‡å®ç”¨çš„å’¨è¯¢é¡¾é—®ã€‚
    è¯·é˜…è¯»å½“å‰çš„åˆ†ææŠ¥å‘Šï¼Œå¹¶å¢åŠ ä¸€ä¸ª **# ğŸš€ æ‰©å±•æ€è€ƒä¸èµ„æº** ç« èŠ‚ã€‚
    
    - å¦‚æœæ˜¯æ•…äº‹/æ¡ˆä¾‹ï¼šæ¨èç›¸å…³çš„ä¹¦ç±ã€ç”µå½±æˆ–æ€¥æ•‘çŸ¥è¯†ã€‚
    - å¦‚æœæ˜¯æŠ€æœ¯ï¼šæ¨èç›¸å…³çš„ GitHub åº“ã€æ›¿ä»£æ–¹æ¡ˆå¯¹æ¯”ã€‚
    
    è¯·ç›´æ¥è¾“å‡º Markdown å†…å®¹è¿½åŠ åˆ°æœ«å°¾ã€‚
    """
    
    messages = [
        SystemMessage(content=get_cached_system_prompt(full_text)),
        HumanMessage(content=task_prompt)
    ]
    
    outlook_content = llm.invoke(messages).content
    
    # 2. æ‹¼æ¥ï¼šåŸæŠ¥å‘Š + Outlook
    final_full_report = current_report + "\n\n" + outlook_content
    
    # === 3. æ–°å¢æ ¸å¿ƒé€»è¾‘ï¼šå°†æ€è€ƒè¿‡ç¨‹è¿½åŠ åˆ°æ–‡æœ« ===
    # ä½¿ç”¨ HTML <details> æ ‡ç­¾å®ç°æŠ˜å æ•ˆæœï¼Œæ—¢ä¿ç•™äº†æ•°æ®ï¼Œåˆä¸å½±å“é˜…è¯»ä½“éªŒ
    if qa_history:
        log_section = "\n\n---\n\n<details>\n<summary>ğŸ§  ç‚¹å‡»æŸ¥çœ‹ AI å®Œæ•´æ€è€ƒä¸æ¨æ¼”è¿‡ç¨‹ (Trace Logs)</summary>\n\n"
        
        log_section += "> ä»¥ä¸‹è®°å½•äº† Agent ä»é˜…è¯»åˆ°æé—®ã€å†åˆ°ç»“åˆå¸¸è¯†æ¨ç†çš„å®Œæ•´æ€ç»´é“¾ã€‚\n\n"
        
        for i, pair in enumerate(qa_history):
            # pair çš„æ ¼å¼å·²ç»æ˜¯ "â“ Q: ... \nğŸ’¡ A: ..."
            # æˆ‘ä»¬ç¨å¾®ç¾åŒ–ä¸€ä¸‹æ ¼å¼
            log_section += f"#### ğŸ”„ ç¬¬ {i+1} è½®æ€è€ƒ\n"
            log_section += f"{pair}\n\n"
            
        log_section += "</details>\n"
        
        final_full_report += log_section
    
    return {
        "final_report": final_full_report,
        "next": "END"
    }

# ==========================================
# PART 2: æ·±åº¦é—®ç­”æµ (Deep QA) - å…¨æ–°ä»£ç 
# ==========================================

# 1. QA ä¸“ç”¨è§„åˆ’å™¨ï¼šç›®çš„æ˜¯"æ‹†è§£ç”¨æˆ·é—®é¢˜"ï¼Œè€Œä¸æ˜¯"å‘ç°æ–‡ç« äº®ç‚¹"
def qa_planner_node(state: AgentState) -> dict:
    full_text = state["full_content"]
    qa_history = state.get("qa_pairs", [])
    user_goal = state["user_goal"] # ç”¨æˆ·çš„é—®é¢˜
    loop = state.get("loop_count", 0)
    MAX_LOOPS = 5  # é—®ç­”æ¨¡å¼å…è®¸æ›´å¤šè½®æ¬¡ä»¥ç¡®ä¿å‡†ç¡®
    
    llm = get_llm()
    history_text = "\n".join(qa_history) if qa_history else "ï¼ˆæš‚æ— ï¼Œç¬¬ä¸€è½®åˆ†æï¼‰"
    
    task_prompt = f"""
    å½“å‰æ€è€ƒè½®æ¬¡: {loop + 1}/{MAX_LOOPS}
    
    ã€ç”¨æˆ·æå‡ºçš„æ ¸å¿ƒé—®é¢˜ã€‘
    "{user_goal}"
    
    ã€æˆ‘ä»¬å·²ä»æ–‡ä¸­æŸ¥è¯çš„ä¿¡æ¯ã€‘
    {history_text}
    
    ã€ä»»åŠ¡ç›®æ ‡ã€‘
    ä½ çš„å”¯ä¸€ç›®æ ‡æ˜¯å®Œæ•´ã€å‡†ç¡®åœ°å›ç­”ç”¨æˆ·çš„æ ¸å¿ƒé—®é¢˜ã€‚
    è¯·åˆ¤æ–­ï¼šåŸºäºã€å·²æŸ¥è¯çš„ä¿¡æ¯ã€‘ï¼Œæˆ‘ä»¬æ˜¯å¦å·²ç»èƒ½å®Œç¾å›ç­”è¿™ä¸ªé—®é¢˜ï¼Ÿ
    
    - å¦‚æœè¿˜ç¼ºä¿¡æ¯ï¼ˆä¾‹å¦‚ç”¨æˆ·é—®å¯¹æ¯”ï¼Œä½†æˆ‘ä»¬åªæŸ¥äº†Aæ–¹ï¼‰ï¼Œè¯·æå‡ºä¸‹ä¸€ä¸ª**å…·ä½“çš„å­é—®é¢˜**ã€‚
    - å¦‚æœç”¨æˆ·é—®çš„æ˜¯ç»†èŠ‚ï¼ˆå¦‚æ•°æ®ï¼‰ï¼Œè¯·é€šè¿‡å­é—®é¢˜åå¤ç¡®è®¤ä¸Šä¸‹æ–‡ã€‚
    
    ã€è¾“å‡ºè¦æ±‚ã€‘
    - å¦‚æœä¿¡æ¯å·²å……è¶³ï¼Œè¯·ç›´æ¥è¾“å‡º "TERMINATE"ã€‚
    - å¦åˆ™ï¼Œè¾“å‡ºä¸€ä¸ª**ä¸ºäº†å›ç­”æ ¸å¿ƒé—®é¢˜å¿…é¡»ææ¸…æ¥šçš„å­é—®é¢˜**ã€‚
    """
    
    messages = [
        SystemMessage(content=get_cached_system_prompt(full_text)),
        HumanMessage(content=task_prompt)
    ]
    
    response = llm.invoke(messages).content.strip()
    question = response.replace('"', '').replace("'", "")
    
    if "TERMINATE" in response or loop >= MAX_LOOPS:
        return {"next": "QAWriter", "current_question": ""}
    else:
        # å¤ç”¨é€šç”¨çš„ Researcherï¼Œå› ä¸ºå®ƒå°±æ˜¯è´Ÿè´£"å»æ–‡ä¸­æ‰¾ç­”æ¡ˆ"çš„
        return {
            "next": "Researcher", 
            "current_question": question, 
            "loop_count": loop + 1
        }

# 2. QA ä¸“ç”¨æ’°å†™è€…ï¼šç›®çš„æ˜¯"ç›´æ¥å›ç­”é—®é¢˜"ï¼Œè€Œä¸æ˜¯"å†™å¯¼è¯»æŠ¥å‘Š"
def qa_writer_node(state: AgentState) -> dict:
    full_text = state["full_content"]
    qa_history = state.get("qa_pairs", [])
    doc_title = state.get("doc_title", "æ–‡æ¡£")
    user_goal = state["user_goal"]
    
    llm = get_llm()
    history_text = "\n\n".join(qa_history)
    
    task_prompt = f"""
    æˆ‘ä»¬é’ˆå¯¹æ–‡æ¡£ã€Š{doc_title}ã€‹è¿›è¡Œäº†é’ˆå¯¹æ€§çš„æ·±åº¦è°ƒç ”ã€‚
    
    ã€ç”¨æˆ·æé—®ã€‘
    {user_goal}
    
    ã€è°ƒç ”è¿‡ç¨‹ä¸å‘ç°ã€‘
    {history_text}
    
    ã€ä»»åŠ¡ã€‘
    è¯·åŸºäºä¸Šè¿°è°ƒç ”å‘ç°ï¼Œæ’°å†™æœ€ç»ˆå›ç­”ã€‚
    
    ã€è¦æ±‚ã€‘
    1. **ç›´å‡»ç—›ç‚¹**ï¼šç¬¬ä¸€å¥è¯ç›´æ¥ç»™å‡ºæ ¸å¿ƒç»“è®ºã€‚
    2. **è¯æ®ç¡®å‡¿**ï¼šå¼•ç”¨æ–‡ä¸­çš„å…·ä½“æ®µè½æˆ–æ•°æ®æ¥æ”¯æŒä½ çš„è§‚ç‚¹ï¼ˆåŸºäºè°ƒç ”å‘ç°ï¼‰ã€‚
    3. **é€»è¾‘é—­ç¯**ï¼šå¦‚æœæ–‡ä¸­æ²¡æœ‰ç›´æ¥ç­”æ¡ˆï¼Œè¯·æ ¹æ®æ–‡ä¸­çš„çº¿ç´¢è¿›è¡Œåˆç†æ¨æ–­ï¼Œå¹¶æ³¨æ˜è¿™æ˜¯æ¨æ–­ã€‚
    4. ä¸è¦å†™æˆ"å¯¼è¯»"æˆ–"è¯»åæ„Ÿ"ï¼Œè¦å†™æˆä¸“ä¸šçš„"ç­”æ¡ˆ"ã€‚
    """
    
    messages = [
        SystemMessage(content=get_cached_system_prompt(full_text)),
        HumanMessage(content=task_prompt)
    ]
    
    answer = llm.invoke(messages).content
    
    # é—®ç­”æ¨¡å¼ç»“æŸåï¼Œç›´æ¥ç»“æŸï¼Œä¸éœ€è¦ Outlookerï¼ˆæ‰©å±•æ€è€ƒï¼‰ï¼Œæˆ–è€…ä½ ä¹Ÿå¯ä»¥ä¿ç•™
    # è¿™é‡Œæˆ‘ä»¬é€‰æ‹©ç›´æ¥ç»“æŸï¼Œè®©ä½“éªŒæ›´åƒ"é—®ç­”"
    return {
        "final_report": answer,
        "next": "END"
    }

# 3. å¤ç”¨èŠ‚ç‚¹ (Researcher)
# æ³¨æ„ï¼šæˆ‘ä»¬éœ€è¦ç¡®ä¿åŸæœ‰ deep_flow.py é‡Œæœ‰ researcher_node
# å¦‚æœæ²¡æœ‰ï¼ˆä¹‹å‰æ˜¯åœ¨ nodes.pyï¼‰ï¼Œè¿™é‡Œéœ€è¦å®šä¹‰å®ƒï¼Œæˆ–è€…ä»åŸå¤„ import
# è¿™é‡Œä¸ºäº†å®Œæ•´æ€§å†™ä¸€é Researcherï¼Œå®ƒæ˜¯ä¸¤ä¸ª Graph å…±ç”¨çš„æ ¸å¿ƒèƒ½åŠ›
def researcher_node(state: AgentState) -> dict:
    full_text = state["full_content"]
    question = state["current_question"]
    llm = get_llm()
    
    task_prompt = f"""
    ã€å¾…è§£å†³é—®é¢˜ã€‘{question}
    ã€è¦æ±‚ã€‘
    1. è¯·ä»”ç»†é˜…è¯»ç¼“å­˜çš„å…¨æ–‡ï¼Œæ‰¾åˆ°åŸæ–‡ä¾æ®ã€‚
    2. ç»“åˆå¸¸è¯†è¿›è¡Œç®€çŸ­åˆ†æã€‚
    3. ç›´æ¥å›ç­”è¿™ä¸ªé—®é¢˜ã€‚
    """
    
    messages = [
        SystemMessage(content=get_cached_system_prompt(full_text)),
        HumanMessage(content=task_prompt)
    ]
    
    answer = llm.invoke(messages).content
    qa_entry = f"â“ **Q**: {question}\nğŸ’¡ **A**: {answer}"
    
    # å…³é”®ï¼šResearcher ä¸å†³å®šä¸‹ä¸€æ­¥å»å“ªï¼Œå®ƒåªè´Ÿè´£æŠŠç»“æœå¡è¿› state
    # å…·ä½“çš„è·¯ç”±ç”± Graph çš„ Edge å†³å®š
    return {"qa_pairs": [qa_entry]} 


# ==========================================
# æ„å»ºå›¾ (ä¿æŒä¸å˜) ===
def build_graph():
    workflow = StateGraph(AgentState)
    workflow.add_node("Planner", planner_node)
    workflow.add_node("Researcher", researcher_node)
    workflow.add_node("Writer", writer_node)
    workflow.add_node("Outlooker", outlook_node)

    workflow.set_entry_point("Planner")

    def route(state): return state["next"]

    workflow.add_conditional_edges(
        "Planner", route, 
        {"Researcher": "Researcher", "Writer": "Writer"}
    )
    
    workflow.add_edge("Researcher", "Planner")
    workflow.add_edge("Writer", "Outlooker")
    workflow.add_edge("Outlooker", END)
    
    return workflow.compile()

deep_graph = build_graph()

# ==========================================
# æ„å»º QA ä¸“ç”¨ Graph
# ==========================================

qa_workflow = StateGraph(AgentState)

# æ·»åŠ èŠ‚ç‚¹
qa_workflow.add_node("QAPlanner", qa_planner_node)
qa_workflow.add_node("Researcher", researcher_node)
qa_workflow.add_node("QAWriter", qa_writer_node)

# è®¾ç½®å…¥å£
qa_workflow.set_entry_point("QAPlanner")

# æ·»åŠ è¾¹
# 1. Planner å†³å®šæ˜¯å»æŸ¥èµ„æ–™(Researcher) è¿˜æ˜¯ å†™ç­”æ¡ˆ(QAWriter)
qa_workflow.add_conditional_edges(
    "QAPlanner", 
    lambda x: x["next"], 
    {"Researcher": "Researcher", "QAWriter": "QAWriter"}
)

# 2. Researcher æŸ¥å®Œèµ„æ–™ï¼Œå¿…é¡»å› QAPlanner ç»§ç»­è§„åˆ’
qa_workflow.add_edge("Researcher", "QAPlanner")

# 3. Writer å†™å®Œç›´æ¥ç»“æŸ
qa_workflow.add_edge("QAWriter", END)

deep_qa_graph = qa_workflow.compile()

# å¯¼å‡ºä¸¤ä¸ªå›¾ï¼šdeep_graph (åŸæœ‰æ·±åº¦è§£è¯») å’Œ deep_qa_graph (æ–°å¢æ·±åº¦é—®ç­”)
__all__ = ['deep_graph', 'deep_qa_graph']