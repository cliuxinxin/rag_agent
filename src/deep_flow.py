import os
from langchain_core.messages import HumanMessage, SystemMessage
from langgraph.graph import StateGraph, END
from src.state import AgentState
from src.nodes import get_llm

# === 0. ç¼“å­˜æ„ŸçŸ¥ System Prompt (ä¿æŒä¸å˜) ===
def get_cached_system_prompt(content: str) -> str:
    return f"""ä½ æ˜¯ä¸€ä¸ªå¤„äº"DeepSeek Context Caching"æ¨¡å¼ä¸‹çš„é¡¶çº§æ·±åº¦é˜…è¯»ä¸“å®¶ã€‚
ä»¥ä¸‹æ˜¯æˆ‘ä»¬éœ€è¦æ·±åº¦å‰–æçš„æ–‡æ¡£å…¨æ–‡ï¼ˆå·²ç¼“å­˜ï¼‰ï¼Œè¯·ä»”ç»†é˜…è¯»æ¯ä¸€ä¸ªæ®µè½ï¼š

<DOCUMENT_START>
{content}
<DOCUMENT_END>
"""

# === 1. Planner Node: çœŸæ­£çš„"é˜…è¯»ç­–ç•¥å®¶" ===
# æ”¹è¿›ç‚¹ï¼šå…·å¤‡äº†"é€šç”¨é˜…è¯»èƒ½åŠ›"ï¼Œèƒ½å¤„ç†å™äº‹ã€æ–°é—»ã€è¯„è®ºç­‰éæŠ€æœ¯æ–‡ç« 
def planner_node(state: AgentState) -> dict:
    full_text = state["full_content"]
    qa_history = state.get("qa_pairs", [])
    loop = state.get("loop_count", 0)
    MAX_LOOPS = 4 
    
    llm = get_llm()
    
    # æ ¼å¼åŒ–å·²æœ‰çš„é—®ç­”å¯¹ï¼Œè®© Planner çŸ¥é“æˆ‘ä»¬å·²ç»ææ‡‚äº†ä»€ä¹ˆ
    history_text = "\n".join(qa_history) if qa_history else "ï¼ˆæš‚æ— ï¼Œè¿™æ˜¯ç¬¬ä¸€è½®åˆ†æï¼‰"
    
    # === æ ¸å¿ƒä¿®æ”¹ï¼šé€šç”¨çš„æ·±åº¦é˜…è¯» Prompt ===
    # è¿™ä¸€æ­¥å®ç°äº†æ‚¨è¯´çš„"ç²—ç•¥é˜…è¯»å¹¶æå‡ºé—®é¢˜"
    task_prompt = f"""
    å½“å‰åˆ†æè½®æ¬¡: {loop + 1}/{MAX_LOOPS}
    
    ã€æˆ‘ä»¬å·²æœ‰çš„ç†è§£ï¼ˆå·²è§£å†³çš„é—®é¢˜ï¼‰ã€‘
    {history_text}
    
    ã€ä»»åŠ¡ç›®æ ‡ã€‘
    è¯·å…ˆå¿«é€Ÿé€šè¯»å…¨æ–‡ï¼Œåˆ¤æ–­æ–‡ç« ä½“è£ï¼ˆæ˜¯æŠ€æœ¯æ–‡æ¡£ã€çºªå®å™äº‹ã€æ–°é—»æŠ¥é“ï¼Œè¿˜æ˜¯è§‚ç‚¹è¯„è®ºï¼Ÿï¼‰ã€‚
    ç„¶åï¼Œæå‡ºä¸‹ä¸€ä¸ªæœ€å€¼å¾—æŒ–æ˜çš„**"æ·±åº¦é—®é¢˜"**ã€‚
    
    ã€æé—®ç­–ç•¥ã€‘
    ä¸è¦é—®æµ…æ˜¾çš„äº‹å®ï¼ˆå¦‚"ä½œè€…æ˜¯è°ï¼Ÿ"ï¼‰ï¼Œè¦é—®éœ€è¦**ç»“åˆåŸæ–‡ä¸å¸¸è¯†**æ‰èƒ½å›ç­”çš„é—®é¢˜ï¼š
    
    1.  **å¦‚æœæ˜¯ã€çºªå®/å™äº‹ã€‘ï¼ˆå¦‚ï¼šã€Šæµæ„Ÿä¸‹çš„åŒ—äº¬ä¸­å¹´ã€‹ï¼‰**ï¼š
        - å…³æ³¨**å› æœé“¾ä¸å†³ç­–ç‚¹**ï¼šä¾‹å¦‚"ä¸ºä»€ä¹ˆæ–‡ä¸­æåˆ°çš„æŸä¸ªå†³å®šå¯¼è‡´äº†åç»­çš„å´©ç›˜ï¼Ÿå¸¸è¯†ä¸Šåº”è¯¥æ€ä¹ˆåšï¼Ÿ"
        - å…³æ³¨**èµ„æºä¸åšå¼ˆ**ï¼šä¾‹å¦‚"åœ¨èµ„æºï¼ˆå¦‚ICUã€é‡‘é’±ï¼‰å—é™æ—¶ï¼Œä¸»è§’é¢ä¸´äº†ä»€ä¹ˆæ ·çš„äººæ€§è€ƒéªŒï¼Ÿ"
        
    2.  **å¦‚æœæ˜¯ã€è§‚ç‚¹/è¯„è®ºã€‘**ï¼š
        - å…³æ³¨**é€»è¾‘æ¼æ´ä¸åº•å±‚å‡è®¾**ï¼šä½œè€…åŸºäºä»€ä¹ˆé¢„è®¾å‰æï¼Ÿè¿™äº›å‰æåœ¨ç°å®ä¸­æˆç«‹å—ï¼Ÿ
        
    3.  **å¦‚æœæ˜¯ã€æŠ€æœ¯/ç§‘æ™®ã€‘**ï¼š
        - å…³æ³¨**æ ¸å¿ƒåŸç†ä¸åº”ç”¨è¾¹ç•Œ**ï¼šè¿™ä¸ªæŠ€æœ¯è§£å†³äº†ä»€ä¹ˆæœ¬è´¨é—®é¢˜ï¼Ÿæœ‰ä»€ä¹ˆä»£ä»·ï¼Ÿ
        
    ã€è¾“å‡ºè¦æ±‚ã€‘
    - å¦‚æœä½ è§‰å¾—æ–‡ç« çš„æ ¸å¿ƒé€»è¾‘ã€å…³é”®å†³ç­–ã€æ·±å±‚å«ä¹‰éƒ½å·²ç»åˆ†æé€å½»äº†ï¼Œè¾“å‡º "TERMINATE"ã€‚
    - å¦åˆ™ï¼Œè¾“å‡º**ä¸€ä¸ª**å…·ä½“çš„ã€æœ‰æ·±åº¦çš„é—®é¢˜ã€‚
    """
    
    messages = [
        SystemMessage(content=get_cached_system_prompt(full_text)),
        HumanMessage(content=task_prompt)
    ]
    
    response = llm.invoke(messages).content.strip()
    
    # æ¸…ç†ä¸€ä¸‹å¼•å·
    question = response.replace('"', '').replace("'", "")
    
    if "TERMINATE" in response or loop >= MAX_LOOPS:
        # å¦‚æœåˆ†æå®Œäº†ï¼Œè½¬äº¤ç»™ä½œå®¶è¿›è¡Œç»¼åˆè¾“å‡º
        return {"next": "Writer", "current_question": ""}
    else:
        # è¿˜æœ‰é—®é¢˜æ²¡ææ‡‚ï¼Œäº¤ç»™ç ”ç©¶å‘˜å»æŸ¥
        return {
            "next": "Researcher", 
            "current_question": question, 
            "loop_count": loop + 1
        }

# === 2. Researcher Node: ç»“åˆå¸¸è¯†çš„è§£ç­”è€… ===
# æ”¹è¿›ç‚¹ï¼šæ˜ç¡®è¦æ±‚"ç»“åˆå¸¸è¯†"è¿›è¡Œè§£ç­”
def researcher_node(state: AgentState) -> dict:
    full_text = state["full_content"]
    question = state["current_question"]
    
    llm = get_llm()
    
    task_prompt = f"""
    ã€å½“å‰å¾…æ”»å…‹çš„é—®é¢˜ã€‘
    {question}
    
    ã€å›ç­”è¦æ±‚ã€‘
    ä½ ä¸ä»…ä»…æ˜¯ä¸€ä¸ªæ‘˜å½•æœºå™¨ï¼Œä½ æ˜¯ä¸€ä¸ªæœ‰æ™ºæ…§çš„åˆ†æå¸ˆã€‚è¯·æŒ‰ä»¥ä¸‹æ­¥éª¤å›ç­”ï¼š
    
    1.  **åŸæ–‡è¯æ®**ï¼šé¦–å…ˆï¼Œä»æ–‡ä¸­æ‰¾åˆ°ç›¸å…³çš„æ®µè½ã€å¯¹è¯æˆ–æ•°æ®ä½œä¸ºä¾æ®ã€‚
    2.  **å¸¸è¯†èåˆ**ï¼š**å…³é”®æ­¥éª¤ï¼** è¯·è°ƒç”¨ä½ çš„å¤–éƒ¨çŸ¥è¯†åº“ï¼ˆå¸¸è¯†ã€åŒ»å­¦å¸¸è¯†ã€ç¤¾ä¼šè¿ä½œé€»è¾‘ã€æŠ€æœ¯åŸç†ï¼‰ï¼Œå¯¹åŸæ–‡å†…å®¹è¿›è¡Œè§£è¯»ã€‚
        - ä¾‹å¦‚ï¼šæ–‡ä¸­è¯´"ç™½ç»†èƒä½"ï¼Œä½ è¦ç»“åˆå¸¸è¯†æŒ‡å‡ºè¿™æ„å‘³ç€"ç—…æ¯’æ„ŸæŸ“ï¼Œå…ç–«ç³»ç»Ÿå—å‹åˆ¶"ã€‚
        - ä¾‹å¦‚ï¼šæ–‡ä¸­è¯´"æ‰¾å…³ç³»è¿›åŒ»é™¢"ï¼Œä½ è¦ç»“åˆå¸¸è¯†æŒ‡å‡ºè¿™åæ˜ äº†"åŒ»ç–—èµ„æºæŒ¤å…‘ä¸‹çš„ç¤¾ä¼šèµ„æœ¬åšå¼ˆ"ã€‚
    3.  **æ·±åº¦ç»“è®º**ï¼šç»¼åˆåŸæ–‡å’Œå¸¸è¯†ï¼Œç»™å‡ºè¿™ä¸ªé—®é¢˜çš„æ·±åˆ»ç­”æ¡ˆã€‚
    
    è¯·ç›´æ¥è¾“å‡ºå›ç­”å†…å®¹ã€‚
    """
    
    messages = [
        SystemMessage(content=get_cached_system_prompt(full_text)),
        HumanMessage(content=task_prompt)
    ]
    
    answer = llm.invoke(messages).content
    
    # è®°å½•è¿™ä¸€è½®çš„ Q&A
    qa_entry = f"â“ **Q**: {question}\nğŸ’¡ **A**: {answer}"
    
    return {
        "qa_pairs": [qa_entry], # ç´¯åŠ åˆ° state ä¸­
        "next": "Planner"       # å›å» Planner çœ‹çœ‹è¿˜éœ€è¦é—®ä»€ä¹ˆ
    }

# === 3. Writer Node: ç»¼åˆè¾“å‡ºè€… ===
# æ”¹è¿›ç‚¹ï¼šèƒ½å¤Ÿå¤„ç†é€šç”¨é•¿æ–‡ï¼Œå°†ç¢ç‰‡åŒ–çš„ Q&A èåˆæˆè¿è´¯çš„æ·±åº¦æŠ¥å‘Š
def writer_node(state: AgentState) -> dict:
    full_text = state["full_content"]
    qa_history = state.get("qa_pairs", [])
    doc_title = state.get("doc_title", "æ–‡æ¡£")
    
    llm = get_llm()
    
    # å°† Planner å’Œ Researcher è¾›è‹¦å‡ è½®æŒ–æ˜å‡ºæ¥çš„"æ·±åº¦ç´ æ"æ‹¼æ¥èµ·æ¥
    history_text = "\n\n".join(qa_history)
    
    task_prompt = f"""
    æˆ‘ä»¬å·²ç»å®Œæˆäº†å¯¹ã€Š{doc_title}ã€‹çš„æ·±åº¦é˜…è¯»ã€‚
    
    ã€æ·±åº¦æ€è€ƒç´ æï¼ˆQ&A è®°å½•ï¼‰ã€‘
    {history_text}
    
    ã€å†™ä½œä»»åŠ¡ã€‘
    è¯·æ ¹æ®æ–‡æ¡£ç±»å‹ï¼Œåˆ©ç”¨ä¸Šè¿°ç´ æï¼Œæ’°å†™ä¸€ä»½**æ·±åº¦å¯¼è¯»ä¸åˆ†ææŠ¥å‘Š**ã€‚
    
    è¯·è‡ªé€‚åº”é€‰æ‹©ä»¥ä¸‹ç»“æ„ä¹‹ä¸€ï¼š
    
    **æ¨¡å¼ Aï¼šå¦‚æœæ˜¯ã€çºªå®/æ•…äº‹/ç¤¾ä¼šæ–°é—»ã€‘**ï¼ˆå¦‚åŒ»ç–—ç»å†ã€äººç‰©ä¼ è®°ï¼‰
    1.  **æ ¸å¿ƒå†²çªä¸èƒŒæ™¯**ï¼šç”¨ä¸€å¥è¯æ¦‚æ‹¬æ•…äº‹çš„æœ¬è´¨çŸ›ç›¾ã€‚
    2.  **å…³é”®å†³ç­–å¤ç›˜ (Timeline & Decisions)**ï¼š
        - æŒ‰æ—¶é—´çº¿æ¢³ç†å…³é”®èŠ‚ç‚¹ã€‚
        - **é‡ç‚¹åˆ†æ**ï¼šåœ¨å“ªäº›èŠ‚ç‚¹åšé”™äº†ï¼Ÿç»“åˆå¸¸è¯†ï¼Œæ­£ç¡®çš„åšæ³•åº”è¯¥æ˜¯ä»€ä¹ˆï¼Ÿ
    3.  **æ·±å±‚ç¤¾ä¼š/äººæ€§æ´å¯Ÿ**ï¼š
        - é€è¿‡æ•…äº‹è¡¨è±¡ï¼Œçœ‹åˆ°äº†ä»€ä¹ˆç¤¾ä¼šè¿ä½œé€»è¾‘ï¼ˆå¦‚åŒ»ç–—èµ„æºã€å®¶åº­å…³ç³»ï¼‰ï¼Ÿ
    4.  **è­¦ç¤ºä¸è¡ŒåŠ¨æŒ‡å—**ï¼š
        - æ™®é€šè¯»è€…è¯»å®Œè¿™ç¯‡é•¿æ–‡ï¼Œæ˜å¤©åº”è¯¥åšä»€ä¹ˆæ”¹å˜ï¼Ÿ
    
    **æ¨¡å¼ Bï¼šå¦‚æœæ˜¯ã€æŠ€æœ¯/å­¦æœ¯/è¯´æ˜æ–‡ã€‘**
    1.  **æ ¸å¿ƒç†å¿µ (The Big Idea)**ï¼šä¸€å¥è¯è§£é‡Šå®ƒè§£å†³äº†ä»€ä¹ˆé—®é¢˜ã€‚
    2.  **å®ç°é€»è¾‘/æ¶æ„æ‹†è§£**ï¼šåŸºäº Q&A ç´ æï¼Œè§£é‡Šå…¶è¿ä½œåŸç†ã€‚
    3.  **ä¼˜åŠ£åŠ¿æ·±åº¦è¾©è¯**ï¼šç»“åˆå¸¸è¯†ï¼Œåˆ†æå®ƒçš„å±€é™æ€§åœ¨å“ªé‡Œï¼Ÿ
    4.  **åº”ç”¨åœºæ™¯**ï¼šåˆ°åº•è¯¥åœ¨ä»€ä¹ˆæƒ…å†µä¸‹ä½¿ç”¨ï¼Ÿ
    
    ã€è¦æ±‚ã€‘
    - æ ‡é¢˜è‡ªæ‹Ÿï¼Œå…·æœ‰å¸å¼•åŠ›ã€‚
    - å¿…é¡»å……åˆ†åˆ©ç”¨ Q&A ä¸­çš„åˆ†ææˆæœï¼Œä¸è¦å¿½ç•¥ Planner çš„åŠ³åŠ¨ã€‚
    - è¯­æ°”ä¸“ä¸šã€å®¢è§‚ã€æœ‰æ·±åº¦ã€‚
    """
    
    messages = [
        SystemMessage(content=get_cached_system_prompt(full_text)),
        HumanMessage(content=task_prompt)
    ]
    
    report = llm.invoke(messages).content
    
    return {
        "final_report": report,
        "next": "Outlooker" # ä¾ç„¶ä¿ç•™ Outlooker åšæœ€åçš„å»¶ä¼¸
    }

# === 4. Outlooker Node: æœ€åçš„å‡å ===
# ä¿æŒåŸæœ‰çš„é€»è¾‘ï¼Œåšæ‰©å±•å»¶ä¼¸ï¼Œæ•ˆæœä¾ç„¶å¾ˆå¥½
def outlook_node(state: AgentState) -> dict:
    full_text = state["full_content"]
    current_report = state["final_report"]
    
    llm = get_llm()
    
    task_prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªæå…¶æ³¨é‡å®ç”¨çš„å’¨è¯¢é¡¾é—®ã€‚
    è¯·é˜…è¯»å½“å‰çš„åˆ†ææŠ¥å‘Šï¼Œå¹¶å¢åŠ ä¸€ä¸ª **# ğŸš€ æ‰©å±•æ€è€ƒä¸èµ„æº** ç« èŠ‚ã€‚
    
    - å¦‚æœæ˜¯æ•…äº‹/æ¡ˆä¾‹ï¼šæ¨èç›¸å…³çš„ä¹¦ç±ã€ç”µå½±æˆ–æ€¥æ•‘çŸ¥è¯†ï¼ˆå¦‚æ¨èã€Šæœ€å¥½çš„å‘Šåˆ«ã€‹ã€å¿ƒè‚ºå¤è‹æŒ‡å—ï¼‰ã€‚
    - å¦‚æœæ˜¯æŠ€æœ¯ï¼šæ¨èç›¸å…³çš„ GitHub åº“ã€æ›¿ä»£æ–¹æ¡ˆå¯¹æ¯”ã€‚
    
    è¯·ç›´æ¥è¾“å‡º Markdown å†…å®¹è¿½åŠ åˆ°æœ«å°¾ã€‚
    """
    
    messages = [
        SystemMessage(content=get_cached_system_prompt(full_text)),
        HumanMessage(content=task_prompt)
    ]
    
    outlook_content = llm.invoke(messages).content
    final_full_report = current_report + "\n\n" + outlook_content
    
    return {
        "final_report": final_full_report,
        "next": "END"
    }

# === æ„å»ºå›¾ (ä¿æŒä¸å˜) ===
def build_graph():
    workflow = StateGraph(AgentState)
    workflow.add_node("Planner", planner_node)
    workflow.add_node("Researcher", researcher_node)
    workflow.add_node("Writer", writer_node)
    workflow.add_node("Outlooker", outlook_node)

    workflow.set_entry_point("Planner")

    def route(state): return state["next"]

    workflow.add_conditional_edges(
        "Planner", route, 
        {"Researcher": "Researcher", "Writer": "Writer"}
    )
    
    workflow.add_edge("Researcher", "Planner")
    workflow.add_edge("Writer", "Outlooker")
    workflow.add_edge("Outlooker", END)
    
    return workflow.compile()

deep_graph = build_graph()