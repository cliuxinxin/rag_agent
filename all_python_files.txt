./code.py
--------------------------------------------------------------------------------
import os

def save_python_files(base_dir):
    output_path = os.path.join(base_dir, "all_python_files.txt")

    with open(output_path, "w", encoding="utf-8") as out:
        for root, dirs, files in os.walk(base_dir):
            # å»æ‰ venv
            if "venv" in dirs:
                dirs.remove("venv")

            for fname in files:
                if fname.endswith(".py"):
                    full_path = os.path.join(root, fname)

                    out.write(full_path + "\n")
                    out.write("-" * 80 + "\n")

                    try:
                        with open(full_path, "r", encoding="utf-8") as f:
                            out.write(f.read())
                    except UnicodeDecodeError:
                        with open(full_path, "r", encoding="latin-1") as f:
                            out.write(f.read())

                    out.write("\n\n")

    print(f"Very good, sir. å·²ç”Ÿæˆï¼š{output_path}")

if __name__ == "__main__":
    base_directory = "."
    save_python_files(base_directory)


./frontend/app.py
--------------------------------------------------------------------------------
"""Streamlit å‰ç«¯å…¥å£ï¼šæ”¯æŒå¤šçŸ¥è¯†åº“ç®¡ç†ã€‚"""

import sys
import os
import streamlit as st
from dotenv import load_dotenv
from langchain_core.documents import Document

# æ·»åŠ  src è·¯å¾„
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

from src.graph import graph
from src.utils import load_file, split_documents
from src.storage import save_kb, load_kbs, list_kbs, delete_kb

load_dotenv()
st.set_page_config(page_title="DeepSeek RAG Pro", layout="wide")

# åˆå§‹åŒ– session state
if "messages" not in st.session_state:
    st.session_state.messages = []

if "selected_kbs" not in st.session_state:
    st.session_state.selected_kbs = []


def render_kb_management():
    """çŸ¥è¯†åº“ç®¡ç†ç•Œé¢ã€‚"""
    st.header("ğŸ“‚ çŸ¥è¯†åº“ç®¡ç†")
    
    col1, col2 = st.columns([1, 2])
    
    with col1:
        st.subheader("ç°æœ‰çŸ¥è¯†åº“")
        existing_kbs = list_kbs()
        if not existing_kbs:
            st.info("æš‚æ— çŸ¥è¯†åº“ï¼Œè¯·åœ¨å³ä¾§åˆ›å»ºã€‚")
        
        for kb in existing_kbs:
            c1, c2 = st.columns([3, 1])
            c1.write(f"ğŸ“„ {kb}")
            if c2.button("åˆ é™¤", key=f"del_{kb}", type="primary"):
                delete_kb(kb)
                st.success(f"å·²åˆ é™¤çŸ¥è¯†åº“: {kb}")
                st.rerun()

    with col2:
        st.subheader("æ–°å»º / è¿½åŠ çŸ¥è¯†")
        
        # 1. é€‰æ‹©æˆ–è¾“å…¥çŸ¥è¯†åº“åç§°
        kb_action = st.radio("æ“ä½œæ¨¡å¼", ["è¿½åŠ åˆ°ç°æœ‰", "æ–°å»ºçŸ¥è¯†åº“"], horizontal=True)
        
        target_kb_name = ""
        if kb_action == "è¿½åŠ åˆ°ç°æœ‰":
            if existing_kbs:
                target_kb_name = st.selectbox("é€‰æ‹©çŸ¥è¯†åº“", existing_kbs)
            else:
                st.warning("è¯·å…ˆæ–°å»ºçŸ¥è¯†åº“")
        else:
            target_kb_name = st.text_input("è¾“å…¥æ–°çŸ¥è¯†åº“åç§° (è‹±æ–‡/æ•°å­—)", placeholder="example_kb")

        # === æ–°å¢ï¼šé€‰æ‹©çŸ¥è¯†åº“è¯­è¨€ ===
        kb_language = st.selectbox(
            "é€‰æ‹©æ–‡æ¡£ä¸»è¦è¯­è¨€ (ç”¨äºä¼˜åŒ–æ£€ç´¢)",
            ["Chinese", "English", "Japanese", "Korean", "French"],
            index=0,
            help="DeepSeek ä¼šå°†æœç´¢è¯è‡ªåŠ¨è½¬æ¢ä¸ºæ­¤è¯­è¨€ï¼Œæé«˜æ£€ç´¢å‡†ç¡®ç‡ã€‚"
        )
        # ===========================

        # 2. ä¸Šä¼ æ–‡ä»¶æˆ–æ–‡æœ¬
        upload_mode = st.tabs(["ğŸ“ ä¸Šä¼ æ–‡ä»¶", "ğŸ“ ç²˜è´´æ–‡æœ¬"])
        raw_docs = []
        
        with upload_mode[0]:
            uploaded_files = st.file_uploader("ä¸Šä¼  PDF/TXT", type=["pdf", "txt"], accept_multiple_files=True)
        
        with upload_mode[1]:
            text_input = st.text_area("è¾“å…¥é•¿æ–‡æœ¬", height=150)

        # 3. æäº¤æŒ‰é’®
        if st.button("ğŸ’¾ ä¿å­˜åˆ°çŸ¥è¯†åº“", use_container_width=True, key="save_kb_btn"):
            if not target_kb_name:
                st.error("çŸ¥è¯†åº“åç§°ä¸èƒ½ä¸ºç©ºï¼")
                return
                
            with st.spinner("æ­£åœ¨å¤„ç†..."):
                # å¤„ç†æ–‡ä»¶
                if uploaded_files:
                    for f in uploaded_files:
                        raw_docs.extend(load_file(f))
                
                # å¤„ç†æ–‡æœ¬
                if text_input:
                    raw_docs.append(Document(page_content=text_input, metadata={"source": "text_input"}))
                
                if not raw_docs:
                    st.warning("æ²¡æœ‰æ£€æµ‹åˆ°è¾“å…¥å†…å®¹ã€‚")
                    return

                # åˆ‡åˆ†å¹¶ä¿å­˜
                chunks = split_documents(raw_docs)
                
                # === ä¿®æ”¹ï¼šä¼ å…¥ selected language ===
                save_kb(target_kb_name, chunks, language=kb_language)
                # ==================================
                
                st.success(f"æˆåŠŸå°† {len(chunks)} ä¸ªç‰‡æ®µå­˜å…¥çŸ¥è¯†åº“: [{target_kb_name}] (è¯­è¨€: {kb_language})")
                st.rerun()


def render_chat():
    """èŠå¤©ç•Œé¢åŠå¤„ç†é€»è¾‘ã€‚"""
    
    # --- ä¾§è¾¹æ ï¼šåœ¨èŠå¤©æ¨¡å¼ä¸‹æ˜¾ç¤ºçŸ¥è¯†åº“é€‰æ‹© ---
    # æ³¨æ„ï¼šStreamlit ä¼šæŒ‰é¡ºåºåœ¨ Sidebar è¿½åŠ å†…å®¹
    with st.sidebar:
        st.divider()
        st.subheader("ğŸ§  çŸ¥è¯†åº“é€‰æ‹©")
        all_kbs = list_kbs()
        selected_kbs = st.multiselect(
            "é€‰æ‹©è¦æ£€ç´¢çš„çŸ¥è¯†åº“", 
            all_kbs,
            default=all_kbs[0] if all_kbs else None
        )
        
        if not selected_kbs:
            st.warning("âš ï¸ è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªçŸ¥è¯†åº“")
        else:
            st.success(f"å·²åŠ è½½ {len(selected_kbs)} ä¸ªåº“")
            
        st.session_state.selected_kbs = selected_kbs

    # --- ä¸»åŒºåŸŸï¼šèŠå¤©å†å² ---
    st.header("ğŸ’¬ æ™ºèƒ½é—®ç­”")

    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

    # --- åº•éƒ¨ï¼šèŠå¤©è¾“å…¥æ¡† ---
    # è¿™é‡Œ st.chat_input æ˜¯åœ¨ä¸»å±‚çº§è°ƒç”¨çš„ï¼Œæ²¡æœ‰åµŒå¥—åœ¨ Tabs é‡Œï¼Œå› æ­¤ä¸ä¼šæŠ¥é”™
    user_input = st.chat_input("è¯·è¾“å…¥é—®é¢˜...", key="chat_input")
    
    if user_input:
        selected_kbs = st.session_state.get("selected_kbs", [])
        
        if not selected_kbs:
            st.error("è¯·å…ˆåœ¨å·¦ä¾§ä¾§è¾¹æ é€‰æ‹©çŸ¥è¯†åº“ï¼")
            return

        # 1. åŠ è½½é€‰ä¸­çš„çŸ¥è¯†åº“æ–‡æ¡£åˆ°å†…å­˜
        with st.spinner("æ­£åœ¨åŠ è½½çŸ¥è¯†åº“ç´¢å¼•..."):
            source_documents = load_kbs(selected_kbs)

        # 2. æ˜¾ç¤ºç”¨æˆ·è¾“å…¥
        st.session_state.messages.append({"role": "user", "content": user_input})
        with st.chat_message("user"):
            st.markdown(user_input)

        # 3. æ„é€ è¾“å…¥å¹¶è°ƒç”¨ Agent
        inputs = {
            "question": user_input,
            "source_documents": source_documents,
            "search_count": 0,
            "search_needed": False,
            # === åˆå§‹åŒ– Trace ===
            "research_trace": [
                {
                    "step": 0, 
                    "original_question": user_input, # å­˜ä½åŸå§‹é—®é¢˜
                    "query": user_input,             # åˆå§‹è®¡åˆ’æ˜¯æœåŸé¢˜
                    "findings": "Initializing", 
                    "missing": "All Info"
                }
            ],
            "final_documents": []
        }

        with st.chat_message("assistant"):
            status_box = st.status("Agent æ€è€ƒä¸­...", expanded=True)
            final_res = ""
            
            try:
                for output in graph.stream(inputs):
                    for key, val in output.items():
                        if key == "retrieve":
                            docs = val.get("retrieved_documents", [])
                            n = len(docs)
                            status_box.write(f"ğŸ” åœ¨é€‰å®šåº“ä¸­æ£€ç´¢åˆ° {n} æ¡çº¿ç´¢")
                            
                            # éå†æ˜¾ç¤ºæ£€ç´¢åˆ°çš„å…·ä½“å†…å®¹
                            for i, doc in enumerate(docs):
                                status_box.markdown(f"**ğŸ“„ çº¿ç´¢ {i + 1}**")
                                # ä½¿ç”¨å¼•ç”¨æ ¼å¼ (>) æ˜¾ç¤ºæ–‡æœ¬å†…å®¹ï¼Œä½¿å…¶åœ¨ UI ä¸Šæœ‰åŒºåˆ†åº¦
                                status_box.markdown(f"> {doc.page_content}")
                                # æ˜¾ç¤ºå…ƒæ•°æ®ï¼ˆä¾‹å¦‚æ–‡ä»¶åï¼‰
                                source = doc.metadata.get("source", "æœªçŸ¥æ¥æº")
                                status_box.caption(f"æ¥æº: {source}")
                                status_box.markdown("---") # æ·»åŠ åˆ†å‰²çº¿
                        elif key == "transform_query":
                            q = val.get("question")
                            status_box.write(f"ğŸ”„ ä¼˜åŒ–æœç´¢è¯: {q}")
                        elif key == "generate":
                            final_res = val.get("generation")
                
                # è¿è¡Œç»“æŸåï¼ŒçŠ¶æ€æ”¹ä¸ºå®Œæˆ
                # expanded=False ä¼šé»˜è®¤æŠ˜å ï¼Œç”¨æˆ·ç‚¹å‡» "å›ç­”å®Œæˆ" å³å¯å†æ¬¡å±•å¼€æŸ¥çœ‹åˆšæ‰çš„çº¿ç´¢
                status_box.update(label="å›ç­”å®Œæˆ (ç‚¹å‡»æŸ¥çœ‹æ€è€ƒè¿‡ç¨‹)", state="complete", expanded=False)
                
                if final_res:
                    st.markdown(final_res)
                    st.session_state.messages.append({"role": "assistant", "content": final_res})
                else:
                    st.warning("æœªèƒ½ç”Ÿæˆå›ç­”ï¼Œè¯·æ£€æŸ¥æ—¥å¿—ã€‚")

            except Exception as e:
                status_box.update(label="å‘ç”Ÿé”™è¯¯", state="error")
                st.error(f"è¿è¡Œé”™è¯¯: {e}")


def main():
    if not os.getenv("DEEPSEEK_API_KEY"):
        st.warning("è¯·é…ç½® .env æ–‡ä»¶ä¸­çš„ DEEPSEEK_API_KEY")

    # --- ä½¿ç”¨ä¾§è¾¹æ è¿›è¡Œé¡µé¢å¯¼èˆª ---
    with st.sidebar:
        st.title("DeepSeek RAG")
        page = st.radio(
            "åŠŸèƒ½å¯¼èˆª", 
            ["ğŸ’¬ å¯¹è¯æ¨¡å¼", "âš™ï¸ çŸ¥è¯†åº“ç®¡ç†"], 
            index=0
        )

    # æ ¹æ®é€‰æ‹©æ¸²æŸ“ä¸åŒçš„é¡µé¢ï¼ˆå‡½æ•°ï¼‰
    if page == "ğŸ’¬ å¯¹è¯æ¨¡å¼":
        render_chat()
    elif page == "âš™ï¸ çŸ¥è¯†åº“ç®¡ç†":
        render_kb_management()


if __name__ == "__main__":
    main()

./src/bm25.py
--------------------------------------------------------------------------------
"""BM25 æ£€ç´¢å™¨å°è£…ã€‚"""

from typing import List
import jieba
from rank_bm25 import BM25Okapi
from langchain_core.documents import Document


class SimpleBM25Retriever:
    """ç®€å•çš„å†…å­˜çº§ BM25 æ£€ç´¢å™¨ã€‚"""

    def __init__(self, documents: List[Document]):
        self.documents = documents
        # é¢„å¤„ç†ï¼šä¸­æ–‡åˆ†è¯
        self.corpus = [self._tokenize(doc.page_content) for doc in documents]
        self.bm25 = BM25Okapi(self.corpus)

    @staticmethod
    def _tokenize(text: str) -> List[str]:
        """ä½¿ç”¨ Jieba è¿›è¡Œä¸­æ–‡åˆ†è¯ã€‚"""
        return list(jieba.cut(text))

    def search(self, query: str, k: int = 3) -> List[Document]:
        """æ‰§è¡Œæ£€ç´¢ã€‚"""
        tokenized_query = self._tokenize(query)
        # è·å– top_k æ–‡æ¡£
        top_docs = self.bm25.get_top_n(tokenized_query, self.documents, n=k)
        return top_docs

./src/graph.py
--------------------------------------------------------------------------------
"""LangGraph å›¾æ„å»ºä¸ç¼–è¯‘ã€‚"""

from langgraph.graph import StateGraph, END
from src.state import AgentState
from src import nodes


def decide_route(state: AgentState) -> str:
    """è·¯ç”±å†³ç­–é€»è¾‘ã€‚"""
    if state.get("search_needed"):
        # ä¾ç„¶é™åˆ¶æœ€å¤§æ­¥æ•°ï¼Œé˜²æ­¢æ­»å¾ªç¯ï¼Œæ¯”å¦‚æœ€å¤šæ·±æŒ– 4 æ¬¡
        if state.get("search_count", 0) < 4:
            return "transform_query"
    return "generate"


workflow = StateGraph(AgentState)

# èŠ‚ç‚¹
workflow.add_node("transform_query", nodes.transform_query) # 1. æ‰§è¡Œè®¡åˆ’è½¬åŒ–
workflow.add_node("retrieve", nodes.retrieve)               # 2. æ‰§è¡Œæœç´¢
workflow.add_node("analyze_and_plan", nodes.analyze_and_plan) # 3. åˆ†æç»“æœå¹¶åˆ¶å®šæ–°è®¡åˆ’
workflow.add_node("generate", nodes.generate)               # 4. æœ€ç»ˆç”Ÿæˆ

# è¾¹
workflow.set_entry_point("transform_query") # å…¥å£ï¼šå…ˆå¤„ç†åˆå§‹çš„ trace[0] è®¡åˆ’

workflow.add_edge("transform_query", "retrieve")
workflow.add_edge("retrieve", "analyze_and_plan")

workflow.add_conditional_edges(
    "analyze_and_plan",
    decide_route,
    {
        "transform_query": "transform_query", # å¾ªç¯ï¼šå»æ‰§è¡Œæ–°ç”Ÿæˆçš„ plan
        "generate": "generate"
    }
)

workflow.add_edge("generate", END)

# ç¼–è¯‘å›¾
graph = workflow.compile()

./src/__init__.py
--------------------------------------------------------------------------------


./src/utils.py
--------------------------------------------------------------------------------
"""æ–‡æ¡£åŠ è½½ä¸åˆ‡åˆ†å·¥å…·å‡½æ•°ã€‚"""

import tempfile
import os
import json
from typing import List
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document


def load_file(uploaded_file) -> List[Document]:
    """å°† Streamlit ä¸Šä¼ çš„æ–‡ä»¶è½¬æ¢ä¸º Document å¯¹è±¡åˆ—è¡¨ã€‚"""
    file_ext = uploaded_file.name.split(".")[-1].lower()
    
    with tempfile.NamedTemporaryFile(delete=False, suffix=f".{file_ext}") as tmp:
        tmp.write(uploaded_file.getvalue())
        tmp_path = tmp.name

    try:
        if file_ext == "pdf":
            loader = PyPDFLoader(tmp_path)
        else:
            loader = TextLoader(tmp_path, encoding="utf-8")
        return loader.load()
    finally:
        if os.path.exists(tmp_path):
            os.remove(tmp_path)


def split_documents(docs: List[Document], chunk_size: int = 500) -> List[Document]:
    """åˆ‡åˆ†æ–‡æ¡£ã€‚"""
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=50,
        separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", "ï¼›", " ", ""]
    )
    return text_splitter.split_documents(docs)


def serialize_document(doc: Document) -> dict:
    """å°† Document å¯¹è±¡åºåˆ—åŒ–ä¸ºå­—å…¸ã€‚"""
    return {
        "page_content": doc.page_content,
        "metadata": doc.metadata
    }


def deserialize_document(data: dict) -> Document:
    """ä»å­—å…¸ååºåˆ—åŒ–ä¸º Document å¯¹è±¡ã€‚"""
    return Document(
        page_content=data["page_content"],
        metadata=data["metadata"]
    )


def serialize_documents(docs: List[Document]) -> List[dict]:
    """å°† Document å¯¹è±¡åˆ—è¡¨åºåˆ—åŒ–ä¸ºå­—å…¸åˆ—è¡¨ã€‚"""
    return [serialize_document(doc) for doc in docs]


def deserialize_documents(data: List[dict]) -> List[Document]:
    """ä»å­—å…¸åˆ—è¡¨ååºåˆ—åŒ–ä¸º Document å¯¹è±¡åˆ—è¡¨ã€‚"""
    return [deserialize_document(item) for item in data]

./src/storage.py
--------------------------------------------------------------------------------
"""çŸ¥è¯†åº“æŒä¹…åŒ–ç®¡ç†æ¨¡å—ã€‚"""

import os
import json
from typing import List
from pathlib import Path
from langchain_core.documents import Document

# æ•°æ®å­˜å‚¨ç›®å½•
STORAGE_DIR = Path("storage")
STORAGE_DIR.mkdir(exist_ok=True)


def list_kbs() -> List[str]:
    """è·å–æ‰€æœ‰å·²å­˜åœ¨çš„çŸ¥è¯†åº“åç§°ã€‚"""
    return [f.stem for f in STORAGE_DIR.glob("*.json")]


def save_kb(kb_name: str, new_docs: List[Document], language: str = "Chinese"):
    """
    ä¿å­˜æ–‡æ¡£åˆ°æŒ‡å®šçŸ¥è¯†åº“ï¼Œå¹¶æ ‡è®°è¯­è¨€å±æ€§ã€‚
    """
    file_path = STORAGE_DIR / f"{kb_name}.json"
    
    existing_data = []
    if file_path.exists():
        with open(file_path, "r", encoding="utf-8") as f:
            existing_data = json.load(f)
    
    # å°† Document å¯¹è±¡è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„å­—å…¸ï¼Œå¹¶æ³¨å…¥è¯­è¨€å±æ€§
    new_data = []
    for doc in new_docs:
        # æ³¨å…¥è¯­è¨€åˆ° metadata
        doc.metadata["language"] = language
        new_data.append({
            "page_content": doc.page_content, 
            "metadata": doc.metadata
        })
    
    merged_data = existing_data + new_data
    
    with open(file_path, "w", encoding="utf-8") as f:
        json.dump(merged_data, f, ensure_ascii=False, indent=2)


def load_kbs(kb_names: List[str]) -> List[Document]:
    """
    åŠ è½½å¤šä¸ªçŸ¥è¯†åº“ï¼Œåˆå¹¶ä¸ºä¸€ä¸ªæ–‡æ¡£åˆ—è¡¨ã€‚
    """
    all_docs = []
    for name in kb_names:
        file_path = STORAGE_DIR / f"{name}.json"
        if not file_path.exists():
            continue
            
        with open(file_path, "r", encoding="utf-8") as f:
            data = json.load(f)
            
        # å°†å­—å…¸è½¬å› Document å¯¹è±¡
        for item in data:
            all_docs.append(
                Document(page_content=item["page_content"], metadata=item["metadata"])
            )
            
    return all_docs


def delete_kb(kb_name: str):
    """åˆ é™¤æŒ‡å®šçŸ¥è¯†åº“ã€‚"""
    file_path = STORAGE_DIR / f"{kb_name}.json"
    if file_path.exists():
        os.remove(file_path)

./src/nodes.py
--------------------------------------------------------------------------------
"""LangGraph èŠ‚ç‚¹é€»è¾‘å®ç°ã€‚"""

import os
import json
from typing import List
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_openai import ChatOpenAI
from src.state import AgentState
from src.bm25 import SimpleBM25Retriever


def get_llm():
    """è·å–LLMå®ä¾‹ã€‚"""
    return ChatOpenAI(
        model="deepseek-chat",
        openai_api_key=os.getenv("DEEPSEEK_API_KEY"),
        openai_api_base=os.getenv("OPENAI_API_BASE", "https://api.deepseek.com"),
        temperature=0.1,
        # é€‚å½“å¢åŠ å¹¶å‘æ•°ï¼Œå–å†³äºä½ çš„ API é™æµæƒ…å†µ
        max_retries=2
    )


def retrieve(state: AgentState) -> dict:
    """ä½¿ç”¨ BM25 æ£€ç´¢æ–‡æ¡£ã€‚"""
    question = state["question"]
    source_docs = state.get("source_documents", [])

    if not source_docs:
        return {"retrieved_documents": []}

    # åœ¨å†…å­˜ä¸­æ„å»ºæ£€ç´¢å™¨ (é’ˆå¯¹æ— å‘é‡åº“åœºæ™¯)
    retriever = SimpleBM25Retriever(source_docs)
    results = retriever.search(question, k=3)  # æ¯æ¬¡å°‘å–ä¸€ç‚¹ï¼Œæ±‚ç²¾
    
    return {"retrieved_documents": results}


def analyze_and_plan(state: AgentState) -> dict:
    """
    æ ¸å¿ƒèŠ‚ç‚¹ï¼šé˜…è¯»æ–‡æ¡£ -> æå–ä¿¡æ¯ -> æ›´æ–°ç¬”è®° -> è§„åˆ’ä¸‹ä¸€æ­¥
    """
    original_question = state.get("original_question", state["question"]) # å»ºè®®åœ¨å¤–éƒ¨å­˜ä¸€ä¸ªåŸå§‹é—®é¢˜ï¼Œæˆ–è€…è¿™é‡Œå‡è®¾ state["question"] ä¼šå˜
    # ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬å‡è®¾ state["question"] æ˜¯å½“å‰çš„æœç´¢è¯ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªå­—æ®µå­˜åŸå§‹é—®é¢˜ã€‚
    # *ä¿®æ­£*: è®©æˆ‘ä»¬çº¦å®š state["question"] å§‹ç»ˆæ˜¯ç”¨æˆ·åŸå§‹é—®é¢˜ï¼Œ
    # è€Œæœç´¢è¯ç”± transform_query ä¸´æ—¶ç”Ÿæˆå¹¶ä¼ ç»™ retrieveï¼Œ
    # ä½†ç”±äº retrieve è¯»å–çš„æ˜¯ state["question"]ï¼Œæˆ‘ä»¬éœ€è¦åœ¨ transform ä¿®æ”¹ state["question"]ã€‚
    # ä¸ºäº†ä¸ä¸¢å¤±åŸå§‹é—®é¢˜ï¼Œå»ºè®®åœ¨ state é‡ŒåŠ ä¸€ä¸ª origin_questionï¼Œæˆ–è€…æˆ‘ä»¬è¿™é‡Œä» trace[0] æ¨æ–­ã€‚
    # *ç®€å•æ–¹æ¡ˆ*: æˆ‘ä»¬åœ¨ workflow å¯åŠ¨æ—¶ï¼ŒæŠŠåŸå§‹é—®é¢˜æ”¾å…¥ research_trace çš„åˆå§‹çŠ¶æ€ã€‚
    
    # è·å–å½“å‰ä¸Šä¸‹æ–‡
    current_docs = state["retrieved_documents"]
    trace = state.get("research_trace", [])
    final_docs = state.get("final_documents", [])
    
    # è·å–ä¸Šä¸€è½®çš„æœç´¢æ„å›¾
    last_step = trace[-1] if trace else {"query": "Initial Search"}
    current_query = last_step.get("query", "")

    llm = get_llm()

    # --- 1. å¹¶è¡Œè¿‡æ»¤æ–‡æ¡£ (ä¿ç•™æœ‰æ•ˆè¯æ®) ---
    valid_new_docs = []
    if current_docs:
        # ç®€å•è¿‡æ»¤ï¼šåªè¦æœ‰ä¸€ç‚¹ç‚¹ç›¸å…³å°±ç•™ç€ï¼Œä½œä¸ºç´ æ
        batch_msgs = [
            [SystemMessage(content="åˆ¤æ–­æ–‡æ¡£ç‰‡æ®µæ˜¯å¦åŒ…å«ä»»ä½•æœ‰ç”¨çš„ä¿¡æ¯ã€‚è¿”å›JSON: {'relevant': 'yes'/'no'}"),
             HumanMessage(content=f"Doc: {d.page_content[:300]}")]
            for d in current_docs
        ]
        results = llm.batch(batch_msgs)
        for doc, res in zip(current_docs, results):
            if "yes" in res.content.lower():
                if doc.page_content not in [fd.page_content for fd in final_docs]:
                    valid_new_docs.append(doc)
    
    # æ›´æ–°è¯æ®åº“
    updated_final_docs = final_docs + valid_new_docs
    
    # --- 2. æ·±åº¦åˆ†æä¸è§„åˆ’ (The Brain) ---
    
    # æ„é€ ä¸Šä¸‹æ–‡ï¼šæŠŠè¿‡å»çš„è°ƒæŸ¥ç»“æœæ‹¼èµ·æ¥
    history_text = ""
    for i, t in enumerate(trace):
        if i == 0: continue # è·³è¿‡åˆå§‹å ä½
        history_text += f"Step {i}: æœäº† '{t.get('query')}' -> å‘ç°: {t.get('findings')}\n"
    
    # æ„é€ æœ¬æ¬¡æ£€ç´¢åˆ°çš„å†…å®¹æ–‡æœ¬
    current_docs_text = "\n".join([f"[Doc {i}] {d.page_content}" for i, d in enumerate(valid_new_docs)])
    if not current_docs_text:
        current_docs_text = "æœ¬æ¬¡æœç´¢æœªæ‰¾åˆ°æœ‰æ•ˆæ–‡æ¡£ã€‚"

    system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„ç ”ç©¶å‘˜ã€‚æ­£åœ¨é€šè¿‡è¿­ä»£æœç´¢æ¥å›ç­”ç”¨æˆ·çš„å¤æ‚é—®é¢˜ã€‚
    
    ã€ç”¨æˆ·åŸå§‹é—®é¢˜ã€‘: {trace[0].get('original_question') if trace else 'Unknown'}
    
    ã€ä¹‹å‰çš„ç ”ç©¶è¿›å±•ã€‘:
    {history_text}
    
    ã€æœ¬æ¬¡æœç´¢æ„å›¾ã€‘: {current_query}
    
    ã€æœ¬æ¬¡æ£€ç´¢åˆ°çš„æ–°æ–‡æ¡£ã€‘:
    {current_docs_text}
    
    è¯·æ‰§è¡Œä»¥ä¸‹æ€è€ƒæ­¥éª¤å¹¶è¾“å‡º JSONï¼š
    1. **Analyze**: ç»¼åˆã€ä¹‹å‰çš„è¿›å±•ã€‘å’Œã€æ–°æ–‡æ¡£ã€‘ï¼Œæˆ‘ä»¬ç°åœ¨çŸ¥é“äº†ä»€ä¹ˆï¼Ÿæœ¬æ¬¡æœç´¢æ˜¯å¦å¡«è¡¥äº†ä¹‹å‰çš„ç©ºç™½ï¼Ÿ
    2. **Gap Check**: å¯¹ç…§ã€ç”¨æˆ·åŸå§‹é—®é¢˜ã€‘ï¼Œè¿˜æœ‰å“ªäº›å…³é”®è¦ç´ æ˜¯å®Œå…¨ç¼ºå¤±æˆ–ä¸ç¡®å®šçš„ï¼Ÿ
    3. **Decide**: 
       - å¦‚æœä¿¡æ¯å·²ç»è¶³å¤Ÿå›ç­”åŸå§‹é—®é¢˜ï¼Œæˆ–è€…å¤šæ¬¡æœç´¢å‡æ— è¿›å±•ï¼Œè®¾ç½® "search_needed": falseã€‚
       - å¦‚æœè¿˜éœ€è¦è¡¥å……ä¿¡æ¯ï¼Œè®¾ç½® "search_needed": trueã€‚
    4. **Plan**: å¦‚æœéœ€è¦ç»§ç»­æœï¼Œè¯·ç”Ÿæˆä¸‹ä¸€ä¸ªå…·ä½“çš„æœç´¢å»ºè®®ï¼ˆnext_queryï¼‰ã€‚
    
    JSON æ ¼å¼ç¤ºä¾‹:
    {{
        "findings_summary": "æœ¬æ¬¡ç¡®è®¤äº†DeepSeek V2æ”¯æŒMoEæ¶æ„ï¼Œä¸”æ€»å‚æ•°é‡ä¸º236Bã€‚",
        "missing_info": "ä»ç„¶ç¼ºä¹å…³äºå…·ä½“æ¿€æ´»å‚æ•°é‡ï¼ˆActive Paramsï¼‰çš„å‡†ç¡®æ•°å€¼ã€‚",
        "search_needed": true,
        "next_query": "DeepSeek V2 active parameters count",
        "reasoning": "å·²çŸ¥æ€»å‚æ•°ï¼Œç¼ºæ¿€æ´»å‚æ•°ï¼Œéœ€å®šå‘æŸ¥æ‰¾ã€‚"
    }}
    """
    
    try:
        response = llm.invoke([HumanMessage(content=system_prompt)])
        content = response.content.strip().replace("```json", "").replace("```", "")
        analysis = json.loads(content)
    except Exception as e:
        # å…œåº•
        analysis = {
            "findings_summary": "è§£æé”™è¯¯æˆ–æ— æ–°å‘ç°ã€‚",
            "missing_info": "ä¸ç¡®å®š",
            "search_needed": False, # é¿å…æ­»å¾ªç¯
            "next_query": ""
        }

    # æ›´æ–° Trace
    # æˆ‘ä»¬æŠŠæœ¬æ¬¡çš„åˆ†æç»“æœï¼Œè¿½åŠ åˆ° trace ä¸­
    # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬æŠŠâ€œä¸Šä¸€è½®çš„ queryâ€å¯¹åº”çš„ findings è¡¥å…¨
    if trace:
        trace[-1]["findings"] = analysis["findings_summary"]
        trace[-1]["missing"] = analysis["missing_info"]
    
    # å¦‚æœéœ€è¦ç»§ç»­æœï¼Œå‡†å¤‡ä¸‹ä¸€æ¡ trace
    if analysis["search_needed"]:
        trace.append({
            "step": len(trace),
            "query": analysis["next_query"], # è¿™é‡Œçš„ query è¿˜æ²¡ç»è¿‡ transform æ¶¦è‰²ï¼Œæš‚æ—¶å­˜å»ºè®®
            "findings": "Pending...",
            "missing": "Pending..."
        })

    return {
        "final_documents": updated_final_docs,
        "research_trace": trace,
        "search_needed": analysis["search_needed"],
        "retrieved_documents": [] # æ¸…ç©ºä¸´æ—¶åŒº
    }


def transform_query(state: AgentState) -> dict:
    """
    æ‰§è¡Œè§„åˆ’ï¼šå–å‡º Trace ä¸­çš„ 'next_query' å»ºè®®ï¼Œè¿›è¡Œ BM25 æ ¼å¼åŒ–/ç¿»è¯‘ã€‚
    """
    trace = state.get("research_trace", [])
    source_docs = state.get("source_documents", [])
    
    # è·å–æœ€æ–°çš„è®¡åˆ’
    if not trace:
        # æå°‘æƒ…å†µï¼Œåˆå§‹åŒ–
        raw_query = state["question"]
        # åˆå§‹åŒ– trace
        trace = [{"step": 0, "original_question": raw_query, "query": raw_query, "findings": "Start", "missing": "All"}]
    else:
        # å–å‡º analyze_and_plan ç”Ÿæˆçš„å»ºè®®
        raw_query = trace[-1].get("query", state["question"])

    # è¯­è¨€ä¼˜åŒ–é€»è¾‘ (å¤ç”¨ä¹‹å‰çš„)
    languages = [doc.metadata.get("language", "Chinese") for doc in source_docs]
    target_language = max(set(languages), key=languages.count) if languages else "Chinese"
    
    llm = get_llm()
    msg = [
        SystemMessage(content=f"""ä½ æ˜¯ä¸€ä¸ªæœç´¢å…³é”®è¯ä¼˜åŒ–åŠ©æ‰‹ã€‚
        ç›®æ ‡è¯­è¨€ï¼šã€{target_language}ã€‘ã€‚
        ç”¨æˆ·çš„æœç´¢æ„å›¾æ˜¯ï¼š"{raw_query}"ã€‚
        è¯·å°†å…¶è½¬æ¢ä¸ºæœ€é€‚åˆ BM25 å€’æ’ç´¢å¼•æ£€ç´¢çš„å…³é”®è¯å­—ç¬¦ä¸²ã€‚
        ä¸è¦è§£é‡Šï¼Œåªè¾“å‡ºå­—ç¬¦ä¸²ã€‚"""),
        HumanMessage(content="Convert this query.")
    ]
    
    bm25_query = llm.invoke(msg).content
    
    # å…³é”®ç‚¹ï¼šæˆ‘ä»¬éœ€è¦æŠŠä¼˜åŒ–åçš„è¯å†™å› traceï¼Œæˆ–è€…æ›´æ–° state["question"] ä¾› retrieve ä½¿ç”¨
    # è¿™é‡Œæˆ‘ä»¬æ›´æ–° state["question"] ä¾› retrieve ç”¨
    # åŒæ—¶æ›´æ–° trace é‡Œçš„ query ä¸ºå®é™…æ‰§è¡Œçš„è¯ï¼ˆå¯é€‰ï¼‰
    
    return {
        "question": bm25_query, 
        "research_trace": trace,
        "search_count": state.get("search_count", 0) + 1
    }


def generate(state: AgentState) -> dict:
    """
    æœ€ç»ˆç”Ÿæˆï¼šåŸºäºå®Œæ•´çš„è°ƒæŸ¥ç¬”è®° (Research Trace) å’Œ è¯æ® (Final Docs) å›ç­”ã€‚
    """
    trace = state.get("research_trace", [])
    final_docs = state.get("final_documents", [])
    original_question = trace[0].get("original_question") if trace else state["question"]
    
    llm = get_llm()
    
    # æ„é€ "è°ƒæŸ¥æŠ¥å‘Š"
    report = "ã€è°ƒæŸ¥è¿‡ç¨‹è®°å½•ã€‘\n"
    for t in trace:
        if t.get("step") == 0: continue
        report += f"- æ­¥éª¤ {t.get('step')}: æœç´¢äº† '{t.get('query')}'\n"
        report += f"  å‘ç°: {t.get('findings')}\n"
        report += f"  é—ç•™é—®é¢˜: {t.get('missing')}\n"
    
    # æ„é€ è¯æ®å¼•ç”¨
    evidence = "\n\n".join([f"[Ref {i+1}] {d.page_content}" for i, d in enumerate(final_docs)])
    
    prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½çŸ¥è¯†åº“åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹çš„ã€è°ƒæŸ¥è¿‡ç¨‹è®°å½•ã€‘å’Œã€å‚è€ƒæ–‡æ¡£è¯æ®ã€‘ï¼Œå›ç­”ç”¨æˆ·çš„æœ€ç»ˆé—®é¢˜ã€‚
    
    ç”¨æˆ·é—®é¢˜: {original_question}
    
    ----------------
    {report}
    ----------------
    
    å‚è€ƒæ–‡æ¡£è¯æ®:
    {evidence}
    
    ----------------
    è¯·æŒ‰ç…§ä»¥ä¸‹é€»è¾‘å›ç­”ï¼š
    1. å…ˆæ€»ç»“ä½ çš„è°ƒæŸ¥æ€è·¯ï¼ˆä¾‹å¦‚ï¼š"æˆ‘å…ˆæŸ¥æ‰¾äº†Aï¼Œç¡®è®¤äº†Xï¼Œç„¶åé’ˆå¯¹Yè¿›è¡Œäº†è¡¥å……æœç´¢..."ï¼‰ã€‚
    2. ç»™å‡ºè¯¦ç»†çš„æœ€ç»ˆç­”æ¡ˆã€‚
    3. å¦‚æœä»æœ‰æ— æ³•ç¡®è®¤çš„ä¿¡æ¯ï¼Œè¯·è¯šå®è¯´æ˜ã€‚
    """
    
    response = llm.invoke(prompt)
    return {"generation": response.content}


./src/state.py
--------------------------------------------------------------------------------
"""src/state.py"""

from typing import List, TypedDict, Optional, Dict, Any
from langchain_core.documents import Document


class AgentState(TypedDict):
    """Agent çš„è¿è¡ŒçŠ¶æ€ã€‚"""
    
    question: str
    
    # 1. åŸå§‹æ–‡æ¡£ (çŸ¥è¯†åº“)
    source_documents: List[Document]
    
    # 2. æœ¬æ¬¡æ£€ç´¢åˆ°çš„æ–‡æ¡£ (ä¸´æ—¶)
    retrieved_documents: List[Document]
    
    # 3. === æ–°å¢ï¼šç§¯ç´¯çš„æ‰€æœ‰ç›¸å…³æ–‡æ¡£ (æœ€ç»ˆç»™ Generate ç”¨çš„) ===
    final_documents: List[Document]
    
    # 4. === æ–°å¢æ ¸å¿ƒï¼šè°ƒæŸ¥ç¬”è®°/æ€è€ƒé“¾ ===
    # ç»“æ„ç¤ºä¾‹: 
    # [
    #   {
    #     "step": 1, 
    #     "query": "DeepSeek API ä»·æ ¼", 
    #     "findings": "æ‰¾åˆ°äº†è¾“å…¥ä»·æ ¼æ˜¯...", 
    #     "missing": "è¿˜æ²¡æ‰¾åˆ°è¾“å‡ºä»·æ ¼", 
    #     "next_plan": "æœç´¢ DeepSeek è¾“å‡º token è®¡è´¹"
    #   },
    #   ...
    # ]
    research_trace: List[Dict[str, Any]]
    
    generation: str
    search_count: int
    search_needed: bool

