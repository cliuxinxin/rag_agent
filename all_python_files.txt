./code.py
--------------------------------------------------------------------------------
import os

def save_python_files(base_dir):
    output_path = os.path.join(base_dir, "all_python_files.txt")

    with open(output_path, "w", encoding="utf-8") as out:
        for root, dirs, files in os.walk(base_dir):
            # å»æ‰ venv
            if "venv" in dirs:
                dirs.remove("venv")

            for fname in files:
                if fname.endswith(".py"):
                    full_path = os.path.join(root, fname)

                    out.write(full_path + "\n")
                    out.write("-" * 80 + "\n")

                    try:
                        with open(full_path, "r", encoding="utf-8") as f:
                            out.write(f.read())
                    except UnicodeDecodeError:
                        with open(full_path, "r", encoding="latin-1") as f:
                            out.write(f.read())

                    out.write("\n\n")

    print(f"Very good, sir. å·²ç”Ÿæˆï¼š{output_path}")

if __name__ == "__main__":
    base_directory = "."
    save_python_files(base_directory)


./frontend/app.py
--------------------------------------------------------------------------------
"""Streamlit å‰ç«¯å…¥å£ï¼šæ”¯æŒå¤šçŸ¥è¯†åº“ç®¡ç†ã€‚"""

import sys
import os
import streamlit as st
from dotenv import load_dotenv
from langchain_core.documents import Document

# æ·»åŠ  src è·¯å¾„
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

from src.graph import graph
from src.utils import load_file, split_documents
from src.storage import save_kb, load_kbs, list_kbs, delete_kb

load_dotenv()
st.set_page_config(page_title="DeepSeek RAG Pro", layout="wide")

# åˆå§‹åŒ– session state
if "messages" not in st.session_state:
    st.session_state.messages = []

if "active_tab" not in st.session_state:
    st.session_state.active_tab = "chat"

if "selected_kbs" not in st.session_state:
    st.session_state.selected_kbs = []


def render_kb_management():
    """çŸ¥è¯†åº“ç®¡ç†ç•Œé¢ (Tab 2)ã€‚"""
    st.header("ğŸ“‚ çŸ¥è¯†åº“ç®¡ç†")
    
    col1, col2 = st.columns([1, 2])
    
    with col1:
        st.subheader("ç°æœ‰çŸ¥è¯†åº“")
        existing_kbs = list_kbs()
        if not existing_kbs:
            st.info("æš‚æ— çŸ¥è¯†åº“ï¼Œè¯·åœ¨å³ä¾§åˆ›å»ºã€‚")
        
        for kb in existing_kbs:
            c1, c2 = st.columns([3, 1])
            c1.write(f"ğŸ“„ {kb}")
            if c2.button("åˆ é™¤", key=f"del_{kb}", type="primary"):
                delete_kb(kb)
                st.success(f"å·²åˆ é™¤çŸ¥è¯†åº“: {kb}")

    with col2:
        st.subheader("æ–°å»º / è¿½åŠ çŸ¥è¯†")
        
        # 1. é€‰æ‹©æˆ–è¾“å…¥çŸ¥è¯†åº“åç§°
        kb_action = st.radio("æ“ä½œæ¨¡å¼", ["è¿½åŠ åˆ°ç°æœ‰", "æ–°å»ºçŸ¥è¯†åº“"], horizontal=True)
        
        target_kb_name = ""
        if kb_action == "è¿½åŠ åˆ°ç°æœ‰":
            if existing_kbs:
                target_kb_name = st.selectbox("é€‰æ‹©çŸ¥è¯†åº“", existing_kbs)
            else:
                st.warning("è¯·å…ˆæ–°å»ºçŸ¥è¯†åº“")
        else:
            target_kb_name = st.text_input("è¾“å…¥æ–°çŸ¥è¯†åº“åç§° (è‹±æ–‡/æ•°å­—)", placeholder="example_kb")

        # 2. ä¸Šä¼ æ–‡ä»¶æˆ–æ–‡æœ¬
        upload_mode = st.tabs(["ğŸ“ ä¸Šä¼ æ–‡ä»¶", "ğŸ“ ç²˜è´´æ–‡æœ¬"])
        raw_docs = []
        
        with upload_mode[0]:
            uploaded_files = st.file_uploader("ä¸Šä¼  PDF/TXT", type=["pdf", "txt"], accept_multiple_files=True)
        
        with upload_mode[1]:
            text_input = st.text_area("è¾“å…¥é•¿æ–‡æœ¬", height=150)

        # 3. æäº¤æŒ‰é’®
        if st.button("ğŸ’¾ ä¿å­˜åˆ°çŸ¥è¯†åº“", use_container_width=True, key="save_kb_btn"):
            if not target_kb_name:
                st.error("çŸ¥è¯†åº“åç§°ä¸èƒ½ä¸ºç©ºï¼")
                return
                
            with st.spinner("æ­£åœ¨å¤„ç†..."):
                # å¤„ç†æ–‡ä»¶
                if uploaded_files:
                    for f in uploaded_files:
                        raw_docs.extend(load_file(f))
                
                # å¤„ç†æ–‡æœ¬
                if text_input:
                    raw_docs.append(Document(page_content=text_input, metadata={"source": "text_input"}))
                
                if not raw_docs:
                    st.warning("æ²¡æœ‰æ£€æµ‹åˆ°è¾“å…¥å†…å®¹ã€‚")
                    return

                # åˆ‡åˆ†å¹¶ä¿å­˜
                chunks = split_documents(raw_docs)
                save_kb(target_kb_name, chunks)
                st.success(f"æˆåŠŸå°† {len(chunks)} ä¸ªç‰‡æ®µå­˜å…¥çŸ¥è¯†åº“: [{target_kb_name}]")


def render_chat():
    """èŠå¤©ç•Œé¢ (Tab 1)ã€‚"""
    st.header("ğŸ’¬ æ™ºèƒ½é—®ç­”")

    # --- ä¾§è¾¹æ ï¼šé€‰æ‹©çŸ¥è¯†åº“ ---
    with st.sidebar:
        st.title("ğŸ§  çŸ¥è¯†åº“é€‰æ‹©")
        all_kbs = list_kbs()
        selected_kbs = st.multiselect(
            "è¯·é€‰æ‹©è¦æ£€ç´¢çš„çŸ¥è¯†åº“ (æ”¯æŒå¤šé€‰)", 
            all_kbs,
            default=all_kbs[0] if all_kbs else None
        )
        
        if not selected_kbs:
            st.warning("âš ï¸ è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªçŸ¥è¯†åº“")
        else:
            st.success(f"å·²åŠ è½½ {len(selected_kbs)} ä¸ªçŸ¥è¯†åº“")
            
        # å°†é€‰ä¸­çš„çŸ¥è¯†åº“å­˜å‚¨åœ¨ session_state ä¸­
        st.session_state.selected_kbs = selected_kbs

    # --- èŠå¤©åŒºåŸŸ ---
    # æ˜¾ç¤ºå†å²æ¶ˆæ¯
    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])


def handle_chat_input():
    """å¤„ç†èŠå¤©è¾“å…¥çš„é€»è¾‘ã€‚"""
    # åªæœ‰åœ¨èŠå¤©æ ‡ç­¾é¡µæ‰æ˜¾ç¤ºè¾“å…¥æ¡†
    if st.session_state.active_tab == "chat":
        user_input = st.chat_input("è¯·è¾“å…¥é—®é¢˜...", key="chat_input")
        
        if user_input:
            selected_kbs = st.session_state.get("selected_kbs", [])
            
            if not selected_kbs:
                st.error("è¯·å…ˆåœ¨å·¦ä¾§ä¾§è¾¹æ é€‰æ‹©çŸ¥è¯†åº“ï¼")
                return

            # 1. åŠ è½½é€‰ä¸­çš„çŸ¥è¯†åº“æ–‡æ¡£åˆ°å†…å­˜
            with st.spinner("æ­£åœ¨åŠ è½½çŸ¥è¯†åº“ç´¢å¼•..."):
                source_documents = load_kbs(selected_kbs)

            # 2. æ˜¾ç¤ºç”¨æˆ·è¾“å…¥
            st.session_state.messages.append({"role": "user", "content": user_input})
            with st.chat_message("user"):
                st.markdown(user_input)

            # 3. æ„é€ è¾“å…¥å¹¶è°ƒç”¨ Agent
            inputs = {
                "question": user_input,
                "source_documents": source_documents,
                "search_count": 0,
                "search_needed": False
            }

            with st.chat_message("assistant"):
                status_box = st.status("Agent æ€è€ƒä¸­...", expanded=True)
                final_res = ""
                
                try:
                    for output in graph.stream(inputs):
                        for key, val in output.items():
                            if key == "retrieve":
                                n = len(val.get("retrieved_documents", []))
                                status_box.write(f"ğŸ” åœ¨é€‰å®šåº“ä¸­æ£€ç´¢åˆ° {n} æ¡çº¿ç´¢")
                            elif key == "transform_query":
                                q = val.get("question")
                                status_box.write(f"ğŸ”„ ä¼˜åŒ–æœç´¢è¯: {q}")
                            elif key == "generate":
                                final_res = val.get("generation")
                    
                    status_box.update(label="å›ç­”å®Œæˆ", state="complete", expanded=False)
                    st.markdown(final_res)
                    st.session_state.messages.append({"role": "assistant", "content": final_res})

                except Exception as e:
                    st.error(f"è¿è¡Œé”™è¯¯: {e}")


def main():
    if not os.getenv("DEEPSEEK_API_KEY"):
        st.warning("è¯·é…ç½® .env æ–‡ä»¶ä¸­çš„ DEEPSEEK_API_KEY")

    # é¡¶éƒ¨å¯¼èˆª
    tab_chat, tab_manage = st.tabs(["ğŸ’¬ å¯¹è¯æ¨¡å¼", "âš™ï¸ çŸ¥è¯†åº“ç®¡ç†"])

    with tab_chat:
        st.session_state.active_tab = "chat"
        render_chat()
    
    with tab_manage:
        st.session_state.active_tab = "manage"
        render_kb_management()
    
    # å¤„ç†èŠå¤©è¾“å…¥ï¼ˆæ”¾åœ¨ tabs å¤–é¢ï¼‰
    handle_chat_input()


if __name__ == "__main__":
    main()

./src/bm25.py
--------------------------------------------------------------------------------
"""BM25 æ£€ç´¢å™¨å°è£…ã€‚"""

from typing import List
import jieba
from rank_bm25 import BM25Okapi
from langchain_core.documents import Document


class SimpleBM25Retriever:
    """ç®€å•çš„å†…å­˜çº§ BM25 æ£€ç´¢å™¨ã€‚"""

    def __init__(self, documents: List[Document]):
        self.documents = documents
        # é¢„å¤„ç†ï¼šä¸­æ–‡åˆ†è¯
        self.corpus = [self._tokenize(doc.page_content) for doc in documents]
        self.bm25 = BM25Okapi(self.corpus)

    @staticmethod
    def _tokenize(text: str) -> List[str]:
        """ä½¿ç”¨ Jieba è¿›è¡Œä¸­æ–‡åˆ†è¯ã€‚"""
        return list(jieba.cut(text))

    def search(self, query: str, k: int = 3) -> List[Document]:
        """æ‰§è¡Œæ£€ç´¢ã€‚"""
        tokenized_query = self._tokenize(query)
        # è·å– top_k æ–‡æ¡£
        top_docs = self.bm25.get_top_n(tokenized_query, self.documents, n=k)
        return top_docs

./src/graph.py
--------------------------------------------------------------------------------
"""LangGraph å›¾æ„å»ºä¸ç¼–è¯‘ã€‚"""

from langgraph.graph import StateGraph, END
from src.state import AgentState
from src import nodes


def decide_route(state: AgentState) -> str:
    """è·¯ç”±å†³ç­–é€»è¾‘ã€‚"""
    if state.get("search_needed"):
        # é™åˆ¶é‡è¯•æ¬¡æ•°ä¸º 3 æ¬¡
        if state.get("search_count", 0) < 3:
            return "transform_query"
    return "generate"


# æ„å»ºå›¾
workflow = StateGraph(AgentState)

# æ·»åŠ èŠ‚ç‚¹
workflow.add_node("retrieve", nodes.retrieve)
workflow.add_node("grade_documents", nodes.grade_documents)
workflow.add_node("transform_query", nodes.transform_query)
workflow.add_node("generate", nodes.generate)

# è®¾ç½®è¾¹
workflow.set_entry_point("retrieve")
workflow.add_edge("retrieve", "grade_documents")

workflow.add_conditional_edges(
    "grade_documents",
    decide_route,
    {
        "transform_query": "transform_query",
        "generate": "generate"
    }
)

workflow.add_edge("transform_query", "retrieve")
workflow.add_edge("generate", END)

# ç¼–è¯‘å›¾
graph = workflow.compile()

./src/__init__.py
--------------------------------------------------------------------------------


./src/utils.py
--------------------------------------------------------------------------------
"""æ–‡æ¡£åŠ è½½ä¸åˆ‡åˆ†å·¥å…·å‡½æ•°ã€‚"""

import tempfile
import os
import json
from typing import List
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document


def load_file(uploaded_file) -> List[Document]:
    """å°† Streamlit ä¸Šä¼ çš„æ–‡ä»¶è½¬æ¢ä¸º Document å¯¹è±¡åˆ—è¡¨ã€‚"""
    file_ext = uploaded_file.name.split(".")[-1].lower()
    
    with tempfile.NamedTemporaryFile(delete=False, suffix=f".{file_ext}") as tmp:
        tmp.write(uploaded_file.getvalue())
        tmp_path = tmp.name

    try:
        if file_ext == "pdf":
            loader = PyPDFLoader(tmp_path)
        else:
            loader = TextLoader(tmp_path, encoding="utf-8")
        return loader.load()
    finally:
        if os.path.exists(tmp_path):
            os.remove(tmp_path)


def split_documents(docs: List[Document], chunk_size: int = 500) -> List[Document]:
    """åˆ‡åˆ†æ–‡æ¡£ã€‚"""
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=50,
        separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", "ï¼›", " ", ""]
    )
    return text_splitter.split_documents(docs)


def serialize_document(doc: Document) -> dict:
    """å°† Document å¯¹è±¡åºåˆ—åŒ–ä¸ºå­—å…¸ã€‚"""
    return {
        "page_content": doc.page_content,
        "metadata": doc.metadata
    }


def deserialize_document(data: dict) -> Document:
    """ä»å­—å…¸ååºåˆ—åŒ–ä¸º Document å¯¹è±¡ã€‚"""
    return Document(
        page_content=data["page_content"],
        metadata=data["metadata"]
    )


def serialize_documents(docs: List[Document]) -> List[dict]:
    """å°† Document å¯¹è±¡åˆ—è¡¨åºåˆ—åŒ–ä¸ºå­—å…¸åˆ—è¡¨ã€‚"""
    return [serialize_document(doc) for doc in docs]


def deserialize_documents(data: List[dict]) -> List[Document]:
    """ä»å­—å…¸åˆ—è¡¨ååºåˆ—åŒ–ä¸º Document å¯¹è±¡åˆ—è¡¨ã€‚"""
    return [deserialize_document(item) for item in data]

./src/storage.py
--------------------------------------------------------------------------------
"""çŸ¥è¯†åº“æŒä¹…åŒ–ç®¡ç†æ¨¡å—ã€‚"""

import os
import json
from typing import List
from pathlib import Path
from langchain_core.documents import Document

# æ•°æ®å­˜å‚¨ç›®å½•
STORAGE_DIR = Path("storage")
STORAGE_DIR.mkdir(exist_ok=True)


def list_kbs() -> List[str]:
    """è·å–æ‰€æœ‰å·²å­˜åœ¨çš„çŸ¥è¯†åº“åç§°ã€‚"""
    return [f.stem for f in STORAGE_DIR.glob("*.json")]


def save_kb(kb_name: str, new_docs: List[Document]):
    """
    ä¿å­˜æ–‡æ¡£åˆ°æŒ‡å®šçŸ¥è¯†åº“ã€‚
    å¦‚æœæ˜¯å·²å­˜åœ¨çš„åº“ï¼Œä¼šè¿½åŠ å†…å®¹ã€‚
    """
    file_path = STORAGE_DIR / f"{kb_name}.json"
    
    existing_data = []
    if file_path.exists():
        with open(file_path, "r", encoding="utf-8") as f:
            existing_data = json.load(f)
    
    # å°† Document å¯¹è±¡è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„å­—å…¸
    new_data = [
        {"page_content": doc.page_content, "metadata": doc.metadata} 
        for doc in new_docs
    ]
    
    merged_data = existing_data + new_data
    
    with open(file_path, "w", encoding="utf-8") as f:
        json.dump(merged_data, f, ensure_ascii=False, indent=2)


def load_kbs(kb_names: List[str]) -> List[Document]:
    """
    åŠ è½½å¤šä¸ªçŸ¥è¯†åº“ï¼Œåˆå¹¶ä¸ºä¸€ä¸ªæ–‡æ¡£åˆ—è¡¨ã€‚
    """
    all_docs = []
    for name in kb_names:
        file_path = STORAGE_DIR / f"{name}.json"
        if not file_path.exists():
            continue
            
        with open(file_path, "r", encoding="utf-8") as f:
            data = json.load(f)
            
        # å°†å­—å…¸è½¬å› Document å¯¹è±¡
        for item in data:
            all_docs.append(
                Document(page_content=item["page_content"], metadata=item["metadata"])
            )
            
    return all_docs


def delete_kb(kb_name: str):
    """åˆ é™¤æŒ‡å®šçŸ¥è¯†åº“ã€‚"""
    file_path = STORAGE_DIR / f"{kb_name}.json"
    if file_path.exists():
        os.remove(file_path)

./src/nodes.py
--------------------------------------------------------------------------------
"""LangGraph èŠ‚ç‚¹é€»è¾‘å®ç°ã€‚"""

import os
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_openai import ChatOpenAI
from src.state import AgentState
from src.bm25 import SimpleBM25Retriever


def get_llm():
    """è·å–LLMå®ä¾‹ã€‚"""
    return ChatOpenAI(
        model="deepseek-chat",
        openai_api_key=os.getenv("DEEPSEEK_API_KEY"),
        openai_api_base=os.getenv("OPENAI_API_BASE", "https://api.deepseek.com"),
        temperature=0.1
    )


def retrieve(state: AgentState) -> dict:
    """ä½¿ç”¨ BM25 æ£€ç´¢æ–‡æ¡£ã€‚"""
    question = state["question"]
    source_docs = state.get("source_documents", [])

    if not source_docs:
        return {"retrieved_documents": []}

    # åœ¨å†…å­˜ä¸­æ„å»ºæ£€ç´¢å™¨ (é’ˆå¯¹æ— å‘é‡åº“åœºæ™¯)
    retriever = SimpleBM25Retriever(source_docs)
    results = retriever.search(question, k=3)
    
    return {"retrieved_documents": results}


def grade_documents(state: AgentState) -> dict:
    """è¯„ä¼°æ–‡æ¡£ç›¸å…³æ€§ã€‚"""
    question = state["question"]
    docs = state["retrieved_documents"]
    filtered_docs = []

    # åˆå§‹åŒ– LLM (DeepSeek)
    llm = get_llm()

    system_prompt = (
        "ä½ æ˜¯ä¸€ä¸ªæ–‡æ¡£è¯„ä¼°å‘˜ã€‚åˆ¤æ–­æ–‡æ¡£å†…å®¹æ˜¯å¦åŒ…å«å›ç­”ç”¨æˆ·é—®é¢˜æ‰€éœ€çš„ä¿¡æ¯ã€‚"
        "åªéœ€å›ç­” 'yes' æˆ– 'no'ã€‚"
    )

    for doc in docs:
        msg = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=f"Doc: {doc.page_content}\n\nQuery: {question}")
        ]
        grade = llm.invoke(msg).content.lower()
        if "yes" in grade:
            filtered_docs.append(doc)

    # å¦‚æœæ²¡æœ‰æ–‡æ¡£é€šè¿‡ç­›é€‰ï¼Œæ ‡è®°éœ€è¦é‡æ–°æœç´¢
    search_needed = len(filtered_docs) == 0
    return {"retrieved_documents": filtered_docs, "search_needed": search_needed}


def transform_query(state: AgentState) -> dict:
    """é‡å†™æŸ¥è¯¢ã€‚"""
    question = state["question"]
    
    # åˆå§‹åŒ– LLM (DeepSeek)
    llm = get_llm()
    
    msg = [
        SystemMessage(content="""ä¹‹å‰çš„æœç´¢æœªæ‰¾åˆ°ç›¸å…³ç»“æœã€‚
        è¯·æ ¹æ®ç”¨æˆ·åŸé—®é¢˜ï¼Œé‡å†™ä¸€ä¸ªæ›´é€‚åˆBM25å…³é”®è¯æœç´¢çš„æŸ¥è¯¢å­—ç¬¦ä¸²ã€‚
        åªè¾“å‡ºæ–°çš„æŸ¥è¯¢è¯­å¥ã€‚"""),
        HumanMessage(content=f"Original: {question}")
    ]
    new_query = llm.invoke(msg).content
    
    return {
        "question": new_query,
        "search_count": state.get("search_count", 0) + 1
    }


def generate(state: AgentState) -> dict:
    """ç”Ÿæˆæœ€ç»ˆå›ç­”ã€‚"""
    question = state["question"]
    docs = state["retrieved_documents"]
    
    # åˆå§‹åŒ– LLM (DeepSeek)
    llm = get_llm()
    
    context = "\n\n".join([d.page_content for d in docs])
    
    prompt = f"""åŸºäºä»¥ä¸‹å‚è€ƒæ–‡æ¡£å›ç­”é—®é¢˜ã€‚å¦‚æœæ— æ³•å›ç­”ï¼Œè¯·è¯´æ˜ã€‚
    
    [ä¸Šä¸‹æ–‡]
    {context}
    
    [é—®é¢˜]
    {question}
    """
    response = llm.invoke(prompt)
    return {"generation": response.content}

./src/state.py
--------------------------------------------------------------------------------
"""å®šä¹‰ LangGraph çš„çŠ¶æ€ç»“æ„ã€‚"""

from typing import List, TypedDict, Optional
from langchain_core.documents import Document


class AgentState(TypedDict):
    """Agent çš„è¿è¡ŒçŠ¶æ€ã€‚"""
    
    question: str
    # åŸå§‹çŸ¥è¯†åº“æ–‡æ¡£ (ç”±å‰ç«¯ä¼ å…¥)
    source_documents: List[Document]
    # æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£
    retrieved_documents: List[Document]
    # æœ€ç»ˆç”Ÿæˆçš„ç­”æ¡ˆ
    generation: str
    # æœç´¢é‡è¯•æ¬¡æ•°
    search_count: int
    # æ˜¯å¦éœ€è¦é‡æ–°æœç´¢æ ‡è®°
    search_needed: bool

