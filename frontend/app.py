"""Streamlit å‰ç«¯å…¥å£ï¼šæ”¯æŒå¤šçŸ¥è¯†åº“ç®¡ç†ã€‚"""

import sys
import os
import streamlit as st
from dotenv import load_dotenv
from langchain_core.documents import Document
from langchain_core.messages import HumanMessage, AIMessage

import re  # æ–°å¢æ­£åˆ™åº“
# æ·»åŠ  src è·¯å¾„
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

from src.graph import graph
from src.utils import load_file, split_documents
from src.storage import save_kb, load_kbs, list_kbs, delete_kb, get_kb_details

load_dotenv()
st.set_page_config(page_title="DeepSeek RAG Pro", layout="wide")

# === åˆå§‹åŒ– Session State ===
if "messages" not in st.session_state:
    st.session_state.messages = []
if "selected_kbs" not in st.session_state:
    st.session_state.selected_kbs = []
# æ–°å¢ï¼šç”¨äºå¤„ç†ç‚¹å‡»å»ºè®®é—®é¢˜åçš„è‡ªåŠ¨è·³è½¬
if "next_query" not in st.session_state:
    st.session_state.next_query = ""
# æ–°å¢ï¼šçŠ¶æ€åˆå§‹åŒ–ç¡®ä¿åç«¯ä¸æŠ¥é”™
if "attempted_searches" not in st.session_state:
    st.session_state.attempted_searches = []
if "research_notes" not in st.session_state:
    st.session_state.research_notes = []
if "failed_topics" not in st.session_state:
    st.session_state.failed_topics = []


def render_kb_management():
    """çŸ¥è¯†åº“ç®¡ç†ç•Œé¢"""
    st.header("ğŸ“‚ çŸ¥è¯†åº“ç®¡ç†")
    
    tabs = st.tabs(["ğŸ“š çŸ¥è¯†åº“åˆ—è¡¨ & æ£€è§†", "â• æ–°å»º/è¿½åŠ çŸ¥è¯†"])
    
    # === Tab 1: åˆ—è¡¨ä¸æ£€è§† ===
    with tabs[0]:
        existing_kbs = list_kbs()
        if not existing_kbs:
            st.info("æš‚æ— çŸ¥è¯†åº“ã€‚è¯·å»ç¬¬äºŒä¸ªæ ‡ç­¾é¡µæ–°å»ºã€‚")
        else:
            col_list, col_detail = st.columns([1, 2])
            with col_list:
                st.subheader("çŸ¥è¯†åº“åˆ—è¡¨")
                selected_kb_to_view = st.radio("é€‰æ‹©çŸ¥è¯†åº“æŸ¥çœ‹è¯¦æƒ…", existing_kbs)
                
                st.markdown("---")
                if st.button(f"ğŸ—‘ï¸ åˆ é™¤ {selected_kb_to_view}", type="primary"):
                    delete_kb(selected_kb_to_view)
                    st.success(f"å·²åˆ é™¤ {selected_kb_to_view}")
                    st.rerun()
            
            with col_detail:
                st.subheader(f"ğŸ” æ£€è§†: {selected_kb_to_view}")
                details = get_kb_details(selected_kb_to_view)
                
                m1, m2 = st.columns(2)
                m1.metric("ç‰‡æ®µæ•°é‡", details["doc_count"])
                m2.metric("æ€»å­—ç¬¦æ•°", details["total_chars"])
                
                st.divider()
                st.write("ğŸ“„ **å†…å®¹é¢„è§ˆ (éšæœºå‰5æ¡)**")
                if details["preview"]:
                    for item in details["preview"]:
                        with st.container(border=True):
                            st.caption(f"æ¥æº: {item['source']}")
                            st.text(item['content'])
                else:
                    st.write("è¯¥çŸ¥è¯†åº“ä¸ºç©ºæˆ–æ— æ³•è¯»å–ã€‚")

    # === Tab 2: æ–°å»º/è¿½åŠ  ===
    with tabs[1]:
        st.subheader("ä¸Šä¼ æ–‡æ¡£")
        kb_action = st.radio("æ¨¡å¼", ["è¿½åŠ åˆ°ç°æœ‰", "æ–°å»ºçŸ¥è¯†åº“"], horizontal=True)
        
        target_kb_name = ""
        if kb_action == "è¿½åŠ åˆ°ç°æœ‰":
            if existing_kbs:
                target_kb_name = st.selectbox("é€‰æ‹©ç›®æ ‡åº“", existing_kbs)
            else:
                st.warning("è¯·å…ˆæ–°å»º")
        else:
            target_kb_name = st.text_input("æ–°åº“åç§° (è‹±æ–‡/æ•°å­—)", placeholder="kb_v1")

        kb_language = st.selectbox("æ–‡æ¡£ä¸»è¦è¯­è¨€", ["Chinese", "English"], index=0)

        upload_mode = st.tabs(["ğŸ“ ä¸Šä¼ æ–‡ä»¶", "ğŸ“ ç²˜è´´æ–‡æœ¬"])
        raw_docs = []
        
        with upload_mode[0]:
            uploaded_files = st.file_uploader("æ”¯æŒ PDF/TXT", type=["pdf", "txt"], accept_multiple_files=True)
        
        with upload_mode[1]:
            text_input = st.text_area("è¾“å…¥æ–‡æœ¬", height=150)

        if st.button("ğŸ’¾ å¼€å§‹å¤„ç†å¹¶ä¿å­˜", use_container_width=True):
            if not target_kb_name:
                st.error("è¯·è¾“å…¥åç§°")
                return
                
            if uploaded_files:
                for f in uploaded_files:
                    raw_docs.extend(load_file(f))
            if text_input:
                from langchain_core.documents import Document
                raw_docs.append(Document(page_content=text_input, metadata={"source": "text_input"}))
            
            if not raw_docs:
                st.warning("æ²¡æœ‰å†…å®¹å¯ä¿å­˜ã€‚")
                return

            # åˆ‡åˆ† (ä½¿ç”¨æ–°çš„é€šç”¨åŒ–å‚æ•° chunk_size=800)
            chunks = split_documents(raw_docs, chunk_size=800)
            st.info(f"å·²åˆ‡åˆ†ä¸º {len(chunks)} ä¸ªç‰‡æ®µ (Chunk Size=800)")
            
            progress_bar = st.progress(0, text="åˆå§‹åŒ–å‘é‡åŒ–...")
            
            try:
                save_kb(target_kb_name, chunks, language=kb_language, progress_bar=progress_bar)
                st.success("âœ… ä¿å­˜æˆåŠŸï¼è¯·åˆ‡æ¢åˆ°â€œçŸ¥è¯†åº“åˆ—è¡¨â€æ ‡ç­¾é¡µæŸ¥çœ‹ã€‚")
                st.rerun()
            except Exception as e:
                st.error(f"ä¿å­˜å¤±è´¥: {e}")


def format_display_message(content):
    """
    è¾…åŠ©å‡½æ•°ï¼šä¼˜åŒ–æ¶ˆæ¯æ˜¾ç¤º
    å°†â€œè°ƒæŸ¥ç¬”è®°â€å’Œâ€œåŸå§‹ç‰‡æ®µâ€æŠ˜å åˆ° Expander ä¸­ï¼Œä¿æŒç•Œé¢æ•´æ´ã€‚
    """
    # 1. å°è¯•åˆ†ç¦»ä¸»è¦å›ç­”å’Œå‚è€ƒèµ„æ–™
    # å‡è®¾ Answerer çš„è¾“å‡ºä¸­åŒ…å«ã€ğŸ•µï¸â€â™‚ï¸ è°ƒæŸ¥ç¬”è®°ã€‘æˆ–ã€ğŸ“š åŸå§‹ç‰‡æ®µã€‘
    split_markers = ["ã€ğŸ•µï¸â€â™‚ï¸ è°ƒæŸ¥ç¬”è®°ã€‘", "ã€ğŸ“š åŸå§‹ç‰‡æ®µã€‘", "ã€åŸå§‹çŸ¥è¯†åº“ç‰‡æ®µã€‘"]
    
    found_marker = None
    split_index = -1
    
    for marker in split_markers:
        idx = content.find(marker)
        if idx != -1:
            if split_index == -1 or idx < split_index:
                split_index = idx
                found_marker = marker
    
    if split_index != -1:
        main_text = content[:split_index]
        ref_text = content[split_index:]
        
        st.markdown(main_text)
        with st.expander("ğŸ“š æŸ¥çœ‹è°ƒæŸ¥ç¬”è®°ä¸åŸå§‹å¼•ç”¨ (ç‚¹å‡»å±•å¼€)", expanded=False):
            st.markdown(ref_text)
    else:
        st.markdown(content)

    # 2. è§£æåç»­å»ºè®®å¹¶æ˜¾ç¤ºä¸ºæŒ‰é’®
    # æ­£åˆ™åŒ¹é…ï¼š 1. [ç‚¹å‡»] é—®é¢˜å†…å®¹
    suggestions = re.findall(r"\d+\.\s+\[ç‚¹å‡»\]\s+(.*)", content)
    if suggestions:
        st.markdown("---")
        st.caption("ğŸ‘‰ **æ‚¨å¯ä»¥ç‚¹å‡»ä»¥ä¸‹é—®é¢˜ç»§ç»­è¿½é—®ï¼š**")
        # ä½¿ç”¨ columns å¸ƒå±€æŒ‰é’®
        cols = st.columns(len(suggestions))
        for idx, question in enumerate(suggestions):
            # æŒ‰é’® key éœ€è¦å”¯ä¸€
            if cols[idx].button(question, key=f"sugg_{hash(content)}_{idx}"):
                st.session_state.next_query = question
                st.rerun()

def render_chat():
    with st.sidebar:
        st.divider()
        st.subheader("ğŸ§  çŸ¥è¯†åº“é€‰æ‹©")
        all_kbs = list_kbs()
        selected_kbs = st.multiselect("é€‰æ‹©çŸ¥è¯†åº“", all_kbs, default=all_kbs[0] if all_kbs else None)
        st.session_state.selected_kbs = selected_kbs

    st.header("ğŸ’¬ DeepSeek Research Agent")

    # æ˜¾ç¤ºå†å²æ¶ˆæ¯
    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            if msg["role"] == "assistant":
                format_display_message(msg["content"])
            else:
                st.markdown(msg["content"])

    # === æ ¸å¿ƒé€»è¾‘ï¼šå¤„ç†è¾“å…¥ (åŒ…æ‹¬è¾“å…¥æ¡†å’ŒæŒ‰é’®ç‚¹å‡») ===
    
    # 1. æ£€æŸ¥æ˜¯å¦æœ‰æ¥è‡ªæŒ‰é’®ç‚¹å‡»çš„é¢„è®¾é—®é¢˜
    preset_query = st.session_state.next_query
    user_input = st.chat_input("è¯·è¾“å…¥é—®é¢˜...")

    # 2. å†³å®šæœ€ç»ˆä½¿ç”¨çš„ query
    final_query = None
    if user_input:
        final_query = user_input
        st.session_state.next_query = "" # æ¸…ç©ºé¢„è®¾
    elif preset_query:
        final_query = preset_query
        st.session_state.next_query = "" # æ¶ˆè´¹æ‰é¢„è®¾ï¼Œé˜²æ­¢å¾ªç¯

    # 3. æ‰§è¡Œå¯¹è¯é€»è¾‘
    if final_query:
        if not st.session_state.selected_kbs:
            st.error("è¯·é€‰æ‹©çŸ¥è¯†åº“ï¼")
            return

        with st.spinner("åŠ è½½ç´¢å¼•..."):
            source_documents, vector_store = load_kbs(st.session_state.selected_kbs)

        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
        st.session_state.messages.append({"role": "user", "content": final_query})
        with st.chat_message("user"):
            st.markdown(final_query)

        # åˆå§‹åŒ– State
        initial_state = {
            "messages": [HumanMessage(content=final_query)],
            "source_documents": source_documents,
            "vector_store": vector_store,
            "next": "Supervisor",
            "current_search_query": "",
            "final_evidence": [],
            "loop_count": 0,
            "attempted_searches": [],
            "research_notes": [],
            "failed_topics": []
        }

        with st.chat_message("assistant"):
            # çŠ¶æ€å®¹å™¨
            status_container = st.status("ğŸ•µï¸â€â™‚ï¸ Agent æ­£åœ¨æ·±åº¦è°ƒç ”...", expanded=True)
            final_answer = ""

            try:
                # å¢åŠ é€’å½’é™åˆ¶é˜²æ­¢æŠ¥é”™
                graph_config = {"recursion_limit": 50}
                
                for step in graph.stream(initial_state, config=graph_config):
                    for node_name, update in step.items():
                        if node_name == "Supervisor":
                            next_node = update.get("next")
                            query = update.get("current_search_query")
                            loop = update.get("loop_count", 0)
                            
                            if next_node == "Searcher":
                                status_container.write(f"ğŸ”„ **ç¬¬ {loop} è½®è°ƒç ”**: å‘ç°ç¼ºå£ï¼ŒæŒ‡æ´¾æœç´¢ `{query}`")
                            elif next_node == "Answerer":
                                status_container.write("âœ… **å†³ç­–**: ä¿¡æ¯å……è¶³ï¼Œæ­£åœ¨æ’°å†™æŠ¥å‘Š...")

                        elif node_name == "Searcher":
                            msgs = update.get("messages", [])
                            if msgs:
                                with status_container.expander(f"ğŸ” æ£€ç´¢æŠ¥å‘Š: {update.get('attempted_searches', [''])[0]}", expanded=False):
                                    st.markdown(msgs[-1].content)

                        elif node_name == "Answerer":
                            msgs = update.get("messages", [])
                            if msgs:
                                final_answer = msgs[-1].content

                status_container.update(label="å›ç­”å®Œæˆ", state="complete", expanded=False)
                
                if final_answer:
                    # ä¿å­˜åˆ°å†å²
                    st.session_state.messages.append({"role": "assistant", "content": final_answer})
                    # æ¸²æŸ“å½“å‰å›ç­” (ä½¿ç”¨ä¼˜åŒ–åçš„æ ¼å¼åŒ–å‡½æ•°)
                    format_display_message(final_answer)

            except Exception as e:
                status_container.update(label="Error", state="error")
                st.error(f"è¿è¡Œé”™è¯¯: {e}")


def main():
    with st.sidebar:
        st.title("DeepSeek RAG")
        page = st.radio("å¯¼èˆª", ["ğŸ’¬ å¯¹è¯", "âš™ï¸ çŸ¥è¯†åº“"], index=0)

    if page == "ğŸ’¬ å¯¹è¯":
        render_chat()
    else:
        render_kb_management()


if __name__ == "__main__":
    main()